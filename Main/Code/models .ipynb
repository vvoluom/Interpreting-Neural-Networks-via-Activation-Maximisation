{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "1. Load data organised into K folds (pickle file) [DONE]\n",
    "2. obtain histograms for classes and object labels [DONE]\n",
    "3. generate one-hot encodings for object labels\n",
    "4. label encoding for output classes\n",
    "5. label encoding for multi-label training\n",
    "6. Split into k folds of similar output class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import system and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Volumes/GoogleDrive/My Drive/my_SR_Research/Code/SR_Lib')\n",
    "# Office old machine\n",
    "sys.path.append('/Users/muskata/Google Drive File Stream/My Drive/my_SR_Research/Code/SR_Lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "#from sVOC2k_lib_feat import compute_geometrical_features\n",
    "#from sVOC2k_lib_util import Object\n",
    "#from sVOC2k_lib_util import cleanObjLabel\n",
    "#from sVOC2k_lib_util import get_csv_string\n",
    "#from sVOC2k_lib_lang import getVOCembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load data organised into K folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python 2 implementation\n",
    "#pkl_file = open('dataset_shuffle_1971_4.pkl', 'rb')\n",
    "#pkl_file = open('dataset_shuffle_1971_5.pkl', 'rb')\n",
    "\n",
    "#pkl_file = open('pickel_spatialvock/dataset_shuffle_1971_2017 .pkl', 'rb')\n",
    "#pkl_file = open('dataset_shuffle_2000_2017.pkl', 'rb')\n",
    "\n",
    "#data = pickle.load(pkl_file)\n",
    "#pkl_file.close()\n",
    "\n",
    "#Python 3 implementation \n",
    "with open('pickel_spatialvock/dataset_shuffle_1971_2017 .pkl', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    p = u.load()\n",
    "    f.close()\n",
    "\n",
    "data = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['obj_cls_one_hot', 'prep_label_enc', 'prep_one_hot', 'rel_pos_one_hot', 'X_best_fold', 'Y_all_fold', 'X_all_fold', 'Y_best_fold'])\n",
      "(1058, 762)\n",
      "(2292, 762)\n",
      "Extract rel_pos_one_hot ...\n",
      "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
      "DONE\n",
      "Extract obj_cls_one_hot ...\n",
      "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
      "DONE\n",
      "Extract prep_label_enc ...\n",
      "LabelEncoder()\n",
      "DONE\n",
      "Extract prep_one_hot ...\n",
      "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "data_keys = data.keys()\n",
    "print (data_keys)\n",
    "print (data['X_best_fold']['fold_3']['X_model'].shape)\n",
    "print (data['X_all_fold']['fold_3']['X_model'].shape)\n",
    "\n",
    "X_best_fold = data['X_best_fold']\n",
    "Y_best_fold = data['Y_best_fold']\n",
    "X_all_fold = data['X_all_fold']\n",
    "Y_all_fold = data['Y_all_fold']\n",
    "\n",
    "print (\"Extract rel_pos_one_hot ...\")\n",
    "rel_pos_one_hot = data['rel_pos_one_hot']\n",
    "print(rel_pos_one_hot)\n",
    "print (\"DONE\")\n",
    "#\n",
    "print (\"Extract obj_cls_one_hot ...\")\n",
    "obj_cls_one_hot = data['obj_cls_one_hot']\n",
    "print(obj_cls_one_hot)\n",
    "print (\"DONE\")\n",
    "#\n",
    "print (\"Extract prep_label_enc ...\")\n",
    "prep_label_enc = data['prep_label_enc']\n",
    "print(prep_label_enc)\n",
    "print (\"DONE\")\n",
    "#\n",
    "print (\"Extract prep_one_hot ...\")\n",
    "prep_one_hot = data['prep_one_hot']\n",
    "print(prep_one_hot)\n",
    "print (\"DONE\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obj_glove': (62, 162),\n",
       " 'obj_code': (0, 2),\n",
       " 'rel_position': (55, 59),\n",
       " 'obj_word2vec': (162, 762),\n",
       " 'geometric': (42, 55),\n",
       " 'obj_one_hot': (2, 42),\n",
       " 'depth_human': (59, 62)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['X_best_fold']['fold_0']['meta-info']['geo_feat'].index('objDiagonalRatioTL')\n",
    "data['X_best_fold']['fold_0'].keys()\n",
    "data['X_best_fold']['fold_0']['X_headings']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_folds(X_folds, Y_folds, fold_list):\n",
    "    \"\"\"\n",
    "    name_str : name of resulting set, i.e \"train\", \"dev\" or \"test\"\n",
    "    X_folds : all the folds, i.e X_best_fold or X_all_fold\n",
    "    Y_folds : all the folds, i.e Y_best_fold or Y_all_fold\n",
    "    fold_vec : a list of fold integers to concatenate\n",
    "    \"\"\"\n",
    "    # check that all numbers in fold_vec are valid\n",
    "    assert(len(fold_list) > 0), \"fold _list is empty!\"\n",
    "    folds = X_folds.keys()\n",
    "    for f in fold_list:\n",
    "        f_query = 'fold_'+str(f)\n",
    "        assert(f_query in folds), f_query + \" not in X_folds\"\n",
    "    #\n",
    "    # Define components to concatenate\n",
    "    Y_comp = [('Y_model','array'), ('best_label', 'list'), ('all_label', 'list'), ('all_code', 'list') ]\n",
    "    X_comp = [('X_model','array'), ('obj_label', 'list')]\n",
    "    #\n",
    "    # Define headings to add\n",
    "    Y_headings = ['Y_headings']\n",
    "    X_headings = ['X_headings']\n",
    "    #\n",
    "    # Define output dictionary\n",
    "    X_set = {}\n",
    "    Y_set = {}\n",
    "    #\n",
    "    # Add Headings\n",
    "    for h in X_headings:\n",
    "        X_set[h] = X_folds['fold_'+str(fold_list[0])][h]\n",
    "    for h in Y_headings:\n",
    "        Y_set[h] = Y_folds['fold_'+str(fold_list[0])][h]\n",
    "    #\n",
    "    # Concatenate folds\n",
    "    # Copy first fold\n",
    "    f = fold_list[0]\n",
    "    for c in X_comp:\n",
    "        #print c\n",
    "        if c[1]=='array':\n",
    "            #print c[0]\n",
    "            X_set[c[0]] = np.copy(X_folds['fold_'+str(f)][c[0]])\n",
    "            #X_set['X_model'] = np.copy(X_folds['fold_'+str(f)]['X_model'])\n",
    "        if c[1]=='list':\n",
    "            #print c[0]\n",
    "            X_set[c[0]] = copy.deepcopy(X_folds['fold_'+str(f)][c[0]])\n",
    "    for c in Y_comp:\n",
    "        if c[1]=='array':\n",
    "            Y_set[c[0]] = np.copy(Y_folds['fold_'+str(f)][c[0]])\n",
    "        if c[1]=='list':\n",
    "            Y_set[c[0]] = copy.deepcopy(Y_folds['fold_'+str(f)][c[0]])\n",
    "    # Then copy the rest\n",
    "    for f in fold_list[1:]:\n",
    "        for c in X_comp:\n",
    "            #print c\n",
    "            if c[1]=='array':\n",
    "                #print c[0]\n",
    "                X_set[c[0]] = np.concatenate((X_set[c[0]],np.copy(X_folds['fold_'+str(f)][c[0]])), axis=0)\n",
    "                #X_set['X_model'] = np.copy(X_folds['fold_'+str(f)]['X_model'])\n",
    "            if c[1]=='list':\n",
    "                #print c[0]\n",
    "                X_set[c[0]] += copy.deepcopy(X_folds['fold_'+str(f)][c[0]])\n",
    "        for c in Y_comp:\n",
    "            if c[1]=='array':\n",
    "                Y_set[c[0]] = np.concatenate((Y_set[c[0]],np.copy(Y_folds['fold_'+str(f)][c[0]])), axis=0)\n",
    "            if c[1]=='list':\n",
    "                Y_set[c[0]] += copy.deepcopy(Y_folds['fold_'+str(f)][c[0]])\n",
    "\n",
    "\n",
    "    return X_set, Y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_TDT_sets(train_folds, dev_folds, test_folds):\n",
    "    \"\"\"\n",
    "    train_folds = e.g [0,1,2]\n",
    "    dev_folds   = e.g [3]\n",
    "    test_folds  = e.g [4]\n",
    "    \"\"\"\n",
    "    global X_b_train, Y_b_train, X_b_dev, Y_b_dev, X_b_test, Y_b_test\n",
    "    global x_b_train, y_b_train, x_b_dev, y_b_dev, x_b_test, y_b_test\n",
    "    #\n",
    "    global X_a_train, Y_a_train, X_a_dev, Y_a_dev, X_a_test, Y_a_test\n",
    "    global x_a_train, y_a_train, x_a_dev, y_a_dev, x_a_test, y_a_test\n",
    "    #\n",
    "    #\n",
    "    # define best_prep sets\n",
    "    X_b_train, Y_b_train = concat_folds(X_best_fold, Y_best_fold, train_folds)\n",
    "    X_b_dev, Y_b_dev = concat_folds(X_best_fold, Y_best_fold, dev_folds)\n",
    "    X_b_test, Y_b_test = concat_folds(X_best_fold, Y_best_fold, test_folds)\n",
    "    #    #\n",
    "    x_b_train = np.copy(X_b_train['X_model'][:,feat_list])\n",
    "    y_b_train = np.copy(Y_b_train['Y_model'][:,0].ravel())\n",
    "    #\n",
    "    x_b_dev = np.copy(X_b_dev['X_model'][:,feat_list])\n",
    "    y_b_dev = np.copy(Y_b_dev['Y_model'][:,0].ravel())\n",
    "    #\n",
    "    x_b_test = np.copy(X_b_test['X_model'][:,feat_list])\n",
    "    y_b_test = np.copy(Y_b_test['Y_model'][:,0].ravel())\n",
    "    #\n",
    "    #\n",
    "    # define all_prep sets\n",
    "    X_a_train,Y_a_train = concat_folds(X_all_fold, Y_all_fold, train_folds)\n",
    "    X_a_dev,Y_a_dev = concat_folds(X_all_fold, Y_all_fold, dev_folds)\n",
    "    X_a_test,Y_a_test = concat_folds(X_all_fold, Y_all_fold, test_folds)\n",
    "    #\n",
    "    x_a_train = np.copy(X_a_train['X_model'][:,feat_list])\n",
    "    y_a_train = np.copy(Y_a_train['Y_model'][:,0].ravel())\n",
    "    #\n",
    "    x_a_dev = np.copy(X_a_dev['X_model'][:,feat_list])\n",
    "    y_a_dev = np.copy(Y_a_dev['Y_model'][:,0].ravel())\n",
    "    #\n",
    "    x_a_test = np.copy(X_a_test['X_model'][:,feat_list])\n",
    "    y_a_test = np.copy(Y_a_test['Y_model'][:,0]).ravel()\n",
    "    #\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_rank(a):\n",
    "    \"\"\"\n",
    "    a : probability vector, shape(number of classes (prepositions),1)\n",
    "    N : len(a)\n",
    "    b : ranked classes (prepositions)\n",
    "    \"\"\"\n",
    "    N=len(a)\n",
    "    b=np.zeros([N],dtype=[('prob',float),('idx',int)])\n",
    "    #for i in range(N)\n",
    "    b['idx']=np.arange(N)\n",
    "    b['prob']=np.copy(a)\n",
    "    b.sort(order='prob')\n",
    "    return b\n",
    "\n",
    "def get_pred_vector(R):\n",
    "    \"\"\"\n",
    "    R : predicted probabilities, shape:(number of test examples, number of classes (prepositions))\n",
    "    b : highest ranked preposition , shape (number of test examples, 1)\n",
    "    \"\"\"\n",
    "    b=np.zeros([R.shape[0],1],dtype=float)\n",
    "    for i in range(len(b)):\n",
    "        #b[i,]=float(get_pred_rank(y_p[i])[-1][1])\n",
    "        b[i,0]=float(get_pred_rank(R[i])[-1][1])\n",
    "\n",
    "    return b\n",
    "\n",
    "def get_recall_at_k(y_pred,y_test,k):\n",
    "    \"\"\"\n",
    "    k      : compute recall @ k\n",
    "    y_pred : predicted probabilities from model, shape (size of y_test, number of classes)\n",
    "    y_test : expected output classes \n",
    "    \"\"\"\n",
    "    m = len(y_test)\n",
    "    correct=0\n",
    "    for i in range(m)[:]:\n",
    "        y_r = get_pred_rank(y_pred[i])  ### chnaged y_p to y_pred\n",
    "        #print y_r\n",
    "        y_rank = np.flip(np.asarray(y_r['idx'],dtype=int),0) \n",
    "        #print y_rank\n",
    "        if int(y_test[i]) in y_rank[:k]:\n",
    "            correct+=1\n",
    "    return float(correct)/float(m)\n",
    "\n",
    "def get_acc_all(Y_pred,y_test_all):\n",
    "    correct=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        if Y_pred[i] in y_test_all[i]:\n",
    "            correct+=1\n",
    "    return float(correct)/len(Y_pred)\n",
    "\n",
    "def get_recall_for_all_at_k(y_pred,y_all_test,k):\n",
    "    \"\"\"\n",
    "    k          : compute recall @ k\n",
    "    y_pred     : predicted probabilities from model, shape (size of y_test, number of classes)\n",
    "    y_all_test : expected output classes \n",
    "    \n",
    "    return:\n",
    "    recall@k per prep\n",
    "    weighted average\n",
    "    average\n",
    "    \"\"\"\n",
    "    m = len(y_all_test)\n",
    "    correct=0\n",
    "    for i in range(m)[:]:\n",
    "        y_r = get_pred_rank(y_p[i])\n",
    "        #print y_r\n",
    "        y_rank = np.flip(np.asarray(y_r['idx'],dtype=int),0) \n",
    "        #print y_rank\n",
    "        #print y_all_test[i]\n",
    "        if any(p in y_all_test[i] for p in y_rank[:k]):\n",
    "            correct+=1\n",
    "    #\n",
    "    return float(correct)/float(m)\n",
    "\n",
    "def get_recall_per_prep_at_k(y_pred,y_test,k):\n",
    "    \"\"\"\n",
    "    k      : compute recall @ k\n",
    "    y_pred : predicted probabilities from model, shape (size of y_test, number of classes)\n",
    "    y_test : expected output classes \n",
    "    \"\"\"\n",
    "    def get_y_histo(y_data):\n",
    "        # histogram for y_data\n",
    "        y_histo={}\n",
    "        m = y_data.shape[0]\n",
    "        p_histo={}\n",
    "        for i in range(m):\n",
    "            if int(y_data[i][0]) not in p_histo:\n",
    "                p_histo[int(y_data[i][0])]=1\n",
    "            else:\n",
    "                p_histo[int(y_data[i][0])]+=1\n",
    "        #\n",
    "        p_data=np.zeros(len(p_histo.keys()),dtype=float)\n",
    "        for p in p_histo.keys():\n",
    "            p_data[p]=p_histo[p]\n",
    "        return p_data\n",
    "    #\n",
    "    prep_in_test=get_y_histo(y_test)\n",
    "    prep_correct = np.zeros(prep_in_test.shape,dtype=float)\n",
    "    m = len(y_test)\n",
    "    for i in range(m)[:]:\n",
    "        y_r = get_pred_rank(y_pred[i])\n",
    "        #print y_r\n",
    "        y_rank = np.flip(np.asarray(y_r['idx'],dtype=int),0) \n",
    "        #print y_rank\n",
    "        if int(y_test[i]) in y_rank[:k]:\n",
    "            prep_correct[int(y_test[i])]+=1\n",
    "    #\n",
    "    recall_per_prep = prep_correct/prep_in_test\n",
    "    recall_weighted_average = np.sum(prep_correct)/np.sum(prep_in_test)\n",
    "    recall_average = np.mean(recall_per_prep)\n",
    "    #\n",
    "    return recall_per_prep, recall_average, recall_weighted_average, prep_in_test, prep_correct\n",
    "\n",
    "def get_precision_at_k(y_pred,y_test,k):\n",
    "    \"\"\"\n",
    "    k      : compute recall @ k\n",
    "    y_pred : predicted probabilities from model, shape (size of y_test, number of classes)\n",
    "    y_test : expected output classes \n",
    "    \"\"\"\n",
    "    m = len(y_test)\n",
    "    correct=0\n",
    "    for i in range(m)[:]:\n",
    "        y_r = get_pred_rank(y_pred[i])\n",
    "        y_rank = np.flip(np.asarray(y_r['idx'],dtype=int),0) \n",
    "        if (y_test[i].any() == p for p in y_rank[:k]):\n",
    "            correct+=1\n",
    "    return float(correct)/float(m)\n",
    "\n",
    "def get_precision_per_prep(y_pred,y_test):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        y_pred : predicted probabilities per preposition size(m,number of unique prepositions)\n",
    "        y_test : gold annotations: could be either single or multi-label\n",
    "    returns:\n",
    "        acc_p(0) : the precision accuracy per preposition or per class\n",
    "                   (-1) means that the preposition was never predicted\n",
    "        global_mean   : the mean over all classes, not just predicted ones\n",
    "        global_wt_mean : the mean over all classes weighted by number of predicted over all classes\n",
    "        acc_p_mean    : the mean accuracy among the predicted classes \n",
    "        acc_p_wt_mean : the sum of class accuracies weighted by the number predicted as a ratio of total\n",
    "                  predicted over all classes\n",
    "    \"\"\"\n",
    "    N = y_pred.shape[1]  #Number of prepositions\n",
    "    m = len(y_test)  #Number of test instances\n",
    "    predicted = {}  # Array: number of times a preposition is predicted\n",
    "    correct = {}    # Array: number of time the prediction is correct\n",
    "    for n in range(N):\n",
    "        predicted[n] = 0\n",
    "        correct[n] = 0\n",
    "    #    \n",
    "    for i in range(m):\n",
    "        y_r = get_pred_rank(y_p[i])\n",
    "        y_rank = np.flip(np.asarray(y_r['idx'],dtype=int),0)\n",
    "        predicted[int(y_rank[0])] += 1\n",
    "        if int(y_rank[0]) in np.array(y_test[i],dtype=int):           \n",
    "            correct[int(y_rank[0])] += 1 \n",
    "    #\n",
    "    acc_p = np.zeros((N),dtype=float)\n",
    "    for n in range(N):\n",
    "        if predicted[n] > 0:\n",
    "            acc_p[n] = float(correct[n])/float(predicted[n])\n",
    "        else:\n",
    "            acc_p[n] = -1.\n",
    "    #\n",
    "    # Compute global means\n",
    "    # remove \"-1s\"\n",
    "    zeroed_acc_p = np.zeros((N),dtype=float)\n",
    "    count_predicted = np.zeros((N),dtype=float)\n",
    "    count_correct = np.zeros((N),dtype=float)\n",
    "    total_predicted = 0\n",
    "    for n in range(N):\n",
    "        #total_predicted += predicted[n]\n",
    "        count_predicted[n] = predicted[n]\n",
    "        count_correct[n] = correct[n]\n",
    "        if acc_p[n] != -1.0:\n",
    "            zeroed_acc_p[n] = acc_p[n]\n",
    "    #\n",
    "    assert(int(count_predicted.sum()) == m), \"Wrong number of predicted prepositions\"\n",
    "    global_mean = np.mean(zeroed_acc_p)\n",
    "    global_wt_mean = np.sum(zeroed_acc_p*(count_predicted/count_predicted.sum()))\n",
    "    #\n",
    "    acc_p_list=[]\n",
    "    count_list =[]\n",
    "    for n in range(N):\n",
    "        if acc_p[n] != -1.0:\n",
    "            acc_p_list.append(acc_p[n])\n",
    "            count_list.append(predicted[n])\n",
    "    #\n",
    "    acc_p_array = np.array(acc_p_list, dtype=float)\n",
    "    count_array = np.array(count_list, dtype=float)\n",
    "    acc_p_mean = np.mean(acc_p_array)\n",
    "    acc_p_wt_mean = np.sum(acc_p_array*count_array/np.sum(count_array))\n",
    "    #\n",
    "    return acc_p, global_mean, global_wt_mean, acc_p_mean, acc_p_wt_mean, count_correct, count_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def prediction_labels(a,p):\n",
    "    #a is the list of percentages in order \n",
    "    #p is the percentage cut off point e.g 50%\n",
    "    #k is the top k answers to take\n",
    "    pred_l = []\n",
    "    temp_index = []\n",
    "    m = len(a)\n",
    "    h_count = 0\n",
    "    for i in range(m):\n",
    "        if a[i]*100 < p:\n",
    "            pred_l.append(0)\n",
    "        else:\n",
    "            pred_l.append(a[i])\n",
    "            h_count+=1\n",
    "    return pred_l,h_count\n",
    "\n",
    "def get_pred_rank1(a):\n",
    "    \"\"\"\n",
    "    a : probability vector, shape(number of classes (prepositions),1)\n",
    "    N : len(a)\n",
    "    b : ranked classes (prepositions)\n",
    "    \"\"\"\n",
    "    N=len(a)\n",
    "    b=np.zeros([N],dtype=[('prob',float),('idx',int)])\n",
    "    #for i in range(N)\n",
    "    b['idx']=np.arange(N)\n",
    "    b['prob']=np.copy(a)\n",
    "    b.sort(order='prob')\n",
    "    return b\n",
    "\n",
    "def ordered_intersection(y, h):\n",
    "    lst3 = []\n",
    "    y_count = 0\n",
    "    h_count = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1: \n",
    "            y_count +=1 \n",
    "        if h[i] == 1:\n",
    "            h_count +=1 \n",
    "        if y[i] == h[i] and y[i] == 1:\n",
    "            lst3.append(y[i])\n",
    "    return lst3 , y_count,h_count\n",
    "\n",
    "def get_precision_at_k1(y_pred,y_test,k):\n",
    "    \"\"\"\n",
    "    k      : compute recall @ k\n",
    "    y_pred : predicted probabilities from model, shape (size of y_test, number of classes)\n",
    "    y_test : expected output classes \n",
    "    \"\"\"\n",
    "    m = len(y_test)\n",
    "    pres = []\n",
    "    correct=0\n",
    "    for i in range(m)[:]:\n",
    "        #pred_l, h_count = prediction_labels(y_pred[i],prob)\n",
    "        y_r = get_pred_rank1(y_pred[i])\n",
    "        y_rank = np.flip(np.asarray(y_r['idx'],dtype=int),0)\n",
    "        \n",
    "        for p in y_rank[:k]:\n",
    "            if (y_test[i][p] == 1):\n",
    "                correct+=1\n",
    "                break\n",
    "    return float(correct)/float(m)\n",
    "\n",
    "predictions = [[1,1,0,0,0,0,0,0,0],\n",
    "               [0,0,1,1,0,0,0,0,0],\n",
    "               [0,0,0,0,1,1,0,0,0],\n",
    "               [0,0,0,0,0,0,1,1,0],\n",
    "               [0,0,0,0,0,0,0,1,1],\n",
    "               [0,0,0,0,0,0,1,1,0],\n",
    "               [0,0,0,0,0,0,1,1,0]]\n",
    "\n",
    "Y_test = predictions.copy()\n",
    "\n",
    "#Y_test[0] = [1,1,0,0,1,0,0,0,0]\n",
    "pres = get_precision_at_k1(predictions,Y_test,3)\n",
    "print(pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "def y_count(a):\n",
    "    m = len(a)\n",
    "    y_count = 0\n",
    "    for i in range(m):\n",
    "        #print(a[i])\n",
    "        if a[i] == 1:\n",
    "            y_count+=1\n",
    "    return y_count\n",
    "\n",
    "def get_recall_at_k1(y_pred,y_test,k):\n",
    "    \"\"\"\n",
    "    k      : compute recall @ k\n",
    "    y_pred : predicted probabilities from model, shape (size of y_test, number of classes)\n",
    "    y_test : expected output classes \n",
    "    \"\"\"\n",
    "    m = len(y_test)\n",
    "    recalls =[]\n",
    "    for i in range(m)[:]:\n",
    "        correct=0\n",
    "        y_len = y_count(y_test[i])\n",
    "        y_r = get_pred_rank1(y_pred[i])\n",
    "        y_rank = np.flip(np.asarray(y_r['idx'],dtype=int),0) \n",
    "        \n",
    "        for p in y_rank[:k]:\n",
    "            if (y_test[i][p] == 1):\n",
    "                correct+=1\n",
    "                \n",
    "        recalls.append(float(correct/y_len))\n",
    "        \n",
    "    return float(sum(recalls))/float(m) \n",
    "\n",
    "    \n",
    "predictions = [[1,1,0,0,0,0,0,0,0],\n",
    "               [0,0,1,1,0,0,0,0,0],\n",
    "               [0,0,0,0,1,1,0,0,0],\n",
    "               [0,0,0,0,0,0,1,1,0],\n",
    "               [0,0,0,0,0,0,0,1,1],\n",
    "               [0,0,0,0,0,0,1,1,0],\n",
    "               [0,0,0,0,0,0,1,1,0]]\n",
    "\n",
    "Y_test = predictions.copy()\n",
    "Y_test[0] = [1,1,0,1,0,0,0,0,0]\n",
    "pres = get_recall_at_k1(predictions,Y_test,3)\n",
    "print(pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def y_count_1(a):\n",
    "    m = len(a)\n",
    "    y_count = []\n",
    "    for i in range(m):\n",
    "        #print(a[i])\n",
    "        if a[i] == 1:\n",
    "            y_count.append(i)\n",
    "    return y_count\n",
    "\n",
    "def get_pred_rank1(a):\n",
    "    \"\"\"\n",
    "    a : probability vector, shape(number of classes (prepositions),1)\n",
    "    N : len(a)\n",
    "    b : ranked classes (prepositions)\n",
    "    \"\"\"\n",
    "    N=len(a)\n",
    "    b=np.zeros([N],dtype=[('prob',float),('idx',int)])\n",
    "    #for i in range(N)\n",
    "    b['idx']=np.arange(N)\n",
    "    b['prob']=np.copy(a)\n",
    "    b.sort(order='prob')\n",
    "    return b\n",
    "\n",
    "def TP_FN(y_pred,y_test,k):\n",
    "    TP = []\n",
    "    FN = []\n",
    "    for j in range(len(y_pred)):\n",
    "        Y = y_test[j]\n",
    "        H = get_pred_rank1(y_pred[j])        \n",
    "        y_count = y_count_1(y_test[j])\n",
    "        y_rank = np.flip(np.asarray(H['idx'],dtype=int),0) \n",
    "        #print(y_rank)\n",
    "        for p in y_count:\n",
    "            if p in y_rank[:k]:\n",
    "                TP.append(p)\n",
    "            else:\n",
    "                #print(p)\n",
    "                FN.append(p)\n",
    "            \n",
    "    return TP,FN \n",
    "\n",
    "def get_per_label(TP,FN,n):\n",
    "    Recalls = []\n",
    "    for i in range(0,n):\n",
    "        TP1 = TP.get(i)\n",
    "        FN1 = FN.get(i)\n",
    "        if TP1 == None:\n",
    "            TP1 = 0\n",
    "        if FN1 == None:\n",
    "            FN1 = 0\n",
    "                \n",
    "        if (TP1+FN1) == 0:\n",
    "            Recall = 0\n",
    "        else:\n",
    "            Recall  = (TP1)/(TP1+FN1)\n",
    "            \n",
    "        Recalls.append(Recall)\n",
    "        \n",
    "    return Recalls\n",
    "\n",
    "#Recall => TP/TP+FN\n",
    "#def Micro_Averaging(y_pred,y_test,k):\n",
    "    \n",
    "predictions = [[1,1,0,0,0,0,0,0,0],\n",
    "               [0,0,1,1,0,0,0,0,0],\n",
    "               [0,0,0,0,1,1,0,0,0],\n",
    "               [0,0,0,0,0,0,1,1,0],\n",
    "               [0,0,0,0,0,0,0,1,1],\n",
    "               [0,0,0,0,0,0,1,1,0],\n",
    "               [0,0,0,0,0,0,1,1,0]]\n",
    "\n",
    "Y_test = predictions.copy()\n",
    "Y_test[0] = [1,1,0,1,0,0,0,0,0]\n",
    "TP,FN  = TP_FN(predictions,Y_test,3)\n",
    "TP_Counter = Counter(TP)\n",
    "FN_Counter = Counter(FN)\n",
    "Recalls = get_per_label(TP_Counter,FN_Counter,len(Y_test[0]))\n",
    "print(Recalls)\n",
    "print(sum(Recalls)/len(Recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type, x_train, y_train, HP_dict):\n",
    "    \"\"\"\n",
    "    model_type: which model to train: LR, RF, DT, NN, kNN, SVM\n",
    "    Retuns: instance of model\n",
    "    \"\"\"\n",
    "    # ================================================================================================\n",
    "    if model_type == \"LR\":\n",
    "        my_C = HP_dict['C']\n",
    "        my_tol = HP_dict['tol']\n",
    "        this_model = LogisticRegression(C=HP_dict['C'], penalty='l2', tol = HP_dict['tol'],\n",
    "                                        multi_class='ovr', max_iter=500, random_state=1971)\n",
    "        this_model.fit(x_train, y_train)\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    if model_type == \"DT\":\n",
    "        this_model = tree.DecisionTreeClassifier(max_depth=HP_dict['tree_depth'],\n",
    "                                                 min_samples_split = HP_dict['min_samples_split'],\n",
    "                                                min_samples_leaf=HP_dict['min_samples_leaf'],\n",
    "                                                random_state=1971)\n",
    "        this_model.fit(x_train, y_train)\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    if model_type == 'RF':\n",
    "        this_model = RandomForestClassifier(n_estimators = HP_dict['n_estimators'],  #40\n",
    "                                            max_features = HP_dict['max_features'],  #40\n",
    "                                            max_depth = HP_dict['max_depth'],    #9\n",
    "                                            min_samples_split = HP_dict['min_samples_split'],\n",
    "                                            min_samples_leaf = HP_dict['min_samples_leaf'],\n",
    "                                            random_state=1971,\n",
    "                                            )\n",
    "        this_model.fit(x_train, y_train)\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    if model_type=='NN':\n",
    "        #print HP_dict['alpha'],HP_dict['tol'],HP_dict['H1'],HP_dict['H2']\n",
    "        this_model = MLPClassifier(solver='lbfgs',  # lbfgs\n",
    "                                   alpha = HP_dict['alpha'], #1e-5,\n",
    "                                   #max_iter=500,\n",
    "                                   tol=HP_dict['tol'], #0.0001\n",
    "                                   hidden_layer_sizes=(HP_dict['H1'],HP_dict['H2']), # 20,10\n",
    "                                   random_state=1971,\n",
    "                                   )\n",
    "        this_model.fit(x_train, y_train)\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    if model_type=='SVM':\n",
    "        #print HP_dict['alpha'],HP_dict['tol'],HP_dict['H1'],HP_dict['H2']\n",
    "        this_model = svm.SVC(      kernel='rbf',  # rbf\n",
    "                                   probability = True, # True\n",
    "                                   decision_function_shape = 'none', #none\n",
    "                                   tol   = HP_dict['tol'], # 0.01\n",
    "                                   C     = HP_dict['C'], # 1.0\n",
    "                                   gamma = HP_dict['gamma'], # 0.01\n",
    "                                   random_state=1971,\n",
    "                                   )\n",
    "        this_model.fit(x_train, y_train)\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    if model_type=='kNN':\n",
    "        this_model = KNeighborsClassifier(      \n",
    "                                   n_neighbors=HP_dict['n_neighbors']\n",
    "                                   )\n",
    "        this_model.fit(x_train, y_train)\n",
    "    # ================================================================================================\n",
    "\n",
    "\n",
    "# if model == 'kNN':\n",
    "#     model_kNN_1 = KNeighborsClassifier(n_neighbors=29)\n",
    "#     model_kNN_1.fit(X_train, Y_train.ravel()) \n",
    "#     #print model_kNN_1\n",
    "#     # predict probabilities for all classes\n",
    "#     print \"Testing Model :\"\n",
    "#     y_p = model_kNN_1.predict_proba(X_test)\n",
    "    \n",
    "    return this_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "K_cv = len(X_best_fold.keys())\n",
    "print (K_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features available:\n",
      "     obj_glove : (62, 162)\n",
      "     obj_code : (0, 2)\n",
      "     rel_position : (55, 59)\n",
      "     obj_word2vec : (162, 762)\n",
      "     geometric : (42, 55)\n",
      "     obj_one_hot : (2, 42)\n",
      "     depth_human : (59, 62)\n",
      "Copying feature indices : geometric :columns: 42 to 54\n",
      "Copying feature indices : rel_position :columns: 55 to 58\n",
      "[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
      "{'geometric': [0, 13], 'rel_position': [13, 17]}\n"
     ]
    }
   ],
   "source": [
    "# compute feature selection array \n",
    "print \"Features available:\"\n",
    "for hh in X_best_fold['fold_0']['X_headings']:\n",
    "    print \"    \", hh, \":\",X_best_fold['fold_0']['X_headings'][hh]\n",
    "feat_head = []\n",
    "#feat_head = ['obj_glove']\n",
    "#feat_head = ['obj_word2vec']\n",
    "#feat_head = ['obj_one_hot','geometric', 'rel_position','depth_human']\n",
    "#feat_head = ['geometric','depth_human']\n",
    "feat_head = ['geometric', 'rel_position']\n",
    "#feat_head = ['obj_one_hot', 'geometric']\n",
    "#feat_head = ['obj_one_hot', 'geometric', 'rel_position']\n",
    "#feat_head = ['obj_code', 'geometric', 'rel_position']\n",
    "#feat_head = ['obj_code', 'geometric']\n",
    "#feat_head = ['obj_one_hot']\n",
    "#feat_head = ['obj_one_hot','depth_human']\n",
    "#\n",
    "# compute feature headings\n",
    "feat_head_col={}\n",
    "feat_list = []\n",
    "col_prev=0\n",
    "for fh in feat_head:\n",
    "    col_0, col_1 = X_best_fold['fold_0']['X_headings'][fh]\n",
    "    col_last = col_prev + (col_1-col_0)\n",
    "    feat_head_col[fh] = [col_prev, col_last]\n",
    "    col_prev = col_last\n",
    "    print \"Copying feature indices :\",fh, ':columns:', col_0, 'to', col_1-1\n",
    "    feat_list += range(col_0, col_1, 1)\n",
    "#============================================================================\n",
    "# Remove diagonal feature\n",
    "#feat_list.remove(53)\n",
    "#============================================================================\n",
    "print feat_list\n",
    "print feat_head_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : ['geometric', 'rel_position']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print \"Features : \" + str(feat_head)\n",
    "print len(feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on training a model\n",
    "# setup_TDT_sets([0,1,2,3], [4], [4])\n",
    "# model_lr = train_model('LR', x_b_train, y_b_train, {'C':10, 'tol':.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obj_glove', 'meta-data', 'X_headings', 'obj_one_hot', 'obj_word2vec', 'obj_label', 'obj_code', 'X_model', 'geometric', 'rel_position', 'depth_human', 'meta-info']\n",
      "['AreaObj1_Norm_wt_Image', 'AreaObj2_Norm_wt_Image', 'AreaOverlap_Norm_wt_Min', 'AspectRatioObj_1', 'AspectRatioObj_2', 'DistBtCentr_Norm_wt_ImageDiag', 'DistanceSizeRatio', 'InvFeatXmaxXmin', 'InvFeatXminXmin', 'InvFeatYmaxYmin', 'InvFeatYminYmin', 'objAreaRatioTrajLand', 'relativePosition']\n"
     ]
    }
   ],
   "source": [
    "print X_best_fold['fold_0'].keys()\n",
    "print X_best_fold['fold_0']['meta-info']['geo_feat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FEATURE SET :  ['geometric', 'rel_position']\n",
      "                    train, dev, test,  tr_sngl  tr_mlti  ts_sngl  ts_mlti   wt_m_1  wt_m_2\n",
      "TEST fold: [0] :   [1, 2, 3, 4][0][0]  0.403    0.893    0.359   0.734    0.500   0.734    0.000   0.000   \n",
      "TEST fold: [1] :   [0, 2, 3, 4][1][1]  0.405    0.896    0.355   0.712    0.457   0.712    0.000   0.000   \n",
      "TEST fold: [2] :   [0, 1, 3, 4][2][2]  0.402    0.890    0.356   0.744    0.517   0.744    0.000   0.000   \n",
      "TEST fold: [3] :   [0, 1, 2, 4][3][3]  0.403    0.892    0.348   0.718    0.443   0.718    0.000   0.000   \n",
      "TEST fold: [4] :   [0, 1, 2, 3][4][4]  0.404    0.892    0.331   0.723    0.531   0.723    0.000   0.000   \n",
      "==========================================================================================\n",
      "                                       0.403    0.893    0.350   0.726    0.490   0.726    0.000   0.000\n",
      "b_BEST_TEST: recall@k results: \n",
      "================================\n",
      "0.359   0.601   0.722   0.820  \n",
      "0.355   0.575   0.710   0.805  \n",
      "0.356   0.599   0.741   0.835  \n",
      "0.348   0.586   0.708   0.806  \n",
      "0.331   0.582   0.711   0.817  \n",
      "================================\n",
      "0.350   0.589   0.718   0.817  \n",
      "================================\n",
      "\n",
      "b_ALL_TEST: recall@k results: \n",
      "================================\n",
      "0.734   0.862   0.921   0.952  \n",
      "0.712   0.832   0.909   0.952  \n",
      "0.744   0.856   0.927   0.957  \n",
      "0.718   0.852   0.925   0.957  \n",
      "0.723   0.863   0.927   0.955  \n",
      "================================\n",
      "0.726   0.853   0.922   0.954  \n",
      "================================\n",
      "precision mean (acc_p(0)) =  0.726161369193\n",
      " 0 Preposition                  F1     F2     F3     F4     F5  wt_mean \n",
      " 0 a_cote_de(300.0)           0.927  0.806  0.903  0.915  0.904  0.887 \n",
      " 1 a_l'exterieur_de(0.0)         -      -      -      -      -      -  \n",
      " 2 au_dessus_de(10.0)         0.667  0.667  1.000  0.000  1.000  0.700 \n",
      " 3 au_niveau_de(86.0)         0.765  0.526  0.400  0.786  0.667  0.628 \n",
      " 4 aucun(0.0)                    -      -      -      -      -      -  \n",
      " 5 autour_de(23.0)            0.750  0.333  1.000  0.500  0.667  0.565 \n",
      " 6 contre(183.0)              0.484  0.500  0.553  0.500  0.613  0.530 \n",
      " 7 dans(22.0)                 0.500  0.500  0.625  0.500  1.000  0.591 \n",
      " 8 derriere(744.0)            0.698  0.685  0.630  0.721  0.652  0.677 \n",
      " 9 devant(728.0)              0.730  0.722  0.718  0.752  0.682  0.720 \n",
      "10 en_face_de(0.0)               -      -      -      -      -      -  \n",
      "11 le_long_de(0.0)               -      -      -      -      -      -  \n",
      "12 loin_de(100.0)             0.824  0.952  0.667  0.824  0.708  0.790 \n",
      "13 par_dela(0.0)                 -      -      -      -      -      -  \n",
      "14 pres_de(2380.0)            0.747  0.742  0.795  0.718  0.757  0.752 \n",
      "15 sous(376.0)                0.737  0.685  0.754  0.656  0.736  0.710 \n",
      "16 sur(365.0)                 0.680  0.653  0.747  0.667  0.647  0.679 \n",
      "================================\n",
      " 0               Preposition   k=1    k=2    k=3    k=4  wt_mean\n",
      " 0 a_cote_de(1263.0)          0.165  0.646  0.874  0.929 \n",
      " 1 a_l'exterieur_de(27.0)     0.000  0.000  0.000  0.037 \n",
      " 2 au_dessus_de(88.0)         0.080  0.136  0.409  0.489 \n",
      " 3 au_niveau_de(115.0)        0.261  0.626  0.817  0.852 \n",
      " 4 aucun(27.0)                0.000  0.000  0.000  0.000 \n",
      " 5 autour_de(35.0)            0.371  0.400  0.457  0.629 \n",
      " 6 contre(151.0)              0.113  0.258  0.404  0.815 \n",
      " 7 dans(45.0)                 0.289  0.378  0.556  0.578 \n",
      " 8 derriere(926.0)            0.414  0.608  0.711  0.822 \n",
      " 9 devant(963.0)              0.417  0.589  0.677  0.834 \n",
      "10 en_face_de(257.0)          0.000  0.004  0.078  0.187 \n",
      "11 le_long_de(12.0)           0.000  0.000  0.000  0.000 \n",
      "12 loin_de(179.0)             0.296  0.760  0.804  0.866 \n",
      "13 par_dela(7.0)              0.000  0.000  0.000  0.286 \n",
      "14 pres_de(347.0)             0.660  0.885  0.971  0.983 \n",
      "15 sous(462.0)                0.567  0.654  0.753  0.851 \n",
      "16 sur(413.0)                 0.586  0.688  0.787  0.857 \n",
      "================================================================\n",
      " 0                           0.350  0.589  0.718  0.817\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1. Model type: LR, RF, NN, SVM, DT [done]\\n2. Features used at input [done]\\n3. acc(k) : System level accuracy, k=1..4 [done]\\n4. acc_p(0) : per preposition precision weighted average  [done]\\n5. acc_p(prep) : per preposition precision [done]\\nEXTRA RESULTS\\n6. standard recall@k on single ground truth preposition [done]\\n7. standard recall@k per preposition on single ground truth preposition [done]\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"FEATURE SET : \", feat_head\n",
    "print \"                    train, dev, test,  tr_sngl  tr_mlti  ts_sngl  ts_mlti   wt_m_1  wt_m_2\"\n",
    "results = np.zeros((K_cv+1,8),dtype=float)\n",
    "results_acc_p = np.zeros((K_cv+1,17),dtype=float)   # 17 should be a variable (number of unique preps)\n",
    "res_prep_predict = np.zeros((K_cv+1,17),dtype=float)   # 17 should be a variable (number of unique preps)\n",
    "res_prep_correct = np.zeros((K_cv+1,17),dtype=float)   # 17 should be a variable (number of unique preps)\n",
    "res_recall = np.zeros((K_cv+1,4),dtype=float)\n",
    "res_recall_all = np.zeros((K_cv+1,4),dtype=float)\n",
    "res_conf_mat = np.zeros((K_cv+1,len(prep_label_enc.classes_),len(prep_label_enc.classes_)),dtype=float)\n",
    "res_recall_tested = np.zeros((4,K_cv+1,17),dtype=float)   # 17 should be a variable (number of unique preps)\n",
    "res_recall_correct = np.zeros((4,K_cv+1,17),dtype=float)   # 17 should be a variable (number of unique preps)\n",
    "\n",
    "\n",
    "for j_test in range(K_cv):\n",
    "    # select folds\n",
    "    all_folds = range(K_cv)\n",
    "    all_folds.remove(j_test)\n",
    "    train_folds = list(all_folds)\n",
    "    dev_folds = [j_test] # Dummy\n",
    "    test_folds = [j_test]\n",
    "    print \"TEST fold:%4s\"%str(test_folds),\":\",\n",
    "    print \"%20s\"%(str(train_folds)+ str(dev_folds)+ str(test_folds)),\n",
    "    setup_TDT_sets(train_folds, dev_folds, test_folds )\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    scale_feat = True\n",
    "    if 'geometric' in feat_head and scale_feat:\n",
    "        scaler = StandardScaler()\n",
    "        col_0, col_1 = feat_head_col['geometric']\n",
    "        scaler.fit(x_b_train[:,col_0:col_1])  # training with best\n",
    "        x_b_train[:,col_0:col_1] = scaler.transform(x_b_train[:,col_0:col_1])  \n",
    "        x_b_dev[:,col_0:col_1] = scaler.transform(x_b_dev[:,col_0:col_1])\n",
    "        x_b_test[:,col_0:col_1] = scaler.transform(x_b_test[:,col_0:col_1])\n",
    "        x_a_train[:,col_0:col_1] = scaler.transform(x_a_train[:,col_0:col_1])  \n",
    "        x_a_dev[:,col_0:col_1] = scaler.transform(x_a_dev[:,col_0:col_1])\n",
    "        x_a_test[:,col_0:col_1] = scaler.transform(x_a_test[:,col_0:col_1])\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    model = 'RF'\n",
    "    data_set = 'expanded'\n",
    "    #data_set = 'best'\n",
    "    #\n",
    "    if data_set == 'expanded': \n",
    "        x_train = x_a_train\n",
    "        y_train = y_a_train\n",
    "        Y_train = Y_a_train\n",
    "        x_test = x_b_test\n",
    "        yy_test = y_b_test\n",
    "        Y_test = Y_b_test        \n",
    "\n",
    "     \n",
    "#         x_test = x_a_test\n",
    "#         yy_test = y_a_test\n",
    "#         Y_test = Y_a_test\n",
    "    if data_set == 'best':\n",
    "        x_train = x_b_train\n",
    "        y_train = y_b_train\n",
    "        Y_train = Y_b_train\n",
    "        \n",
    "        x_test = x_b_test\n",
    "        yy_test = y_b_test\n",
    "        Y_test = Y_b_test        \n",
    "    #\n",
    "    \n",
    "    h_param = {}\n",
    "    h_param['LR'] = {'C':1, 'tol':.001}\n",
    "    h_param['DT'] = {'tree_depth':8, 'min_samples_split':4, 'min_samples_leaf':2}\n",
    "    h_param['RF'] = {'n_estimators':100, 'max_features':17,'max_depth':9, 'min_samples_leaf':2, 'min_samples_split':10}\n",
    "    #h_param['NN'] = {'alpha':1e-4, 'H1':12,  'H2':10, 'tol':0.001}\n",
    "    h_param['SVM'] = {'C':10.0, 'gamma':0.01, 'tol':0.01}\n",
    "    h_param['kNN'] = {'n_neighbors':17}\n",
    "    #\n",
    "    model_exp = train_model(model, x_train, y_train, h_param[model]) \n",
    "    \n",
    "    #\n",
    "    #if model == 'DT':\n",
    "    #    model_exp = train_model(model, x_train, y_train, {'tree_depth':6}) # 6 or 7\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    #\n",
    "    # test on BEST_TRAIN_SINGLE\n",
    "    y_p = model_exp.predict_proba(x_train)\n",
    "    y_test = y_train\n",
    "    y_all_test = Y_train['all_code']\n",
    "\n",
    "    #y_p = model_exp.predict_proba(x_a_train)\n",
    "    #y_test = y_a_train\n",
    "    #y_all_test = Y_a_train['all_code']\n",
    "    results[j_test,0] = get_recall_at_k(y_p, y_test, 1)\n",
    "    results[j_test,1] = get_recall_for_all_at_k(y_p, y_all_test, 1)\n",
    "    print \" %5.3f    %5.3f   \"%(results[j_test,0], results[j_test,1]),\n",
    "\n",
    "    # test on BEST_TEST_SINGLE and BEST_TEST_MULTI\n",
    "    y_p = model_exp.predict_proba(x_test)\n",
    "    y_test = yy_test\n",
    "    y_all_test = Y_test['all_code']   \n",
    "    \n",
    "    #y_p = model_exp.predict_proba(x_a_test)\n",
    "    #y_test = y_a_test\n",
    "    #y_all_test = Y_a_test['all_code']   \n",
    "    results[j_test,2] = get_recall_at_k(y_p, y_test, 1)\n",
    "    for r in range(4):\n",
    "        res_recall[j_test,r] = get_recall_at_k(y_p, y_test, r+1)\n",
    "        res_recall_all[j_test,r] = get_recall_for_all_at_k(y_p, y_all_test, r+1)\n",
    "    results[j_test,3] = get_recall_for_all_at_k(y_p, y_all_test, 1)\n",
    "    #Y_pred=get_pred_vector(y_p)\n",
    "    #res_conf_mat[j_test] = np.copy(confusion_matrix(y_test, Y_pred))\n",
    "    print \"%5.3f   %5.3f   \"%(results[j_test,2], results[j_test,3]),\n",
    "    acc_p, global_mean, global_wt_mean, acc_p_mean, acc_p_wt_mean, count_correct, count_predicted =\\\n",
    "                                                            get_precision_per_prep(y_p, y_all_test)\n",
    "    results[j_test,4] = global_mean\n",
    "    results[j_test,5] = global_wt_mean\n",
    "    results_acc_p[j_test] = np.copy(acc_p)\n",
    "    res_prep_predict[j_test] = np.copy(count_predicted)\n",
    "    res_prep_correct[j_test] = np.copy(count_correct)\n",
    "    #results[j_test,6] = acc_p_mean\n",
    "    #results[j_test,7] = acc_p_wt_mean\n",
    "    #\n",
    "    print \"%5.3f   %5.3f   \"%(results[j_test,4], results[j_test,5] ),\n",
    "    print \"%5.3f   %5.3f   \"%(results[j_test,6], results[j_test,7] )\n",
    "    # Compute recall per preposition\n",
    "    for r in range(4):\n",
    "        recall_per_prep, recall_average, recall_weighted_average, prep_in_test, prep_correct =\\\n",
    "                                    get_recall_per_prep_at_k(y_p, y_test.reshape((y_test.shape[0],1)), r+1)\n",
    "        res_recall_tested[r,j_test,:] = np.copy(prep_in_test)\n",
    "        res_recall_correct[r,j_test,:] = np.copy(prep_correct)\n",
    "\n",
    "    \n",
    "    \n",
    "print \"==========================================================================================\"\n",
    "results[-1,:] = np.mean(results[0:-1,:],axis=0)\n",
    "print \"                                       %5.3f    %5.3f    %5.3f   %5.3f    %5.3f   %5.3f    %5.3f   %5.3f\"\\\n",
    "                                                    %(results[-1,0],\\\n",
    "                                                    results[-1,1], results[-1,2], results[-1,3],\\\n",
    "                                                    results[-1,4], results[-1,5],\\\n",
    "                                                    results[-1,6], results[-1,7])\n",
    "#\n",
    "# standard recall@k\n",
    "print\"b_BEST_TEST: recall@k results: \"\n",
    "print\"================================\"\n",
    "res_recall[-1,:] = np.mean(res_recall[0:-1,:],axis=0)\n",
    "for j in range(K_cv+1):\n",
    "    for r in range(4):\n",
    "        print \"%5.3f  \"%(res_recall[j,r]),\n",
    "    print\n",
    "    if j==K_cv-1:\n",
    "        print\"================================\"\n",
    "print\"================================\"\n",
    "print\n",
    "#\n",
    "# system level accuracy\n",
    "print\"b_ALL_TEST: recall@k results: \"\n",
    "print\"================================\"\n",
    "res_recall_all[-1,:] = np.mean(res_recall_all[0:-1,:],axis=0)\n",
    "for j in range(K_cv+1):\n",
    "    for r in range(4):\n",
    "        print \"%5.3f  \"%(res_recall_all[j,r]),\n",
    "    print\n",
    "    if j==K_cv-1:\n",
    "        print\"================================\"\n",
    "print\"================================\"\n",
    "\n",
    "# Compute precison per preposition (acc_p(0))\n",
    "p_predict = np.sum(res_prep_predict, axis=0)\n",
    "p_correct = np.sum(res_prep_correct, axis=0)\n",
    "N=p_predict.shape[0]\n",
    "acc_p_0 = np.zeros((N),dtype=float)\n",
    "for n in range(N):\n",
    "    if p_predict[n] > 0:\n",
    "        acc_p_0[n] = float(p_correct[n])/float(p_predict[n])\n",
    "    else:\n",
    "        acc_p_0[n] = -1.\n",
    "results_acc_p[-1,:] = np.copy(acc_p_0)\n",
    "acc_p_0_wt_mean = np.sum(acc_p_0*p_predict/p_predict.sum())\n",
    "print \"precision mean (acc_p(0)) = \",acc_p_0_wt_mean\n",
    "print '%2d %-25s %5s  %5s  %5s  %5s  %5s  %5s '%(0,'Preposition', \"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"wt_mean\")\n",
    "for j in range(N):\n",
    "    print '%2d %-25s '%(j,prep_label_enc.inverse_transform(j)+'('+str(p_predict[j])+')'),\n",
    "    #print '%2d %-14s '%(j,''),\n",
    "\n",
    "    for i in range(results_acc_p.shape[0]):\n",
    "        if results_acc_p[i,j]==-1:\n",
    "            print \"%5s \"%\" - \",\n",
    "        else:\n",
    "            print \"%5.3f \"%results_acc_p[i,j],\n",
    "    print\n",
    "print\"================================\"\n",
    "\n",
    "#acc_p_0_wt_mean = np.sum(acc_p_0*p_predict/p_predict.sum())\n",
    "#print acc_p_0_wt_mean\n",
    "#\n",
    "# Recall@k per preposition\n",
    "res_recall_tested[:,-1,:] = np.sum(res_recall_tested[:,0:-1,:], axis=1)\n",
    "res_recall_correct[:,-1,:] = np.sum(res_recall_correct[:,0:-1,:], axis=1)\n",
    "recall_per_p_at_k = np.copy(res_recall_correct[:,-1,:]/res_recall_tested[:,-1,:])\n",
    "recall_per_p_wt_mean = np.copy(np.sum(recall_per_p_at_k*res_recall_tested[:,-1,:]/\n",
    "                               np.sum(res_recall_tested[:,-1,:], axis=1, keepdims=True), axis=1))\n",
    "#\n",
    "print '%2d %25s %5s  %5s  %5s  %5s  %5s'%(0,'Preposition', \"k=1\",\"k=2\",\"k=3\",\"k=4\",\"wt_mean\")\n",
    "for j in range(N):\n",
    "    print '%2d %-25s '%(j,prep_label_enc.inverse_transform(j)+'('+str(res_recall_tested[0,-1,j])+')'),\n",
    "    for i in range(recall_per_p_at_k.shape[0]):    \n",
    "        print \"%5.3f \"%recall_per_p_at_k[i,j],\n",
    "    print    \n",
    "print\"================================================================\"\n",
    "print '%2d %-25s %5.3f  %5.3f  %5.3f  %5.3f'%(0,'',recall_per_p_wt_mean[0],\\\n",
    "                                      recall_per_p_wt_mean[1],\\\n",
    "                                      recall_per_p_wt_mean[2],\\\n",
    "                                      recall_per_p_wt_mean[3])\n",
    "print\"================================================================\"\n",
    "#\n",
    "#\n",
    "# Confusion matrix\n",
    "#\n",
    "# Save to file\n",
    "## Save to file\n",
    "\"\"\"\n",
    "1. Model type: LR, RF, NN, SVM, DT [done]\n",
    "2. Features used at input [done]\n",
    "3. acc(k) : System level accuracy, k=1..4 [done]\n",
    "4. acc_p(0) : per preposition precision weighted average  [done]\n",
    "5. acc_p(prep) : per preposition precision [done]\n",
    "EXTRA RESULTS\n",
    "6. standard recall@k on single ground truth preposition [done]\n",
    "7. standard recall@k per preposition on single ground truth preposition [done]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = True\n",
    "if save_flag:\n",
    "    out_file = open('results_2000_2017_BB_for_MULTI.txt','a')\n",
    "    out_file.write('\\n/START RUN =========================================================================\\n\\n')\n",
    "    out_file.write('Model : ' + str(model)+'\\n\\n')\n",
    "    out_file.write('HP :'+str(h_param[model])+'\\n\\n')\n",
    "    out_file.write('Dataset : ' + str(data_set)+'\\n\\n')\n",
    "    out_file.write('Features : ' + str(feat_head)+'\\n\\n')\n",
    "    out_file.write('Ratio of train/test - Single label: train=%4.3f : test=%4.3f : ratio=%4.3f\\n'%\\\n",
    "                                                      (results[-1,0], results[-1,2],\n",
    "                                                      results[-1,2]/results[-1,0]))\n",
    "    out_file.write('Ratio of train/test -  Multi label: train=%4.3f : test=%4.3f : ratio=%4.3f\\n'%\\\n",
    "                                                      (results[-1,1], results[-1,3],\n",
    "                                                      results[-1,3]/results[-1,1]))\n",
    "    out_file.write('\\n')\n",
    "    #\n",
    "    out_file.write('     k :  1      2      3      4\\n')\n",
    "    out_file.write('acc(k) : ')\n",
    "    for r in range(4):\n",
    "        out_file.write(\"%5.3f  \"%(res_recall_all[-1,r]))\n",
    "    out_file.write('\\n\\n')\n",
    "    #\n",
    "    out_file.write(\"precision weighted mean (acc_p(0)) = %5.3f\\n\"%acc_p_0_wt_mean)\n",
    "    out_file.write('%2s  %20s  %8s  %8s\\n'%('','Preposition',\"Count\", \"acc_p(0)\"))\n",
    "    for j in range(N):\n",
    "        out_file.write('%2d %20s  %8.1f  %5.3f\\n'%(j,prep_label_enc.inverse_transform(j),\\\n",
    "                                                 p_predict[j], results_acc_p[-1,j]))\n",
    "    out_file.write(\"============================================\\n\")\n",
    "    out_file.write('%2s %20s  %8.1f  %5.3f\\n'%('','weighted mean',\\\n",
    "                                                 p_predict.sum(), acc_p_0_wt_mean))\n",
    "    out_file.write(\"============================================\\n\")\n",
    "    #\n",
    "    out_file.write(\"\\n========= EXTRA RESULTS ============\\n\\n\")\n",
    "    #\n",
    "    out_file.write('       k :  1      2      3      4\\n')\n",
    "    out_file.write('recall@k : ')\n",
    "    for r in range(4):\n",
    "        out_file.write(\"%5.3f  \"%(res_recall[-1,r]))\n",
    "    out_file.write('\\n\\n')\n",
    "    #\n",
    "    out_file.write('%2s %20s %8s  %5s  %5s  %5s  %5s\\n'%('','Preposition',\"Count\", \"k=1\",\"k=2\",\"k=3\",\"k=4\"))\n",
    "    for j in range(N):\n",
    "        out_file.write('%2d %20s %8.1f  '%(j,prep_label_enc.inverse_transform(j), res_recall_tested[0,-1,j]))\n",
    "        for i in range(recall_per_p_at_k.shape[0]):    \n",
    "            out_file.write(\"%5.3f  \"%recall_per_p_at_k[i,j])\n",
    "        out_file.write('\\n')    \n",
    "    out_file.write(\"================================================================\\n\")\n",
    "    out_file.write('%2s %20s %8.1f  %5.3f  %5.3f  %5.3f  %5.3f\\n'%('','weighted mean',\\\n",
    "                                          res_recall_tested[0,-1,:].sum(),\\\n",
    "                                          recall_per_p_wt_mean[0],\\\n",
    "                                          recall_per_p_wt_mean[1],\\\n",
    "                                          recall_per_p_wt_mean[2],\\\n",
    "                                          recall_per_p_wt_mean[3]))\n",
    "    out_file.write(\"================================================================\\n\\n\")\n",
    "    #\n",
    "    out_file.write('/END RUN =========================================================================\\n\\n')\n",
    "    out_file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn_1 = MLPClassifier(solver='lbfgs', \n",
    "                           alpha=1e-4,\n",
    "                           #max_iter=500,\n",
    "                           tol = 0.01,\n",
    "                           hidden_layer_sizes=(10,10),\n",
    "                           random_state=1971,\n",
    "                           )\n",
    "model_nn_1.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_param['NN'] = {'alpha':1e-4, 'H1':10,  'H2':10, 'tol':0.001}\n",
    "model_exp = train_model(model, x_train, y_train, h_param[model]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'r') as f:\n",
    "    res_file_raw = f.readlines()\n",
    "#\n",
    "res_file=[]\n",
    "for item in res_file_raw:\n",
    "    item_list = item.split()\n",
    "    if len(item_list)>0:\n",
    "        res_file.append(item_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(res_batch):\n",
    "    ret_model = -1\n",
    "    for item in res_batch:\n",
    "        if item[0] == 'Model':\n",
    "            ret_model = item[2]\n",
    "    #\n",
    "    return ret_model\n",
    "#\n",
    "#\n",
    "def get_acc_k(res_batch):\n",
    "    ret_acc_k = -1\n",
    "    for item in res_batch:\n",
    "        if item[0] == 'acc(k)':\n",
    "            ret_acc_k = item[2:]\n",
    "    #\n",
    "    return ret_acc_k\n",
    "#\n",
    "#\n",
    "def get_acc_p_prep_data(res_batch):\n",
    "    \"\"\"\n",
    "    return prep labels, counts, acc_p(0)\n",
    "    \"\"\"\n",
    "    ret_acc_p = -1\n",
    "    find_table = False\n",
    "    table_start = -1\n",
    "    for i,item in enumerate(res_batch[:-1]):\n",
    "        if item[0] == 'precision':\n",
    "            #and res_batch[i+1]=='Preposition':\n",
    "            find_table = True\n",
    "            table_start = i+2\n",
    "    if find_table:\n",
    "        ret_acc_p = res_batch[table_start:table_start+17] + [res_batch[table_start+18]]\n",
    "    #\n",
    "    return ret_acc_p\n",
    "#\n",
    "#\n",
    "def get_acc_p_wt_average(res_batch):\n",
    "    return 0\n",
    "\n",
    "\n",
    "with open('results_2017.txt', 'r') as f:\n",
    "    res_file_raw = f.readlines()\n",
    "#\n",
    "res_file=[]\n",
    "for item in res_file_raw:\n",
    "    item_list = item.split()\n",
    "    if len(item_list)>0:\n",
    "        res_file.append(item_list)\n",
    "\n",
    "\n",
    "# Find start and ends\n",
    "start_list = []\n",
    "end_list = []\n",
    "for row,item in enumerate(res_file):\n",
    "    if item[0] == '/START':\n",
    "        start_list.append(row)\n",
    "    if item[0] == '/END':\n",
    "        end_list.append(row)\n",
    "#\n",
    "print start_list\n",
    "print end_list\n",
    "assert(len(start_list)==len(end_list)), \"start and end list not of same size\"\n",
    "no_batches = len(start_list)\n",
    "#\n",
    "# collect acc(k) results\n",
    "acc_k = [['','k=1','k=2','k=3','k=4']]\n",
    "i=0\n",
    "batch = list(res_file[start_list[i]: end_list[i]+1])\n",
    "acc_p = [['','Model:']]\n",
    "acc_p.append(['n','Preposition'])\n",
    "prep_table = get_acc_p_prep_data(batch)\n",
    "for item in prep_table:\n",
    "    acc_p.append([item[0],item[1]])\n",
    "#\n",
    "for i in range(no_batches):\n",
    "    batch = list(res_file[start_list[i]: end_list[i]+1])\n",
    "    batch_model = get_model(batch)\n",
    "    # get acc_k\n",
    "    acc_k.append([batch_model] + get_acc_k(batch))\n",
    "    #\n",
    "    # get acc_p\n",
    "    acc_p[0]+=[batch_model, batch_model]\n",
    "    acc_p[1]+=['count','acc_p']\n",
    "    batch_table = get_acc_p_prep_data(batch)\n",
    "    for j,item in enumerate(batch_table):\n",
    "        assert(acc_p[j+2][1] == item[1]), \"wrong preposition label\"\n",
    "        if item[3] == '-1.000':\n",
    "            item[3]='-'\n",
    "        acc_p[j+2] += [str(int(float(item[2]))),item[3]]\n",
    "#\n",
    "# print table\n",
    "print \"acc_k table\"\n",
    "for row in acc_k:\n",
    "    for col in row:\n",
    "        print \"%6s\"%col,\n",
    "    print\n",
    "print\n",
    "acc_p[-1][0]=''\n",
    "acc_p[-1][1]='wt_mean'\n",
    "print \"acc_p(0) table\"\n",
    "for row in acc_p: \n",
    "    print \"%3s %18s \"%(row[0],row[1]),\n",
    "    for col in row[2:]:\n",
    "        print \"%6s\"%col,\n",
    "    print\n",
    "\n",
    "save_flag = True\n",
    "if save_flag:\n",
    "    out_file = open('summary_2017.txt','w')\n",
    "    out_file.write(\"=========================================================================\\n\")\n",
    "    out_file.write(\"acc_k table\\n\")\n",
    "    for row in acc_k:\n",
    "        for col in row:\n",
    "            out_file.write(\"%6s\"%col)\n",
    "        out_file.write('\\n')\n",
    "    out_file.write('\\n')\n",
    "    #                                           \n",
    "    acc_p[-1][0]=''\n",
    "    acc_p[-1][1]='wt_mean'\n",
    "    out_file.write(\"acc_p(0) table\\n\")\n",
    "    for row in acc_p: \n",
    "        gap = 0\n",
    "        out_file.write(\"%3s %18s \"%(row[0],row[1]))\n",
    "        for col in row[2:]:\n",
    "            out_file.write(\"%6s\"%col)\n",
    "            gap += 1\n",
    "            if gap%2==0:\n",
    "                out_file.write(' ')\n",
    "        out_file.write('\\n')\n",
    "        if row==acc_p[-2] or row==acc_p[1]:\n",
    "                out_file.write(\"=========================================================================\\n\")\n",
    "\n",
    "\n",
    "    out_file.write(\"=========================================================================\\n\")\n",
    "    out_file.write(\"\\n\")\n",
    "    #\n",
    "    out_file.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version_2\n",
    "#\n",
    "#================================================================================================\n",
    "model = 'kNN'\n",
    "data_set = 'expanded'\n",
    "#data_set = 'best'\n",
    "#================================================================================================\n",
    "#\n",
    "#================================================================================================\n",
    "# set hyper parameter grid\n",
    "\n",
    "if model =='kNN':\n",
    "    my_k = range(1,30,1)\n",
    "    hp_set = []\n",
    "    for K in my_k:\n",
    "        hp_set.append([K])\n",
    "#\n",
    "if model == 'LR':\n",
    "    my_C=[.1,  1, 10, 100]\n",
    "    my_tol=[.01, 0.001, .0001]\n",
    "    hp_set =[]\n",
    "    for C in my_C:\n",
    "        for tol in my_tol:\n",
    "            hp_set.append([C,tol])\n",
    "#\n",
    "if model == 'SVM':\n",
    "    my_C=[.1,  1, 10]\n",
    "    my_gamma = [.1, .01]\n",
    "    my_tol=[.01, 0.001,]\n",
    "    hp_set =[]\n",
    "    for C in my_C:\n",
    "        for g in my_gamma:\n",
    "            for tol in my_tol:\n",
    "                hp_set.append([C,g, tol])\n",
    "#\n",
    "if model=='DT':\n",
    "    Tree_depth=[5, 6, 7, 8, 9]\n",
    "    Min_samples_leaf =[1,2, 4, 6]\n",
    "    Min_samples_split = [2, 4, 6]\n",
    "\n",
    "    Tree_depth=[7, 8]\n",
    "    Min_samples_leaf =[1, 2, 3]\n",
    "    Min_samples_split = [2, 3, 4]\n",
    "    \n",
    "    hp_set =[]\n",
    "    for D in Tree_depth:\n",
    "        for L in Min_samples_leaf:\n",
    "            for S in Min_samples_split:\n",
    "                hp_set.append([D, S, L])\n",
    "#\n",
    "if model=='RF':\n",
    "    #n_estimators = [50, 60, 70, 90] \n",
    "    n_estimators = [100, 110] \n",
    "    #max_features =  [20, 30, 40, 50]\n",
    "    max_features =  [40]\n",
    "    #max_depth    = [ 7, 8, 9]\n",
    "    max_depth    = [ 9]\n",
    "    min_samples_leaf  = [1, 2, 3]\n",
    "    min_samples_split = [8, 10, 12]\n",
    "    hp_set=[]\n",
    "    for n in n_estimators:\n",
    "        for mf in max_features:\n",
    "            for D in max_depth:\n",
    "                for msl in min_samples_leaf:\n",
    "                    for mss in min_samples_split:\n",
    "                        hp_set.append([n, mf, D, msl, mss])\n",
    "#\n",
    "if model == 'NN':\n",
    "    alpha = [1e-3, 1e-4, 1e-5]\n",
    "    H1 = [10, 17, 25]\n",
    "    H2 = [10, 17, 25]\n",
    "    tol = [.01, .001, .0001]\n",
    "    hp_set=[]\n",
    "    for a in alpha:\n",
    "        for h1 in H1:\n",
    "            for h2 in H2:\n",
    "                for t in tol:\n",
    "                    hp_set.append([a, h1, h2, t])\n",
    "#\n",
    "#================================================================================================\n",
    "#\n",
    "#print \"train, dev, test, tr_Best  tr_All  Ts_Best   Ts_ALL\"\n",
    "results = np.zeros((K_cv+1,4),dtype=float)  # the last row is for computing the average\n",
    "#res_fold ={}\n",
    "cres_fold ={}\n",
    "\n",
    "for j_dev in range(K_cv):\n",
    "    #================================================================================================\n",
    "    # select folds\n",
    "    all_folds = range(K_cv)\n",
    "    j_test = (j_dev + 1) % K_cv\n",
    "    all_folds.remove(j_test)\n",
    "    all_folds.remove(j_dev)\n",
    "    train_folds = list(all_folds)  # all_folds -dev -test\n",
    "    dev_folds = [j_dev]\n",
    "    test_folds = [j_test]\n",
    "    test_folds = dev_folds\n",
    "    print \"TEST fold = \",test_folds,\n",
    "    #res_fold[str(test_folds)]=np.zeros((len(hp_set),6),dtype=float)\n",
    "    cres_fold[str(test_folds)] = np.zeros((len(hp_set)),dtype=[('hp_val','U50'),('tr_sngl',float),\n",
    "                                                                             ('tr_mlti',float),\n",
    "                                                                             ('ts_sngl',float),\n",
    "                                                                             ('ts_mlti',float)])\n",
    "\n",
    "    setup_TDT_sets(train_folds, dev_folds, test_folds)\n",
    "    #================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    scale_feat = True\n",
    "    if 'geometric' in feat_head and scale_feat:\n",
    "        scaler = StandardScaler()\n",
    "        col_0, col_1 = feat_head_col['geometric']\n",
    "        scaler.fit(x_b_train[:,col_0:col_1])  # training with best\n",
    "        x_b_train[:,col_0:col_1] = scaler.transform(x_b_train[:,col_0:col_1])  \n",
    "        x_b_dev[:,col_0:col_1] = scaler.transform(x_b_dev[:,col_0:col_1])\n",
    "        x_b_test[:,col_0:col_1] = scaler.transform(x_b_test[:,col_0:col_1])\n",
    "        x_a_train[:,col_0:col_1] = scaler.transform(x_a_train[:,col_0:col_1])  \n",
    "        x_a_dev[:,col_0:col_1] = scaler.transform(x_a_dev[:,col_0:col_1])\n",
    "        x_a_test[:,col_0:col_1] = scaler.transform(x_a_test[:,col_0:col_1])\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    # ================================================================================================\n",
    "    #\n",
    "    if data_set == 'expanded': \n",
    "        x_train = x_a_train\n",
    "        y_train = y_a_train\n",
    "        Y_train = Y_a_train\n",
    "        x_test = x_a_dev\n",
    "        yy_test = y_a_dev\n",
    "        Y_test = Y_a_dev\n",
    "    if data_set == 'best':\n",
    "        x_train = x_b_train\n",
    "        y_train = y_b_train\n",
    "        Y_train = Y_b_train\n",
    "        x_test = x_b_dev\n",
    "        yy_test = y_b_dev\n",
    "        Y_test = Y_b_dev        \n",
    "    #\n",
    "# ================================================================================================\n",
    "#\n",
    "\n",
    "\n",
    "    print \"                     TR_sngl  TR_exp  TS_sngl   TS_EXP\"\n",
    "    for i_hp,hp in enumerate(hp_set):\n",
    "        # learn model\n",
    "        ff=\"\"\n",
    "        for hh in hp:\n",
    "            ff += \"%10.5f\" % (hh)\n",
    "        #ff = \"%10.5f%10.5f\" % (hp[0], hp[1])\n",
    "        cres_fold[str(test_folds)][i_hp]['hp_val'] = ff \n",
    "        print \"hp = %30s :\" % str(hp),\n",
    "        #my_C=hp[0]\n",
    "        #my_tol=hp[1]\n",
    "        #model_lr_1 = train_model('LR', x_b_train, y_b_train, {'C':hp[0], 'tol':hp[1]})\n",
    "        #model_lr_1 = train_model('LR', x_train, y_train, {'C':hp[0], 'tol':hp[1]})\n",
    "\n",
    "        h_param = {}\n",
    "        if model == 'LR':\n",
    "            h_param['LR'] = {'C':hp[0], 'tol':hp[1]}\n",
    "        if model == 'DT':\n",
    "            h_param['DT'] = {'tree_depth':hp[0], 'min_samples_split':hp[1], 'min_samples_leaf':hp[2]}\n",
    "        if model == 'RF':\n",
    "            h_param['RF'] = {'n_estimators':hp[0], 'max_features':hp[1],'max_depth':hp[2],\n",
    "                             'min_samples_leaf':hp[3], 'min_samples_split':hp[4]}\n",
    "        if model == 'SVM':\n",
    "            h_param['SVM'] = {'C':hp[0], 'gamma':hp[1], 'tol':hp[2]}\n",
    "        if model == 'kNN':\n",
    "            h_param['kNN'] = {'n_neighbors':hp[0]}\n",
    "        if model == 'NN':\n",
    "            h_param['NN'] = {'alpha':hp[0], 'H1':hp[1], 'H2':hp[2], 'tol':hp[3]}\n",
    "        #\n",
    "        #\n",
    "        model_exp = train_model(model, x_train, y_train, h_param[model]) \n",
    "\n",
    "\n",
    "#         if model == 'LR':\n",
    "#             #model_exp = train_model('LR', x_a_train, y_a_train, {'C':10, 'tol':.001}) \n",
    "#             model_exp = train_model('LR', x_train, y_train, {'C':hp[0], 'tol':hp[1]}) \n",
    "\n",
    "#         if model == 'DT':\n",
    "#             model_exp = train_model(model, x_train, y_train, {'tree_depth':hp[0]}) \n",
    "        \n",
    "        #print model_lr_1\n",
    "        #\n",
    "        #\n",
    "        #res_fold[str(test_folds)][i_hp,0]=hp[0]\n",
    "        #res_fold[str(test_folds)][i_hp,1]=hp[1]\n",
    "        y_p = model_exp.predict_proba(x_train)\n",
    "        y_test = y_train\n",
    "        y_all_test = Y_train['all_code']\n",
    "        #Y_pred=get_pred_vector(y_p)\n",
    "        #print \"shape Y_pred :\", Y_pred.shape\n",
    "        #res_fold[str(test_folds)][i_hp,2] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "        cres_fold[str(test_folds)][i_hp]['tr_sngl'] = get_recall_at_k(y_p, y_test, 1)\n",
    "        #results[j_dev,0] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "        #res_fold[str(test_folds)][i_hp,3] = get_acc_all(Y_pred,y_all_test)\n",
    "        cres_fold[str(test_folds)][i_hp]['tr_mlti'] = get_recall_for_all_at_k(y_p, y_all_test, 1)\n",
    "        #cres_fold[str(test_folds)][i_hp]['tr_mlti'] = get_acc_all(Y_pred,y_all_test)\n",
    "        #results[j_dev,1] = get_acc_all(Y_pred,y_all_test)\n",
    "        #print \" %5.3f    %5.3f   \"%(get_recall_at_k(Y_pred, y_test, 1), get_acc_all(Y_pred,y_all_test)),\n",
    "\n",
    "        print \" %5.3f    %5.3f   \"%(cres_fold[str(test_folds)][i_hp]['tr_sngl'],\\\n",
    "                                    cres_fold[str(test_folds)][i_hp]['tr_mlti']),\n",
    "        #print\n",
    "        y_p = model_exp.predict_proba(x_test)\n",
    "        y_test = yy_test\n",
    "        y_all_test = Y_test['all_code']\n",
    "        #Y_pred=get_pred_vector(y_p)\n",
    "        #res_fold[str(test_folds)][i_hp,4] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "        cres_fold[str(test_folds)][i_hp]['ts_sngl'] = get_recall_at_k(y_p, y_test, 1)\n",
    "\n",
    "        #results[j_dev,2] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "        #res_fold[str(test_folds)][i_hp,5] = get_acc_all(Y_pred,y_all_test)\n",
    "        cres_fold[str(test_folds)][i_hp]['ts_mlti'] = get_recall_for_all_at_k(y_p, y_all_test, 1)\n",
    "\n",
    "        #results[j_dev,3] = get_acc_all(Y_pred,y_all_test)\n",
    "        #print \"%5.3f   %5.3f   \"%(get_recall_at_k(Y_pred, y_test, 1), get_acc_all(Y_pred,y_all_test))\n",
    "        print \"%5.3f   %5.3f   \"%(cres_fold[str(test_folds)][i_hp]['ts_sngl'],\n",
    "                                  cres_fold[str(test_folds)][i_hp]['ts_mlti']),\n",
    "        print \"%5.3f\"%(cres_fold[str(test_folds)][i_hp]['ts_mlti']/cres_fold[str(test_folds)][i_hp]['tr_mlti'])\n",
    "\n",
    "#    print \"=====================================================\"\n",
    "#    results[-1,:] = np.mean(results[0:-1,:],axis=0)\n",
    "#    print \"                   %5.3f    %5.3f   %5.3f   %5.3f   \"%(results[-1,0], results[-1,1], results[-1,2], results[-1,3])\n",
    "print \"HPO...COMPLETED\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 2:\n",
    "# Rank HP sets\n",
    "# for idx in range(K_cv):\n",
    "#     print \"fold :\",idx\n",
    "#     cres_fold['['+str(idx)+']'].sort(order='ts_sngl')\n",
    "#     #print cres_fold['['+str(idx)+']']\n",
    "    \n",
    "N=cres_fold['[0]'].shape[0]  # number of unique parameter sets\n",
    "print \"Number of unique HP parameter sets =\",N\n",
    "hp_wt = {}\n",
    "for idx in range(K_cv):  # for each fold\n",
    "    #print \"fold :\",idx\n",
    "    cres_fold['['+str(idx)+']'].sort(order='ts_mlti')  # choose metric for ranking\n",
    "    for i in range(N):        \n",
    "        ff =  cres_fold['['+str(idx)+']'][i]['hp_val']\n",
    "        if ff not in hp_wt:\n",
    "            hp_wt[ff]=[]\n",
    "        hp_wt[ff].append(float(i)/float(N))\n",
    "#\n",
    "#f_rank = np.zeros((N,3),dtype=float)\n",
    "f_rank = np.zeros((N),dtype=[('hp_val','U50'),('weight',float)])\n",
    "for i,ff in enumerate(hp_wt.keys()):\n",
    "    #print \"%30s:   %5.3f\"%(ff, sum(hp_wt[ff]))\n",
    "    f_rank[i]['hp_val'] = ff\n",
    "    f_rank[i]['weight']=sum(hp_wt[ff])\n",
    "f_rank.sort(order='weight')\n",
    "print\n",
    "\n",
    "for i in range(f_rank.shape[0]):\n",
    "    print \"%50s:   %5.3f\"%(f_rank[i]['hp_val'], f_rank[i]['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect acc(k) results\n",
    "acc_k = []\n",
    "max_rows = len(res_file)\n",
    "row=0\n",
    "while row<max_rows: \n",
    "    item_list = res_file[row].split()\n",
    "    if len(item_list) >0:\n",
    "        if item_list[0] == 'Model':\n",
    "            entry =[item_list[2][0:2]]\n",
    "            while item_list[0] != 'acc(k)':\n",
    "                row +=1\n",
    "                item_list = res_file[row].split()\n",
    "\n",
    "            entry += [item_list[2],item_list[4], item_list[6], item_list[8]]\n",
    "            #print item_list\n",
    "            acc_k.append(entry)    \n",
    "    #\n",
    "    row +=1\n",
    "acc_k\n",
    "#\n",
    "# collect acc(k) results\n",
    "acc_k = []\n",
    "max_rows = len(res_file)\n",
    "max_rows = 42\n",
    "row=14\n",
    "while row<max_rows: \n",
    "    item_list = res_file[row].split()\n",
    "    print item_list\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_predict.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_recall_tested[0,-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(res_recall_tested[:,-1,:], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_recall_tested[:,-1,:] = np.sum(res_recall_tested[:,0:-1,:], axis=1)\n",
    "res_recall_correct[:,-1,:] = np.sum(res_recall_correct[:,0:-1,:], axis=1)\n",
    "recall_per_p_at_k = np.copy(res_recall_correct[:,-1,:]/res_recall_tested[:,-1,:])\n",
    "recall_per_p_wt_mean = np.copy(np.sum(recall_per_p*res_recall_tested[:,-1,:]/\n",
    "                               np.sum(res_recall_tested[:,-1,:], axis=1, keepdims=True), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_per_p_wt_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_per_p_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_per_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over all folds\n",
    "p_tested = np.sum(res_prep_predict, axis=0)\n",
    "p_correct = np.sum(res_prep_correct, axis=0)\n",
    "N=p_predict.shape[0]\n",
    "acc_p_0 = np.zeros((N),dtype=float)\n",
    "for n in range(N):\n",
    "    if p_predict[n] > 0:\n",
    "        acc_p_0[n] = float(p_correct[n])/float(p_predict[n])\n",
    "    else:\n",
    "        acc_p_0[n] = -1.\n",
    "results_acc_p[-1,:] = np.copy(acc_p_0)\n",
    "acc_p_0_wt_mean = np.sum(acc_p_0*p_predict/p_predict.sum())\n",
    "print \"mean = \",acc_p_0_wt_mean\n",
    "for j in range(N):\n",
    "    print '%2d %-25s '%(j,prep_label_enc.inverse_transform(j)+'('+str(p_predict[j])+')'),\n",
    "    #print '%2d %-14s '%(j,''),\n",
    "\n",
    "    for i in range(results_acc_p.shape[0]):\n",
    "        if results_acc_p[i,j]==-1:\n",
    "            print \"%5s \"%\" - \",\n",
    "        else:\n",
    "            print \"%5.3f \"%results_acc_p[i,j],\n",
    "    print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over all folds\n",
    "p_predict = np.sum(res_prep_predict, axis=0)\n",
    "p_correct = np.sum(res_prep_correct, axis=0)\n",
    "N=p_predict.shape[0]\n",
    "acc_p_0 = np.zeros((N),dtype=float)\n",
    "for n in range(N):\n",
    "    if p_predict[n] > 0:\n",
    "        acc_p_0[n] = float(p_correct[n])/float(p_predict[n])\n",
    "    else:\n",
    "        acc_p_0[n] = -1.\n",
    "results_acc_p[-1,:] = np.copy(acc_p_0)\n",
    "acc_p_0_wt_mean = np.sum(acc_p_0*p_predict/p_predict.sum())\n",
    "print \"mean = \",acc_p_0_wt_mean\n",
    "for j in range(N):\n",
    "    print '%2d %-25s '%(j,prep_label_enc.inverse_transform(j)+'('+str(p_predict[j])+')'),\n",
    "    #print '%2d %-14s '%(j,''),\n",
    "\n",
    "    for i in range(results_acc_p.shape[0]):\n",
    "        if results_acc_p[i,j]==-1:\n",
    "            print \"%5s \"%\" - \",\n",
    "        else:\n",
    "            print \"%5.3f \"%results_acc_p[i,j],\n",
    "    print\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-1+-1+.3333+1)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_prep_correct[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_prep_predict[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_prep_correct[:,1].sum()/res_prep_predict[:,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5./19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Loop on BEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"FEATURE SET : \", feat_head\n",
    "print \"                    train, dev, test,  tr_Best  tr_EXP  b_sngl  b_mlti    e_singl  e_mlti\"\n",
    "results = np.zeros((K_cv+1,6),dtype=float)\n",
    "res_recall = np.zeros((K_cv+1,4),dtype=float)\n",
    "res_recall_all = np.zeros((K_cv+1,4),dtype=float)\n",
    "res_conf_mat = np.zeros((K_cv+1,len(prep_label_enc.classes_),len(prep_label_enc.classes_)),dtype=float)\n",
    "for j_test in range(K_cv):\n",
    "    # select folds\n",
    "    all_folds = range(K_cv)\n",
    "    #j_test = (j_dev + 1) % K_cv\n",
    "    all_folds.remove(j_test)\n",
    "    #all_folds.remove(j_dev)\n",
    "    train_folds = list(all_folds)\n",
    "    dev_folds = [j_test] # dummy\n",
    "    test_folds = [j_test]\n",
    "    print \"TEST fold:%4s\"%str(test_folds),\":\",\n",
    "    #res_fold[str(test_folds)]=np.zeros((len(hp_set),6),dtype=float)\n",
    "    print \"%20s\"%(str(train_folds)+ str(dev_folds)+ str(test_folds)),\n",
    "\n",
    "    #setup_TDT_sets(train_folds+dev_folds, dev_folds, test_folds )\n",
    "    setup_TDT_sets(train_folds, dev_folds, test_folds )\n",
    "\n",
    "    # ================================================================================================\n",
    "    scale_feat = True\n",
    "    if 'geometric' in feat_head and scale_feat:\n",
    "        scaler = StandardScaler()\n",
    "        col_0, col_1 = feat_head_col['geometric']\n",
    "        scaler.fit(x_b_train[:,col_0:col_1])  # training with best\n",
    "        x_b_train[:,col_0:col_1] = scaler.transform(x_b_train[:,col_0:col_1])  \n",
    "        x_b_dev[:,col_0:col_1] = scaler.transform(x_b_dev[:,col_0:col_1])\n",
    "        x_b_test[:,col_0:col_1] = scaler.transform(x_b_test[:,col_0:col_1])\n",
    "        x_a_train[:,col_0:col_1] = scaler.transform(x_a_train[:,col_0:col_1])  \n",
    "        x_a_dev[:,col_0:col_1] = scaler.transform(x_a_dev[:,col_0:col_1])\n",
    "        x_a_test[:,col_0:col_1] = scaler.transform(x_a_test[:,col_0:col_1])\n",
    "    # ================================================================================================\n",
    "    model = 'LR'\n",
    "    \n",
    "    if model == 'LR':\n",
    "        model_best = train_model('LR', x_b_train, y_b_train, {'C':10, 'tol':.001}) #train on best\n",
    "    #\n",
    "    # test on BEST_TRAIN_SINGLE\n",
    "    y_p = model_best.predict_proba(x_b_train)\n",
    "    y_test = y_b_train\n",
    "    y_all_test = Y_b_train['all_code']\n",
    "    #Y_pred=get_pred_vector(y_p)    \n",
    "    #results[j_test,0] = get_recall_for_all_at_k(y_p, y_test, 1)\n",
    "    results[j_test,0] = get_recall_at_k(y_p, y_test, 1)\n",
    "    #results[j_test,1] = get_acc_all(Y_pred,y_all_test)\n",
    "    results[j_test,1] = get_recall_for_all_at_k(y_p, y_all_test, 1)\n",
    "    #print \" %5.3f    %5.3f   \"%(get_recall_at_k(y_p, y_test, 1), get_recall_for_all_at_k(y_p, y_all_test, 1)),\n",
    "    print \" %5.3f    %5.3f   \"%(results[j_test,0], results[j_test,1]),\n",
    "\n",
    "    #get_acc_all(Y_pred,y_all_test)),\n",
    "    #\n",
    "    # test on BEST_TEST_SINGLE and BEST_TEST_MULTI\n",
    "    y_p = model_best.predict_proba(x_b_test)\n",
    "    y_test = y_b_test\n",
    "    y_all_test = Y_b_test['all_code']   \n",
    "    results[j_test,2] = get_recall_at_k(y_p, y_test, 1)\n",
    "    for r in range(4):\n",
    "        res_recall[j_test,r] = get_recall_at_k(y_p, y_test, r+1)\n",
    "        res_recall_all[j_test,r] = get_recall_for_all_at_k(y_p, y_all_test, r+1)\n",
    "    results[j_test,3] = get_recall_for_all_at_k(y_p, y_all_test, 1)\n",
    "    #Y_pred=get_pred_vector(y_p)\n",
    "    #res_conf_mat[j_test] = np.copy(confusion_matrix(y_test, Y_pred))\n",
    "    print \"%5.3f   %5.3f   \"%(results[j_test,2], results[j_test,3]),\n",
    "    acc_p, global_mean, global_wt_mean, acc_p_mean, acc_p_wt_mean = get_precision_per_prep(y_p, y_all_test)\n",
    "\n",
    "    #\n",
    "    # test on EXPANDED_SINGLE and EXPANDED_MULTI\n",
    "    y_p = model_best.predict_proba(x_a_test)\n",
    "    y_test = y_a_test\n",
    "    y_all_test = Y_a_test['all_code']\n",
    "    #Y_pred=get_pred_vector(y_p)\n",
    "    results[j_test,4] = get_recall_at_k(y_p, y_test, 1)\n",
    "    results[j_test,5] = get_recall_for_all_at_k(y_p, y_all_test, 1)\n",
    "    print \"%5.3f   %5.3f   \"%(results[j_test,4], results[j_test,5] )\n",
    "\n",
    "\n",
    "print \"==========================================================================================\"\n",
    "results[-1,:] = np.mean(results[0:-1,:],axis=0)\n",
    "print \"                                       %5.3f    %5.3f    %5.3f   %5.3f    %5.3f   %5.3f   \"%(results[-1,0],\\\n",
    "                                                    results[-1,1], results[-1,2], results[-1,3],\\\n",
    "                                                    results[-1,4], results[-1,5])\n",
    "\n",
    "#\n",
    "print\"b_BEST_TEST: recall@k results: \"\n",
    "print\"================================\"\n",
    "res_recall[-1,:] = np.mean(res_recall[0:-1,:],axis=0)\n",
    "for j in range(K_cv+1):\n",
    "    for r in range(4):\n",
    "        print \"%5.3f  \"%(res_recall[j,r]),\n",
    "    print\n",
    "    if j==K_cv-1:\n",
    "        print\"================================\"\n",
    "print\"================================\"\n",
    "print\n",
    "print\"b_ALL_TEST: recall@k results: \"\n",
    "print\"================================\"\n",
    "res_recall_all[-1,:] = np.mean(res_recall_all[0:-1,:],axis=0)\n",
    "for j in range(K_cv+1):\n",
    "    for r in range(4):\n",
    "        print \"%5.3f  \"%(res_recall_all[j,r]),\n",
    "    print\n",
    "    if j==K_cv-1:\n",
    "        print\"================================\"\n",
    "print\"================================\"\n",
    "#\n",
    "# Confusion matrix\n",
    "#\n",
    "# Save to file\n",
    "## Save to file\n",
    "save_flag = True\n",
    "if save_flag:\n",
    "    out_file = open('results.txt','a')\n",
    "    out_file.write('/START RUN =========================================================================\\n')\n",
    "    out_file.write('Model : ' + str(model)+'\\n')\n",
    "    out_file.write('Features : ' + str(feat_head)+'\\n')\n",
    "    #out_file.write(get_csv_string(line_entry)+'\\n')\n",
    "    out_file.write('/END RUN =========================================================================\\n')\n",
    "    out_file.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = res_conf_mat.sum(axis=0)\n",
    "print cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_all_test[0],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALL per preposition\n",
    "print model_exp\n",
    "# test on BEST_TRAIN\n",
    "y_p = model_exp.predict_proba(x_b_train)\n",
    "y_test = y_b_train\n",
    "y_all_test = Y_b_train['all_code']\n",
    "Y_pred=get_pred_vector(y_p)\n",
    "#print \"shape Y_pred :\", Y_pred.shape\n",
    "#res_fold[str(test_folds)][i_hp,2] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "results[j_test,0] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "#res_fold[str(test_folds)][i_hp,3] = get_acc_all(Y_pred,y_all_test)\n",
    "results[j_test,1] = get_acc_all(Y_pred,y_all_test)\n",
    "print \" %5.3f    %5.3f   \"%(get_recall_at_k(Y_pred, y_test, 1), get_acc_all(Y_pred,y_all_test)),\n",
    "#print\n",
    "    \n",
    "get_recall_per_prep_at_k(y_p, y_test.reshape((y_test.shape[0],1)), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4260/11200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows are expected = sum of a row\n",
    "cc=confusion_matrix(y_test, Y_pred)\n",
    "c=cc\n",
    "ci=np.array(c,dtype=float)\n",
    "d=100.0*ci/ci.sum(axis=1, keepdims=True)\n",
    "t = c.sum(axis=1)\n",
    "#c=c/np.sum(c,axis=1,dtype=float)*100\n",
    "#for j in range(c.shape[0]):\n",
    "#    c[j,:] = c[j,:]/float(np.sum(c[j]))*100\n",
    "   \n",
    "print '                         ',\n",
    "for i in range(d.shape[1]):\n",
    "    print \"%3d \"%i,\n",
    "print\n",
    "print\n",
    "for j in range(d.shape[0]):\n",
    "    print '%2d %-22s '%(j,prep_label_enc.inverse_transform(j)+'('+str(t[j])+')'),\n",
    "    #print '%2d %-14s '%(j,''),\n",
    "\n",
    "    for i in range(d.shape[1]):\n",
    "        if d[j,i]==0.0:\n",
    "            print \" -  \",\n",
    "        else:\n",
    "            print \"%3.0f \"%d[j,i],\n",
    "    print\n",
    "    print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_mat(c):\n",
    "    ci=np.array(c,dtype=float)\n",
    "    d=100.0*ci/ci.sum(axis=1, keepdims=True)\n",
    "    t = c.sum(axis=1)\n",
    "    #  \n",
    "    print '                         ',\n",
    "    for i in range(d.shape[1]):\n",
    "        print \"%3d \"%i,\n",
    "    print\n",
    "    print\n",
    "    for j in range(d.shape[0]):\n",
    "        print '%2d %-22s '%(j,prep_label_enc.inverse_transform(j)+'('+str(t[j])+')'),\n",
    "        #print '%2d %-14s '%(j,''),\n",
    "\n",
    "        for i in range(d.shape[1]):\n",
    "            if d[j,i]==0.0:\n",
    "                print \" -  \",\n",
    "            else:\n",
    "                print \"%3.0f \"%d[j,i],\n",
    "        print\n",
    "        print\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_lang_geo_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_lang_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_lang_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_geo_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cc_lang_depth = np.copy(cc)\n",
    "#cc_lang_geo_depth = np.copy(cc)\n",
    "#cc_geo_depth = np.copy(cc)\n",
    "#cc_geo = np.copy(cc)\n",
    "#cc_lang_geo = np.copy(cc)\n",
    "#cc_lang = np.copy(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print d.sum(axis=1)\n",
    "print t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cii=np.array(ci,dtype=float)\n",
    "dii=cii/cii.sum(axis=1, keepdims=True)\n",
    "dii.sum(axis=1)\n",
    "ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Loop on ALL set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"FEATURE SET : \", feat_head\n",
    "print \"                    train, dev, test,  tr_Best  tr_All  b_Best  b_ALL    e_singl  e_ALL\"\n",
    "results = np.zeros((K_cv+1,6),dtype=float)\n",
    "res_recall = np.zeros((K_cv+1,4),dtype=float)\n",
    "res_recall_all = np.zeros((K_cv+1,4),dtype=float)\n",
    "res_conf_mat = np.zeros((K_cv+1,len(prep_label_enc.classes_),len(prep_label_enc.classes_)),dtype=float)\n",
    "for j_test in range(K_cv):\n",
    "    # select folds\n",
    "    all_folds = range(K_cv)\n",
    "    all_folds.remove(j_test)\n",
    "    train_folds = list(all_folds)\n",
    "    dev_folds = [j_test] # dummy\n",
    "    test_folds = [j_test]\n",
    "    print \"TEST fold:%4s\"%str(test_folds),\":\",\n",
    "    #res_fold[str(test_folds)]=np.zeros((len(hp_set),6),dtype=float)\n",
    "    print \"%20s\"%(str(train_folds)+ str(dev_folds)+ str(test_folds)),\n",
    "\n",
    "    #setup_TDT_sets(train_folds+dev_folds, dev_folds, test_folds )\n",
    "    setup_TDT_sets(train_folds, dev_folds, test_folds )\n",
    "\n",
    "    # ================================================================================================\n",
    "    if 'geometric' in feat_head:\n",
    "        scaler = StandardScaler()\n",
    "        col_0 = X_best_fold['fold_0']['X_headings']['geometric'][0]\n",
    "        col_1 = X_best_fold['fold_0']['X_headings']['geometric'][1]\n",
    "        #col_0 = 0\n",
    "        #col_1 = 18\n",
    "        scaler.fit(x_b_train[:,col_0:col_1])  # training with best\n",
    "        x_b_train[:,col_0:col_1] = scaler.transform(x_b_train[:,col_0:col_1])  \n",
    "        x_b_test[:,col_0:col_1] = scaler.transform(x_b_test[:,col_0:col_1])\n",
    "        x_a_train[:,col_0:col_1] = scaler.transform(x_a_train[:,col_0:col_1])  \n",
    "        x_a_test[:,col_0:col_1] = scaler.transform(x_a_test[:,col_0:col_1])\n",
    "    # ================================================================================================\n",
    "\n",
    "    model_lr_1 = train_model('LR', x_a_train, y_a_train, {'C':10, 'tol':.001})\n",
    "\n",
    "\n",
    "    # test on BEST_TRAIN\n",
    "    y_p = model_lr_1.predict_proba(x_a_train)\n",
    "    y_test = y_a_train\n",
    "    y_all_test = Y_a_train['all_code']\n",
    "    Y_pred=get_pred_vector(y_p)\n",
    "    #print \"shape Y_pred :\", Y_pred.shape\n",
    "    #res_fold[str(test_folds)][i_hp,2] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "    results[j_test,0] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "    #res_fold[str(test_folds)][i_hp,3] = get_acc_all(Y_pred,y_all_test)\n",
    "    results[j_test,1] = get_acc_all(Y_pred,y_all_test)\n",
    "    print \" %5.3f    %5.3f   \"%(get_recall_at_k(Y_pred, y_test, 1), get_acc_all(Y_pred,y_all_test)),\n",
    "    #print\n",
    "    #\n",
    "    # test on BEST_TEST\n",
    "    y_p = model_lr_1.predict_proba(x_b_test)\n",
    "    y_test = y_b_test\n",
    "    y_all_test = Y_b_test['all_code']\n",
    "    Y_pred=get_pred_vector(y_p)\n",
    "    results[j_test,2] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "    #for r in range(4):\n",
    "    #    res_recall[j_test,r] = get_recall_at_k(Y_pred, y_test, r+1)\n",
    "    #    res_recall_all[j_test,r] = get_recall_for_all_at_k(Y_pred, y_all_test, r+1)\n",
    "    results[j_test,3] = get_acc_all(Y_pred,y_all_test)\n",
    "    #res_conf_mat[j_test] = np.copy(confusion_matrix(y_test, Y_pred))\n",
    "    \n",
    "    print \"%5.3f   %5.3f   \"%(get_recall_at_k(Y_pred, y_test, 1), get_acc_all(Y_pred,y_all_test)),\n",
    "    #\n",
    "    # test on ALL_TEST\n",
    "    y_p = model_lr_1.predict_proba(x_a_test)\n",
    "    y_test = y_a_test\n",
    "    y_all_test = Y_a_test['all_code']\n",
    "    Y_pred=get_pred_vector(y_p)\n",
    "    results[j_test,4] = get_recall_at_k(Y_pred, y_test, 1)\n",
    "    for r in range(4):\n",
    "        res_recall[j_test,r] = get_recall_at_k(Y_pred, y_test, r+1)\n",
    "        res_recall_all[j_test,r] = get_recall_for_all_at_k(Y_pred, y_all_test, r+1)\n",
    "    results[j_test,5] = get_acc_all(Y_pred,y_all_test)\n",
    "    res_conf_mat[j_test] = np.copy(confusion_matrix(y_test, Y_pred))\n",
    "    print \"%5.3f   %5.3f   \"%(get_recall_at_k(Y_pred, y_test, 1), get_acc_all(Y_pred,y_all_test))\n",
    "\n",
    "\n",
    "print \"==========================================================================================\"\n",
    "results[-1,:] = np.mean(results[0:-1,:],axis=0)\n",
    "print \"                                       %5.3f    %5.3f    %5.3f   %5.3f    %5.3f   %5.3f   \"%(results[-1,0],\\\n",
    "                                                    results[-1,1], results[-1,2], results[-1,3],\\\n",
    "                                                    results[-1,4], results[-1,5])\n",
    "\n",
    "#\n",
    "print\"b_BEST_TEST: recall@k results: \"\n",
    "print\"================================\"\n",
    "res_recall[-1,:] = np.mean(res_recall[0:-1,:],axis=0)\n",
    "for j in range(K_cv+1):\n",
    "    for r in range(4):\n",
    "        print \"%5.3f  \"%(res_recall[j,r]),\n",
    "    print\n",
    "    if j==K_cv-1:\n",
    "        print\"================================\"\n",
    "print\"================================\"\n",
    "print\n",
    "print\"b_ALL_TEST: recall@k results: \"\n",
    "print\"================================\"\n",
    "res_recall_all[-1,:] = np.mean(res_recall_all[0:-1,:],axis=0)\n",
    "for j in range(K_cv+1):\n",
    "    for r in range(4):\n",
    "        print \"%5.3f  \"%(res_recall_all[j,r]),\n",
    "    print\n",
    "    if j==K_cv-1:\n",
    "        print\"================================\"\n",
    "print\"================================\"\n",
    "#\n",
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = res_conf_mat.sum(axis=0)\n",
    "print cc.sum()/5.\n",
    "#cc_a_lang_geo = np.copy(cc)\n",
    "cc_aa_lang_geo = np.copy(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_aa_lang_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_mat(cc_a_lang_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_TDT_sets([0,1,2,3],[3],[4])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "col_0 = X_best_fold['fold_0']['X_headings']['geometric'][0]\n",
    "col_1 = X_best_fold['fold_0']['X_headings']['geometric'][1]\n",
    "#col_0 = 0\n",
    "#col_1 = x_b_train.shape[1]\n",
    "#print x_b_train[:,col_0: col_1].shape\n",
    "scaler.fit(x_b_train[:,col_0:col_1])  # training with best\n",
    "x_b_train[:,col_0:col_1] = scaler.transform(x_b_train[:,col_0:col_1])  \n",
    "x_b_test[:,col_0:col_1] = scaler.transform(x_b_test[:,col_0:col_1])\n",
    "x_a_train[:,col_0:col_1] = scaler.transform(x_a_train[:,col_0:col_1])  \n",
    "x_a_test[:,col_0:col_1] = scaler.transform(x_a_test[:,col_0:col_1])\n",
    "#print scaler.mean_\n",
    "#print scaler.var_\n",
    "# print \n",
    "# print \"Size of x_b_train :\", x_b_train.shape\n",
    "# print \"Size of y_b_train :\", y_b_train.shape\n",
    "# print \"Size of x_b_test :\", x_b_test.shape\n",
    "# print \"Size of y_b_test :\", y_b_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_C=100.0\n",
    "my_tol=0.0001\n",
    "model_lr_1=LogisticRegression(C=my_C,penalty='l2', tol=my_tol, multi_class='ovr',random_state=1971)\n",
    "model_lr_1.fit(x_b_train, y_b_train)\n",
    "print model_lr_1\n",
    "# predict probabilities for all classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_lr_1.predict_proba(x_b_train)\n",
    "y_test = y_b_train\n",
    "y_all_test = Y_b_train['all_code']\n",
    "Y_pred=get_pred_vector(y_p)\n",
    "print \"shape Y_pred :\", Y_pred.shape\n",
    "print 'Train Overall RECALL =',sum(((Y_pred.ravel()-y_test)**2)==0)/float((y_test.shape[0]))\n",
    "print 'Train Overall RECALL of all good preps =',get_acc_all(Y_pred,y_all_test)\n",
    "print\n",
    "y_p = model_lr_1.predict_proba(x_b_test)\n",
    "y_test = y_b_test\n",
    "y_all_test = Y_b_test['all_code']\n",
    "Y_pred=get_pred_vector(y_p)\n",
    "print \"shape Y_pred :\", Y_pred.shape\n",
    "print 'Test Overall RECALL =',sum(((Y_pred.ravel()-y_test)**2)==0)/float((y_test.shape[0]))\n",
    "print 'Test Overall RECALL of all good preps =',get_acc_all(Y_pred,y_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'NN'\n",
    "#\n",
    "if model == 'kNN':\n",
    "    model_kNN_1 = KNeighborsClassifier(n_neighbors=29)\n",
    "    model_kNN_1.fit(X_train, Y_train.ravel()) \n",
    "    #print model_kNN_1\n",
    "    # predict probabilities for all classes\n",
    "    print \"Testing Model :\"\n",
    "    y_p = model_kNN_1.predict_proba(X_test)\n",
    "\n",
    "if model=='NN':\n",
    "    #my_C=100.0\n",
    "    #my_tol=0.0001\n",
    "    model_nn_1 = MLPClassifier(solver='lbfgs', \n",
    "                               alpha=1e-5,\n",
    "                               max_iter=500,\n",
    "                               hidden_layer_sizes=(20,10),\n",
    "                               random_state=1971,\n",
    "                               )\n",
    "    model_nn_1.fit(x_b_train, y_b_train)\n",
    "    print model_nn_1\n",
    "    # predict probabilities for all classes\n",
    "    print \"Testing Model :\",\n",
    "    #            \n",
    "    y_p = model_nn_1.predict_proba(x_b_test)\n",
    "    #y_p = model_lr_1.predict_proba(X_valid)\n",
    "    print \"DONE\"\n",
    "    \n",
    "if model=='LR':\n",
    "    my_C=100.0\n",
    "    my_tol=0.0001\n",
    "    model_lr_1=LogisticRegression(C=my_C,penalty='l2', tol=my_tol, multi_class='ovr',random_state=1971)\n",
    "    model_lr_1.fit(x_b_train, y_b_train)\n",
    "    print model_lr_1\n",
    "    # predict probabilities for all classes\n",
    "    print \"Testing Model :\",\n",
    "    #            \n",
    "    y_p = model_lr_1.predict_proba(x_b_test)\n",
    "    #y_p = model_lr_1.predict_proba(X_valid)\n",
    "    print \"DONE\"\n",
    "#\n",
    "#\n",
    "if model=='DT':\n",
    "    Tree_depth=6\n",
    "    model_dt_1 = tree.DecisionTreeClassifier(max_depth=Tree_depth,random_state=1971)\n",
    "    model_dt_1.fit(X_train['X_model'], Y_train['Y_model'][:,0].ravel())\n",
    "    # predict probabilities for all classes\n",
    "    print model_dt_1\n",
    "    print \"Testing Model\"\n",
    "    y_p = model_dt_1.predict_proba(X_test['X_model'])\n",
    "#\n",
    "#\n",
    "if model=='RF':\n",
    "    model_rf_1 = RandomForestClassifier(n_estimators=80,  #40\n",
    "                                        max_features=30,  #40\n",
    "                                        max_depth=7,    #9\n",
    "                                        #min_samples_split=40,\n",
    "                                        min_samples_leaf=10,\n",
    "                                        random_state=1971,\n",
    "                                        )\n",
    "    #model_rf_1.fit(x_b_train, y_b_train)\n",
    "    model_rf_1.fit(x_a_train, y_a_train)\n",
    "    #\n",
    "    print model_rf_1\n",
    "    print \"Testing Model : \",    # predict probabilities for all classes\n",
    "    #y_p = model_rf_1.predict_proba(x_b_test) \n",
    "    y_p = model_rf_1.predict_proba(x_a_test) \n",
    "    print \"DONE\"\n",
    "\n",
    "if model=='SVM':\n",
    "    model_svm_1=svm.SVC(    C=1.0,\n",
    "                            gamma=.01,\n",
    "                            probability=True,\n",
    "                            kernel='rbf',\n",
    "                            tol=0.01,\n",
    "                            decision_function_shape = 'none',\n",
    "                            random_state=1971,\n",
    "                            #cache_size=1000\n",
    "                            #class_weight='balanced' \n",
    "                            #penalty='l1', \n",
    "                            #fit_intercept=True, \n",
    "                            #warm_start=False\n",
    "                            )\n",
    "    model_svm_1.fit(X_train, Y_train.ravel())\n",
    "\n",
    "    # predict probabilities for all classes\n",
    "    y_p = model_svm_1.predict_proba(X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_nn_1.predict(x_b_train)\n",
    "y_test = Y_b_train['Y_model'][:,18:35]\n",
    "print \"train dataset size :\",y_test.shape\n",
    "print \"train: over all annotations :\",np.sum(y_test*y_p)/float(np.sum(y_test))\n",
    "print \"train: one which is good :\",np.sum(y_test*y_p)/float(y_test.shape[0])\n",
    "#\n",
    "y_p = model_nn_1.predict(x_b_test)\n",
    "y_test = Y_b_test['Y_model'][:,18:35]\n",
    "print\n",
    "print \"test dataset size :\",y_test.shape\n",
    "print \"test: over all annotations :\",np.sum(y_test*y_p)/float(np.sum(y_test))\n",
    "print \"test: one which is good :\",np.sum(y_test*y_p)/float(y_test.shape[0])\n",
    "#\n",
    "# y_test = Y_b_test['Y_model'][:,1:18]\n",
    "# print\n",
    "# print \"test dataset size :\",y_test.shape\n",
    "# print \"test: over all annotations :\",np.sum(y_test*y_p)/float(np.sum(y_test))\n",
    "# print \"test: one which is good :\",np.sum(y_test*y_p)/float(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.sum(y_test*y_p,axis=1)/np.sum(y_test,axis=1)\n",
    "print np.sum(y_test)/a.shape[0]\n",
    "print np.sum(a)/a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = Y_b_test['Y_model'][:,18:35]\n",
    "print np.sum(y_test - y_p)**2\n",
    "print np.sum(y_test)\n",
    "print \"over all annotations :\",np.sum(y_test*y_p)/float(np.sum(y_test))\n",
    "print \"one which is good :\",np.sum(y_test*y_p)/float(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_nn_1.predict_proba(x_b_train)\n",
    "y_test = Y_b_train['Y_model'][:,18:35]\n",
    "#y_all_test = Y_b_train['all_code']\n",
    "Y_pred=get_pred_vector(y_p)\n",
    "print \"shape Y_pred :\", Y_pred.shape\n",
    "print 'Train Overall RECALL =',sum(((Y_pred.ravel()-y_test)**2)==0)/float((y_test.shape[0]))\n",
    "print 'Train Overall RECALL of all good preps =',get_acc_all(Y_pred,y_all_test)\n",
    "print\n",
    "y_p = model_nn_1.predict_proba(x_b_test)\n",
    "y_test = Y_b_test['Y_model'][:,0].ravel()\n",
    "y_all_test = Y_b_test['all_code']\n",
    "Y_pred=get_pred_vector(y_p)\n",
    "print \"shape Y_pred :\", Y_pred.shape\n",
    "print 'Test Overall RECALL =',sum(((Y_pred.ravel()-y_test)**2)==0)/float((y_test.shape[0]))\n",
    "print 'Test Overall RECALL of all good preps =',get_acc_all(Y_pred,y_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print i, Y_b_train['Y_model'][i, int(Y_pred[i][0])+18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_b_train['Y_model'][i,18:35]\n",
    "print Y_pred[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_p = model_nn_1.predict_proba(x_b_train)\n",
    "y_test = y_b_train\n",
    "y_all_test = Y_b_train['all_code']\n",
    "Y_pred=get_pred_vector(y_p)\n",
    "print \"shape Y_pred :\", Y_pred.shape\n",
    "print 'Train Overall RECALL =',sum(((Y_pred.ravel()-y_test)**2)==0)/float((y_test.shape[0]))\n",
    "print 'Train Overall RECALL of all good preps =',get_acc_all(Y_pred,y_all_test)\n",
    "print\n",
    "y_p = model_nn_1.predict_proba(x_b_test)\n",
    "y_test = y_b_test\n",
    "y_all_test = Y_b_test['all_code']\n",
    "Y_pred=get_pred_vector(y_p)\n",
    "print \"shape Y_pred :\", Y_pred.shape\n",
    "print 'Test Overall RECALL =',sum(((Y_pred.ravel()-y_test)**2)==0)/float((y_test.shape[0]))\n",
    "print 'Test Overall RECALL of all good preps =',get_acc_all(Y_pred,y_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_nn_1.predict_proba(x_b_test)\n",
    "#y_p = model_nn_1.predict_proba(x_b_train)\n",
    "#y_p = model_rf_1.predict_proba(x_b_test)\n",
    "y_test = y_b_test\n",
    "y_all_test = Y_b_test['all_code']\n",
    "#\n",
    "#y_p = model_lr_1.predict_proba(x_a_test)\n",
    "# y_p = model_rf_1.predict_proba(x_a_test)\n",
    "# y_test = y_a_test\n",
    "# y_all_test = Y_a_test['all_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print y_p.shape\n",
    "print y_test.shape\n",
    "print len(y_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=get_pred_vector(y_p)\n",
    "print 'Overall RECALL =',sum(((Y_pred.ravel()-y_test)**2)==0)/float((y_test.shape[0]))\n",
    "#print 'Overall RECALL =',sum(((Y_pred-Y_valid)**2)==0)/float(len(Y_valid))\n",
    "\n",
    "print 'Overall RECALL of all good preps =',get_acc_all(Y_pred,y_all_test)\n",
    "print 'RECALL@k : %2d:%5.4f,%2d:%5.4f,%2d:%5.4f,%2d:%5.4f'%(\\\n",
    "                                    1, get_recall_at_k(Y_pred, y_test, 1),\\\n",
    "                                    2, get_recall_at_k(Y_pred, y_test, 2),\\\n",
    "                                    3, get_recall_at_k(Y_pred, y_test, 3),\\\n",
    "                                    4, get_recall_at_k(Y_pred, y_test, 4))\n",
    "\n",
    "# ff=precision_recall_fscore_support(Y_test, Y_pred,average=None)\n",
    "# #\n",
    "# # recall_per_prep.append(ff[1])\n",
    "# # recall_micro.append(precision_recall_fscore_support(y_test, Y_pred,average='micro')[1])\n",
    "# # recall_macro.append(precision_recall_fscore_support(y_test, Y_pred,average='macro')[1])\n",
    "\n",
    "# print 'recall'\n",
    "# output_vector(ff[1])\n",
    "# output_vector(ff[0])\n",
    "# output_vector(ff[2])\n",
    "\n",
    "print 'average recall micro    :',precision_recall_fscore_support(y_test, Y_pred,average='micro')[1]\n",
    "print 'average recall macro    :',precision_recall_fscore_support(y_test, Y_pred,average='macro')[1]\n",
    "print 'average recall weighted :',precision_recall_fscore_support(y_test, Y_pred,average='weighted')[1]\n",
    "# #\n",
    "print 'average precision micro    :',precision_recall_fscore_support(y_test, Y_pred,average='micro')[0]\n",
    "print 'average precision macro    :',precision_recall_fscore_support(y_test, Y_pred,average='macro')[0]\n",
    "print 'average precision weighted :',precision_recall_fscore_support(y_test, Y_pred,average='weighted')[0]\n",
    "# #\n",
    "# print 'my precision @ k           :',get_precision_at_k(y_p,Y_test,2)\n",
    "print 'F-score                    :',precision_recall_fscore_support(y_test, Y_pred,average='weighted')\n",
    "# #     print 'Overall RECALL of all good preps =',get_acc_all(Y_pred,y_test_all)\n",
    "# #     print 'Overall RECALL of all good per prep() =',idx2prep[9],get_acc_all_per_prep(9,Y_pred,y_test_all)\n",
    "# #     print 'Overall RECALL of all good per prep() =',idx2prep[8],get_acc_all_per_prep(8,Y_pred,y_test_all)\n",
    "# print \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_pred.ravel()-y_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(x_b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print scaler.mean_\n",
    "print scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test['Y_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_test['all_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=model_lr_1.predict_proba(x_test[0:-1])\n",
    "print pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler_all = StandardScaler()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit only to the training data\n",
    "X_train=np.array([[1,0,0,0,.8,1.5],[0,0,1,0,-1.2,3.4],\n",
    "                  [0,1,0,0,2.2,-2.1],[0,0,0,1,-1.2,0.8],[1,0,0,1,-.2,-2.2]], dtype=float)\n",
    "scaler.fit(X_train[:,4:6])\n",
    "scaler_all.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print scaler.mean_\n",
    "print scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "X_train_s = scaler.transform(X_train[:,4:6])\n",
    "X_train_a = scaler_all.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_a[:,4:]-X_train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[9.,2,3],[4,1,6],[7,8,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort(axis=0)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(a.view('i8,i8,i8'), order=['f0'], axis=0).view(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.sort(a.view('i8,i8,i8'), order=['f1'], axis=0).view(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(a.view('f8,f8,f8'), order=['f1'], axis=0).view(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sizes for train/dev/test sets\n",
    "print\n",
    "print \"Size of x_b_train :\", x_b_train.shape\n",
    "print \"Size of y_b_train :\", y_b_train.shape\n",
    "print \"Size of x_b_dev :\", x_b_dev.shape\n",
    "print \"Size of y_b_dev :\", y_b_dev.shape\n",
    "print \"Size of x_b_test :\", x_b_test.shape\n",
    "print \"Size of y_b_test :\", y_b_test.shape\n",
    "\n",
    "print\n",
    "print \"Size of x_a_train :\", x_a_train.shape\n",
    "print \"Size of y_a_train :\", y_a_train.shape\n",
    "print \"Size of x_a_dev :\", x_a_dev.shape\n",
    "print \"Size of y_a_dev :\", y_a_dev.shape\n",
    "print \"Size of x_a_test :\", x_a_test.shape\n",
    "print \"Size of y_a_test :\", y_a_test.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
