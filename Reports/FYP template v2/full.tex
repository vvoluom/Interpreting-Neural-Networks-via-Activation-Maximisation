\documentclass{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\title{Interpreting Neural Networks via Activation Maximization(May 2019)}
\author{Vitaly Volozhinov}

\begin{document}
\maketitle
\begin{abstract}
Decision trees are models whose structure allows for tracing an explanation of how the
final decision was taken. Neural Networks known as ’black box’ models, do not readily and
explicitly offer an explanation of how the decision was reached. However since Neural Networks are capable of learning knowledge representation it will be very useful to interpret
the model’s decisions. In this project the Visual Relationship Detection problem will be explored in the form of different Neural Network implementations and training methods. These implementations include two Convolutional Neural Network architectures (VGG16 and SmallVGG) and two Feed Forward Neural Networks trained using Geometric features and Geometric
with Language Features. These models will be treated as two kinds of problems, one is the
Multi-Label Classification(MLC) problem and the other is the Single-Label Classification(SLC) problem. Activation Maximisation will then be used to interpret the different Convolutional Neural Networks under different training methods by maximizing a specific class output to visualize what it is learning.
\end{abstract}

\section{Introduction}
Convolutional Neural Networks(CNNs) have become very precise and effective in solving the problems of object detection and localization in images. The focus is shifting from object recognition to the study of visual relationships between objects in an image. This is the Visual Relationship Detection (VRD) problem. In this problem given a subject and an object, the machine learning model must predict the best predicate that describes the visual relationship between those two objects. The two classification problem types are MLC and SLC and it would prove useful to understand under which training methods this problem would provide the best results. Activation Maximization's method of interpreting CNNs has shown to be a useful tool to understanding what CNNs are looking for when classifying classes. This tool will be applied to the VRD problem to understand what the CNN is looking at when classifying relations under different learning conditions. Apart from CNNs a Feed Forward Neural Network trained using the geometric features together with the language features would be trained to be compared to the CNNs in terms of accuracy and performance. A Feed Forward Neural Network would have language features added to it which is something that couldn't be added to a CNN.
\section{Background}
\subsection{Visual Relationship Detection}
\subsubsection{Visual Relationship Detection with Language Priors}
A solution to problems caused by Sadeghi and Farhadi’s(2011)~\cite{VisualPhrases} was to divide the problem up into parts. The first part would be to perform object detection on the two objects and then pass their Union into a new network which was specialized in predicate prediction as done by Lu et al(2016)~\cite{lu2016visual}. Given N objects and K predicates the problem was taken from $O(N^2 K)$ unique detectors and transformed into to $O(N+K)$.
\subsubsection{A Study on the Detection of Visual Relationships}
Mizzi (2018)~\cite{detectionRelationships} has worked on expanding Dai et al(2017)'s ~\cite{Dai2017DetectingVR} spatial masks method of preparing images for training. Before the focus had been on the Image size and the amount of Convolutional layers a CNN has but this focuses on the way the images are prepared. Training was done as a SLC problem with evaluation metric of Recall@1. Methods include the following : Union, Union-WB, Union-WB-B, Blur, Segment, Segment-B that had been trained and tested on the VGG16 and VGG19 CNNs using the Stanford VRD dataset. The best performing method was Union-WB-B method which worked by taking the Union of the subject and object bounding boxes cropping the image to those dimensions and having the background set to black [0,0,0], subject bounding box set to green [0,255,0] and the object bounding box set to [0,0,255](Square brackets represent RGB pixel values). When there is an overlap of bounding boxes the pixel values are set to [0,255,255] a combination of green and blue.The cropped image containing the subject and object image pixels was then resized to 224 by 224 and fed into the CNNs.
\subsection{Multi-Label Classification}
\subsubsection{A Review on Multi-Label Learning Algorithms}
Zhang et al(2014)'s ~\cite{6471714} concepts taken for this dissertation are the evaluation metrics used for evaluating multi-label classifiers. The number of examples is denoted as n, the ground truth label is $Y_{i}$ and $h(x_{i})$ is the predicted label output of the $i^{th}$ example.
\\
\small 
Example Based:
\begin{equation}
Precision = \frac{1}{n}\sum_{i=1}^{n}\frac{|Y_{i}\cap h(x_{i})|}{|h(x_{i})|}
\end{equation}

\begin{equation}
Recall = \frac{1}{n}\sum_{i=1}^{n}\frac{|Y_{i}\cap h(x_{i})|}{|Y_{i}|}
\end{equation}

Label Based:
\begin{equation}
B \in { Accuracy, Precision, Recall, F^B} 
\end{equation}
Macro-averaging
\begin{equation}
•B(h) = \frac{1}{q}\sum_{j=1}^{q} B(TP_j,FP_j,TN_j,FN_j)
\end{equation}
Micro-averaging
\begin{equation}
•B(h) = B(\sum_{j=1}^{q} TP_j,\sum_{j=1}^{q} FP_j,\sum_{j=1}^{q} TN_j,\sum_{j=1}^{q} FN_j)
\end{equation}

\begin{equation}
Recall(TP_j,FP_j,TN_j,FN_j) = \frac{TP_j}{(TP_j + FN_j)}
\end{equation}

\begin{equation}
Precision(TP_j,FP_j,TN_j,FN_j) = \frac{TP_j}{(TP_j + FP_j)}
\end{equation}

\begin{equation}
F1(TP_j,FP_j,TN_j,FN_j) = \frac{(1+B^2)*TP_j}{((1+B^2)*TP_j+B^2*FN_j+FP_j)}
\end{equation}
\normalsize
\subsection{Activation Maximization}
Qin et al(2018) ~\cite{AM} explored different methods to try and understand what Convoulational Neural Networks had been seeing. These methods include Network Inversion, Deconvolutional Neural Networks, Network Dissection based visualization and Activation Maximization(AM). AM starts with a random input image and through back propagation maximizes an output class, this creates a visualized activation map representing that class.
\subsection{Datasets}
Stanford VRD~\cite{lu2016visual} and SpatialVoc2k~\cite{belz-etal-2018-spatialvoc2k} datasets are used. The Stanford VRD dataset is mainly a Single-Label Dataset while SpatialVoc2k~\cite{belz-etal-2018-spatialvoc2k} is a Multi-Label, multilingual dataset focused on spatial relations.

\section{Aims and Objectives}
Different CNN architectures VGG16 and SmallVGG will be trained and tested as Multi-Label and Single-Label Classification problems. The same will be done for the Feed Forward Neural Networks trained on the Geometric with Language Features and Geometric only. These networks will be trained using the previously mentioned datasets so that the effects of the label distribution in the datasets can be compared. The SLC networks are evalutaed using Recall@1, Precision@1, F1@1 where the top most predicted predicate will be taken. The MLC networks will be evaluated using the Metrics from Zhang et al(2014)~\cite{6471714} as seen above. AM will be used on the CNN models to interpret what the model is learning this will help give a better understanding to the achieved results. It will also give us an insight to the difference between MLC and SLC training methods.

\section{Design and Implementation}
\subsection{Dataset preparation}
Data entries where a pair of objects with same bounding boxes and multiple relationships had been concatenated to form one data entry with multiple labels. Then the final list was randomized and stratified sampled into training/testing/validation data with a split of 60\%/20\%/20\%. Stratified sampling was done by first distributing all the multi-label entries and then distributing the single-label entries as it would create a more specific distribution. This was repeated for 10 times to have 10 different training/testing/validation datasets so that multiple models could be trained and their averages taken. Images were preapared using Union-WB-B and since this method doesn't use any of the actual objects but only their bounding boxes, there is no need for an actual image only it’s meta-data. Hence the datasets had been prepared in the form of [Labels, width, height, subject bounding box, object bounding box, subject label, object label]. To create images from this data OpenCv was used create a black image of a certain width, height together with green, blue rectangles added to it in positions of the subject/object bounding boxes. This method was much faster as it didn’t require any space, processing and loading time for images. The created images are then resized to 224x224 as that is what the VGG16 network had been initially trained using. To train the Feed Forward Neural Network the Geometric Features in Table 1 had be created from the data. Note : Let distance from image edge of left and right edges be a1,b1 for first box and a2,b2 for second box the same thing was done for top and bottom edges for c1,d1 and c2,d2. To Create the Single-Label datasets the previously created Multi-Label datasets had been loaded in and the labels with multiple relationships had been expanded to have their own data entries. This preserved the label distribution and stratified sampling previously done for fair comparisons.
\small
\begin{table}[!htbp]
\centering
\begin{tabular}{|l|lll}
\cline{1-1}
F0 : Object Label Ls: Subject \\ \cline{1-1}
F1 : Object Label Lo: Object  \\ \cline{1-1}
F2 : Area of Objs normalized by Union Box Size \\ \cline{1-1}
F3 : Area of Objo normalized by Union Box size \\ \cline{1-1}
F4 : Ratio of Bounding Box Objs to that of Objo  \\ \cline{1-1}
F5 : Distance between box centers normalized by Union Box Diagonal \\ \cline{1-1}
F6 : Area of Overlap normalized by area of smaller bounding box \\ \cline{1-1}
F7 : Minimum Distance between the two bounding boxes \\ \cline{1-1}
F8 : Position of Objs relative to Objo in terms of N,S,W,E \\ \cline{1-1}
F9 : (a2-a1)/(b1-a1), F10 : (b2-a1)/(b1-a1) \\ \cline{1-1}
F11: (c2-c1)/(d1-c1), F12 : (d2-c1)/(d1-c1) \\ \cline{1-1}
F13 : Aspect ratio of box Objs \\ \cline{1-1}
F14 : Aspect ratio of box Objo \\ \cline{1-1}
F15 : Relationship \\ \cline{1-1}
\end{tabular}
\caption{Geometric and Language Features}
\end{table}
\normalsize

\subsection{Training}
Fine-Tuned VGG16 with ImageNet weights and a SmallVGG with no preloaded weights are trained on the datasets. Two Feed Forward Neural Networks had been trained using these datasets one containing only Geometric Features and another Geometric Features with Language Features. AM is then applied on the final output layers of the CNN models and their activation maps are recorded.

\subsubsection{VGG16}
The VGG16 model was first loaded then last Dense layer was replaced with a new Dense layer with a specified amount of classes and activation function that is set before training according to the problem. The MLC problem would use the loss function of binary-crossentropy and an activation function of Sigmoid while the SLC problem would use the categorical-crossentropy loss function and an activation function of Softmax. All the layers had been set to non-trainable except for the last dense and fully connected layers. Using the optimizer stochastic gradient descent(SGD) with a learning of 0.001 and a Nesterov momentum of 0.9 the model was run for 5 Epochs for the VRD dataset and 15 Epochs for SpatialVoc2k dataset. Once the model finished training it was saved and a new model was created with all the layers set to non-trainable except that of the fully connected layers and the last convolutional block (Conv Block 5). The previous models weights had been loaded into the new model and again run for 5 Epochs for the VRD dataset and 15 Epochs for SpatialVoc2k dataset. Finally the last step was repeated but with a learning rate of 0.00001 for 5 Epochs for the VRD dataset and 15 Epochs for SpatialVoc2k dataset. This was done 10 times for both datasets and both problem types.

\subsubsection{SmallVGG}
The SmallVGG was created out of 7 Convolutional Layers instead of 16 like the VGG16. This was trained from scratch for 10 Epochs for the VRD dataset and 20 Epochs for the SpatialVoc2k dataset. The same loss, optimizer and activation functions have been used for this as the VGG16 network. This was trained as both MLC and SLC problems and evaluated using the appropriate MLC and SLC metrics. This was done 10 times for both datasets and both problem types. The SmallVGG trained using SpatialVoc2k produced inconsistent results on the validation data during training.

\subsubsection{Feed Forward Neural Network}
A Feed Forward Neural Network was trained using the Geometric Features and Geometric with Language Features. The network had been made up out of two fully connected layers (256 neurons followed by 128 neurons) and a densly connected output layer containing the number of output classes with an activation function according to the problem being solved. The same parameters were applied to it with regards to MLC and SLC problems. The optimizer ADAM was used with default values and was trained for 10 Epochs. This was done 10 times for both datasets and both problem types and their results recorded.

\subsubsection{Evaluation and Metrics}
The predict function was used over the evaluation function as specific MLC evaluation metrics had to be implemented to evaluate the MLC models. On a given input prediction a set of probabilities had been returned corresponding to the probability of each class detected by the model. Since this is a MLC problem the Sigmoid activation function was used so each class had their own independent probability ranging from 0\% to 100\%. A threshold of 50\% had been chosen as a probability cut off point. If a value was above 50\% then it would be turned on (set to 1) and if it was below then it would have been turned off (set to 0). The predicted values had then been compared to the ground truth labels and evaluated using MLC metrics ~\cite{6471714}. To evaluate SLC models Recall@1, Precision@1 and F1-Score@1 had been utilized. Since @1 is used it means that the highest probability value is used for the metric. The predictions were first ranked in descending order by probability and the highest predicted class is compared to the ground truth value. The results are recorded per class together with the Micro/Macro averages.
                      
\subsubsection{Interpreting the models}
To interpret the models via Activation Maximization the trained models are first loaded in, then the last activation function of the fully connected prediction layer was chosen and replaced by a linear activation. AM was then initialized on a random input image and run for 1024 back propagation iterations maximizing the output for each class. The resulting images were then saved and compared for each model.
\section{Results}
\small 
\begin{table}[!htbp]
\caption{VRD Convolutional Neural Network Evaluation Results}
\centering
\begin{tabular}{|l|l|l|l|l|llll}
\cline{1-5}
        &  VGG16 & VGG16 & SmallVGG & SmallVGG \\ \cline{1-5}
Class   &  MLC   &  SLC    & MLC  & SLC  \\ \cline{1-5}
English &  Recall & Recall@1  & Recall  & Recall@1  \\ \cline{1-5}
above	&	0.44  &	0.63	  &	0.43    &	0.59  \\ \cline{1-5}
at	    &	0.00  &	0	      &	0.00    &	0.01  \\ \cline{1-5}
behind	&	0.03  &	0.26	  &	0.02    &	0.08  \\ \cline{1-5}
below	&	0.03  &	0.22      &	0.04    &	0.28  \\ \cline{1-5}
beside	&	0.00  &	0	      &	0.00    &	0.02  \\ \cline{1-5}
by	    &	0.00  &	0.00      &	0.00    &	0.01  \\ \cline{1-5}
front	&	0.00  &	0.21	  &	0.00    &	0.06  \\ \cline{1-5}
in	    &	0.48  &	0.55      &	0.48    &	0.56  \\ \cline{1-5}
left	&	0.00  &	0.22      &	0.02    &	0.22  \\ \cline{1-5}
near	&	0.00  &	0.03	  &	0.00    &	0.04  \\ \cline{1-5}
next	&	0.02  &	0.44	  &	0.04    &	0.29  \\ \cline{1-5}
on	    &	0.69  &	0.86	  &	0.69    &	0.54  \\ \cline{1-5}
over	&	0.00  &	0.00	  &	0.00    &	0.02  \\ \cline{1-5}
right	&	0.00  &	0.2	      &	0.00    &	0.14  \\ \cline{1-5}
top	    &	0.00  &	0.00	  &	0.00    &	0.02  \\ \cline{1-5}
under	&	0.05  &	0.45	  &	0.07    &	0.30  \\ \cline{1-5}
\end{tabular}
\caption{VGG16 and SmallVGG (VRD) Recalls for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|llll}
\cline{1-5}
               &  VGG16 & VGG16 & SmallVGG & SmallVGG \\ \cline{1-5}
Class          &  MLC       &  SLC     &  MLC  &  SLC  		\\ \cline{1-5}
Metric Type    &  Recall  & Recall@1  &  Recall  & Recall@1 \\ \cline{1-5}
Macro-Average  &	0.11  &	0.26	  &	0.11     &	0.20	\\ \cline{1-5}
Micro-Average  &	0.29  &	0.47      &	0.30     &	0.35	\\ \cline{1-5}
Example        &	0.31  &           &	0.31     &          \\ \cline{1-5}
\end{tabular}
\caption{VGG16 and SmallVGG (VRD) Label and Example Recalls for MLC and SLC}
\end{table}

\begin{table}[!htbp]
\caption{VRD Feed Forward Neural Network Evaluation Results}
\centering
\begin{tabular}{|l|l|l|l|l|llll}
\cline{1-5}
        &  G+L & G+L & G & G \\ \cline{1-5}
Class   &  MLC       & SLC        &  MLC       & SLC        \\ \cline{1-5}
English &  Recall    & Recall@1   &  Recall    & Recall@1   \\ \cline{1-5}
above	&	0.62	 &	0.64  &	0.35      &	0.63    \\ \cline{1-5}
at	   	&	0.11	 &	0.13  &	0.00 	&	0.00	\\ \cline{1-5} 
behind	&	0.41	 &	0.49  &	0.00  	&	0.12    \\ \cline{1-5} 
below	&	0.24 	 &	0.25  &	0.01 	&	0.11	\\ \cline{1-5} 
beside	&	0.04     &	0.07  &	0.00	&	0.00	\\ \cline{1-5} 
by 	   	&	0.05	 &	0.07  &	0.00	&	0.00	\\ \cline{1-5} 
front	&	0.26   	 &	0.34  &	0.00	&	0.08	\\ \cline{1-5} 
in   	&	0.68   	 &	0.70  &	0.48	&	0.56	\\ \cline{1-5} 
left	&	0.12   	 &	0.19  &	0.00	&	0.25	\\ \cline{1-5} 
near	&	0.07 	 &	0.13  &	0.00	&	0.01 	\\ \cline{1-5}
next	&	0.21	 &	0.32  &	0.01	&	0.45 	\\ \cline{1-5} 
on	   	&	0.76	 &	0.77  &	0.63	&	0.86 	\\ \cline{1-5} 
over	&	0.09	 &	0.11  &	0.00	&	0.00	\\ \cline{1-5} 
right	&	0.12	 &	0.19  &	0.00	&	0.24 	\\ \cline{1-5} 
top	   	&	0.03	 &	0.05  &	0.00	&	0.00 	\\ \cline{1-5} 
under	&	0.50	 &	0.54  &	0.02    &	0.47 	\\ \cline{1-5}
\end{tabular}
\caption{Feed Forward NN(VRD) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|llll}
\cline{1-5}
      &  G+L  & G+L   &  G   & G 	 \\ \cline{1-5}
Class &  MLC  & SLC 				 \\ \cline{1-5}
Metric Type    &  Recall  & Recall@1  & Recall & Recall@1    \\ \cline{1-5}
Macro-Average &	0.27      &	0.31	  &	0.09   &	0.24	 \\ \cline{1-5}
Micro-Average &	0.47      &	0.51      &	0.27   &	0.45	 \\ \cline{1-5}
Example       &	0.49     			  &	0.28	             \\ \cline{1-5}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for Feed Forward(VRD)}
\end{table}

\begin{table}[!htbp]
\caption{SpatialVoc2k Convolutional Neural Network Evaluation Results}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l}
\cline{1-6}
	  			  & 	  			  & VGG16 		& VGG16    & SmallVGG & SmallVGG  \\ \cline{1-6}
Class 			  &  Class 			  & MLC   		& SLC  	   & MLC      & SLC       \\ \cline{1-6}
French            & English           &  Recall     & Recall@1 &  Recall  & Recall@1  \\ \cline{1-6}
a cote de	      &	next to           &	0.52		&	0.20   &	0.40  &	0.11      \\ \cline{1-6}
a l'exterieur de  &	outside	          &	0.00		&	0.01   &	0.09  &	0.00	  \\ \cline{1-6}
au dessus de	  &	beyond		      &	0.00		&	0.09   &	0.37  &	0.03	  \\ \cline{1-6}
au niveau de	  &	near		      &	0.47	    &	0.17   &	0.42  &	0.10	  \\ \cline{1-6}
aucun	          &	 none		  	  &	0.00		&	0.02   &	0.39  &	0.02	  \\ \cline{1-6} 
autour de	      &	around		      &	0.00		&	0.31   &	0.40  &	0.06	  \\ \cline{1-6}
contre			  &	against		      &	0.14	    &	0.16   &	0.43  &	0.14	  \\ \cline{1-6}
dans			  &	in				  &	0.00     	&	0.28   &	0.22  &	0.13  	  \\ \cline{1-6}
derriere		  &	behind		      &	0.42		&	0.34   &	0.32  &	0.19      \\ \cline{1-6}
devant			  &	front		      &	0.46		&	0.34   &	0.28  &	0.25      \\ \cline{1-6} 
en face de		  & across from		  &	0.00		&	0.02   &	0.36  &	0.04	  \\ \cline{1-6} 
le long de	      &	along 		      & 0.00		&   0.00   &	0.12  &	0.01 	  \\ \cline{1-6} 
loin de			  &	far from		  &	0.25		&	0.30   &	0.44  &	0.25	  \\ \cline{1-6}
par dela	      &	beyond	          & 0.00	    &	0.00   &	0.25  &	0.01      \\ \cline{1-6}
pres de			  &	by		          &	0.72		&	0.44   &	0.54  &	0.32	  \\ \cline{1-6}
sous		   	  &	under	          &	0.41	    &	0.48   &	0.25  &	0.32 	  \\ \cline{1-6}
sur				  &	on			      &	0.45	    &	0.46   &	0.22  &	0.31	  \\ \cline{1-6}
\end{tabular}
\caption{VGG16 and SmallVGG(SpatialVoc2k) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|llll}
\cline{1-5}
	   		  & VGG16 & VGG16   & SmallVGG & SmallVGG  		\\ \cline{1-5}
       		  &  MLC  	 & SLC      &  MLC      & SLC       \\ \cline{1-5}
Metric Type   &  Recall  & Recall@1 &  Recall   & Recall@1  \\ \cline{1-5}
Macro-Average &	0.23	&	0.21	&	0.32 	&	0.13	\\ \cline{1-5}
Micro-Average &	0.47	&	0.30	&	0.40	&	0.21	\\ \cline{1-5}
Example       &	0.46	& 			&	0.39	&			\\ \cline{1-5}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for SmallVGG and VGG16(SpatialVoc2k)}
\end{table}

\begin{table}[!htbp]
\caption{SpatialVoc2k Feed Forward Neural Network Evaluation Results}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l}
\cline{1-6}
	  			  & 	  			  & G+L 		& G+L      & G  	  & G   	  \\ \cline{1-6}
Class 			  &  Class 			  & MLC   		& SLC  	   & MLC      & SLC       \\ \cline{1-6}
French            & English           & Recall      & Recall@1 &  Recall  & Recall@1  \\ \cline{1-6}
a cote de	      &	next to           &	0.40		&   0.11   &	0.54  &	0.65     \\ \cline{1-6}
a l'exterieur de  &	outside	          &	0.09		&	0.00   &	0.00  &	0.00	  \\ \cline{1-6}
au dessus de	  &	beyond		      &	0.37		&	0.03   &	0.00  &	0.00	  \\ \cline{1-6}
au niveau de	  &	near		      &	0.42	    &	0.10   &	0.36  &	0.00	  \\ \cline{1-6}
aucun	          &	none		  	  &	0.39		&	0.02   &	0.00  &	0.00	  \\ \cline{1-6} 
autour de	      &	around		      &	0.40		&	0.06   &	0.08  &	0.09	  \\ \cline{1-6}
contre			  &	against		      &	0.43	    &	0.14   &	0.00  &	0.17	  \\ \cline{1-6}
dans			  &	in				  &	0.22     	&	0.13   &	0.00  &	0.07  	  \\ \cline{1-6}
derriere		  &	behind		      &	0.32		&	0.19   &	0.27  &	0.29      \\ \cline{1-6}
devant			  &	front		      &	0.28		&	0.25   &	0.32  &	0.31      \\ \cline{1-6} 
en face de		  & across from		  &	0.36		&	0.04   &	0.00  &	0.00	  \\ \cline{1-6} 
le long de	      &	along		      &	0.12		&	0.01   &	0.00  &	0.00 	  \\ \cline{1-6} 
loin de			  &	far from		  &	0.44		&	0.25   &	0.24  &	0.18	  \\ \cline{1-6}
par dela	      &	beyond	          &	0.25	    &	0.00   &	0.00  &	0.00      \\ \cline{1-6}
pres de			  &	by		          &	0.54		&	0.32   &	0.71  &	0.66	  \\ \cline{1-6}
sous		   	  &	under	          &	0.25	    &	0.32   &	0.00  &	0.30	  \\ \cline{1-6}
sur				  &	on			      &	0.22	    &	0.31   &	0.09  &	0.40	  \\ \cline{1-6}
\end{tabular}
\caption{VGG16 and SmallVGG(SpatialVoc2k) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|llll}
\cline{1-5}
	   		  &  G+L    & G+L       &  G        & G  		\\ \cline{1-5}
       		  &  MLC  	& SLC       &  MLC      & SLC       \\ \cline{1-5}
Metric Type   &  Recall & Recall@1  &  Recall   & Recall@1  \\ \cline{1-5}
Macro-Average &	0.27	&	0.25	&	0.15	&	0.15	\\ \cline{1-5}
Micro-Average &	0.51	&	0.34	&	0.39	&	0.29	\\ \cline{1-5}
Example       &	0.52	& 			&	0.36	&			\\ \cline{1-5}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for Feed Forward(SpatialVoc2k)}
\end{table}
\normalsize

\begin{figure}[!htbp]
	\includegraphics[scale=0.40]{VGG16_Small_VRD.pdf}
	\vspace{-4mm}
\end{figure}

\begin{figure}[!htbp]
	\includegraphics[scale=0.40]{SV_16_small.pdf}
	\vspace{-4mm}
\end{figure}

\section{Evaluation}
\subsection{VRD Results}
\subsubsection{Multi-label vs Single-label Classification}
For all networks the SLC training method outperformed the MLC training method for all per-predicate accounts of Recall. SLC has more predicate results due to the difference in metrics, MLC metrics have a threshold of 50\% which filter out low probability labels, while SLC uses @1 so low probability labels can be retrieved as long as they are the highest among the predicted.

\subsubsection{VGG16 vs SmallVGG}
The SmallVGG outperformed the Fine-Tuned VGG16 on the MLC training method for the Recall Metrics by a small margin, while the VGG16 achieved better results for the SLC training method.

\subsubsection{VGG16 vs Feed Forward Neural Network}
The Feed Forward Neural Network trained on the Geometric Features with Language Features(G+L) achieved better results than the VGG16 CNN trained using Union-WB-B. These language features that have been combined with the geometric features are the main reason that the Feed Forward Network outperforms the VGG16. Language features allow for more ambiguous predicates to be predicted as these predicates would be indistinguishable as spatial masks or geometric features alone but would have a very distinct occurance between a pair of object categories. To show the importance of the language features the network trained only on Geometric Features alone(G) have achieved results lower than that of the VGG16 for both MLC and SLC.

\subsubsection{Activation Maximization}
Knowing that the subject is Green and object is Blue we can interpret the activation maps. The predicates Above,Over have similar activation maps while Top has a less clear version of those maps. These maps show that Green is Above/Over/Top the Blue colour which is a good indicator of how the network perceives these spatial relations. The below and under activation maps show the Green colour below/under the Blue colour which is a straight forward indicator. Left and Right also have quite clear activation maps that are easily understandable. The predicate "In" is shown to be a mix of $[0,255,255]$ pixels surrounded by an outline of Blue pixels indicating that the Green subject is inside the Blue object. "On" has multiple small concentrations  of Green points above small concentrations of $[0,255,255]$ followed by Blue points, this indicates that the predicate "On" has a Green subject constantly in contact with the Blue object while being in a higher position over it at the same time. While "above/over/top" predicates have the concentration of Green confined to the upper limits of the activation map "On" has them more spread out. The predicates Beside, By, Near and Next have similiar looking SLC activaion maps which is good as they are similiar predicates, the MLC activation maps for those predicates didn't show anything clear and it makes sense as the MLC results had been poorer that the SLC ones. The MLC activations produced similar looking maps to that of the SLC activations but they are less clear.

\subsubsection{Conclusions}
These results dictate that even though the VRD problem should be taken as a MLC problem the label distributions in the datasets play a large role in how well the model will be able to recognise multiple relationships. Training a model which is heavily composed of single labels(VRD) as a Multi-Label model hinders the models ability to recall labels correctly. The Feed Forward Neural Network outperforms the CNN mainly due to the Language Features otherwise having only Geometric Features produce lower scores than the CNN. Language features also greatly close the gap in results between MLC and SLC for the VRD dataset. The activation maps proved to be useful in confirming the metric results between the training methods.

\subsection{SpatialVoc2k Results}
\subsubsection{Multi-label vs Single-label Classification}
The models trained achieved better results when they are trained as MLC rather than SLC problems as SpatialVoc2k is mainly Multi-Label. The SLC method has noteably higher results on the predicates "around/into" compared to the zero results retrieved by MLC, this can be explained further by looking at the activation maps for insight to what the CNN is looking for. 

\subsubsection{VGG16 vs SmallVGG}
The Fine-Tuned VGG16 had achieved better results than the SmallVGG for all predicates and both training methods, this is confirmed by the Activation Maps generated, as the SmallVGG activation maps are less comprehensible than that of the VGG16.

\subsubsection{VGG16 vs Feed Forward Neural Network}
The Feed Forward Neural Network(G+L) achieved better results than the VGG16 for all the Macro/Micro-Average metrics. The predicates:=\{next to, near, front, behind\} had higher results for the Spatial Features over the Geometric and Language Features. Geometric Features alone(G) performed worse than the Spatial Features for all metrics and per predicate results.

\subsubsection{Activation Maximization}
MLC has clearer maps than SLC except on "Around" and "In" where SLC has had higher metric scores. The SLC activation maps of predicates:=\{next to, near, behind, front, far from, near by, on\} do not show anything concretely interpretable and the maps look quite similar. "Around" is shown to have the Green subject around a Blue object, "In" has a Blue box outline surrounding the pixel values of $[0,255,255]$ which means that Green is inside the Blue object and the Blue object is bigger in size. The MLC activation map "Front" shows a light green covering around the map which is a good indicator that Green subject is in front of the Blue object. The MLC predicates:=\{Next to, Near, and Neary By\} have a similar patter inside them where the middle part of the activation map contains vertical lines, this would be interpreted as that when objects appear with those predicates they would be close to each other and at the same level. The SmallVGG had clearer looking SLC maps while the MLC maps were incomprehensible and mostly random.
 
\subsubsection{Conclusions}
The MLC method is prefered over the SLC method for Multi-Label datasets and problems. Geometric Features with Language Features(G+L) outperform the Spatial Features but the Geometric Features(G) alone without he Language Feature perform worse than the Spatial Features. Activation Maximization proved useful to understanding which predicates are similar to each other and why a certain training type outperforms the other.

\section{Conclusions and Future work}
The label distribution in the dataset plays a big factor in deciding the way the model should be trained as MLC vs SLC. Language Features play a big part in classifying the best relationship. Activation maximization is a useful way of understading and interpreting what CNN's are learning and why some methods perform better than others. It groups same predicates together which is useful if you would want to train a multi-label dataset as a single label by grouping predicates under the same activation maps under a single label. The MLC activation maps produced by the SmallVGG trained used SpatialVoc2k had been incomprehensible and mostly random meaning that the CNN hasn't learned anything concrete using the MLC training method.
\\
It would be interesting to apply Activation Maximization to Feed Forward Neural Networks to be able to achieve inputs that maximize certain classes. The Language Features included with the Geometric Features can be upgraded from One Hot Encoding to word2vec to save space and increase accuracy over unseen examples as was done by Muscat-Belz(2018)~\cite{belz-etal-2018-spatialvoc2k}. More Geometric Features should be explored not including the language features, geometric models took less time to train and it would be better to get them to the same levels of accuracy as that of Union-WB-B.

\section*{Acknowledgements.}
I would like to thank and express my special gratitude to my supervisor Dr.Adrian Muscat for assisting me and guiding me throughout this final year project. He was very crucial to my learning experience as he guided me to specific learning resources in this sea of data.

\bibliography{mybib}{}
\bibliographystyle{plain}

\end{document}
