\documentclass{csfyp}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{glossaries}

\makeglossaries
\newglossaryentry{binaryCross}
{
  name=binary-crossentropy,
  description={Loss function used on problems involving binary decisions}
}
\newglossaryentry{categoricalCross}
{
  name=categorical-crossentropy,
  description={Loss function used on problems involving categorical decisions}
}
\newglossaryentry{loss}
{
  name=loss,
  description={-method of evaluating how well the algorithm is modelling specific data}
}
\newglossaryentry{epoch}
{
  name=epoch,
  description={-training iteration over the training data}
}
\newglossaryentry{UnionWBB}
{
  name=Union-WB-B,
  description={-method of preparing images, composed of green and blue object boxes with a black background}
}
\newglossaryentry{ohe}
{
  name=One Hot Enconding,
  description={-convert categorical variables into a form that a Neural Network can understand}
}
\newglossaryentry{predicate}
{
  name=Predicate,
  description={-verb stating something about the subject with regards to the object}
}
\newglossaryentry{ai}
{
  name=A.I,
  description={-Artificial Intelligence is the simulation of human intelligence by machines}
}
\newglossaryentry{Recall1}
{
  name=Recall@1,
  description={-Fraction of relevant documents successfully retrieved, @1 means the top-1 highest probability classes are retrieved}
}

\title{Interpreting Neural Networks via Activation Maximization}
\author{Vitaly Volozhinov}
\supervisor{Dr. Adrian Muscat}

\date{May 2019}

\longabstract{
	Decision trees are models whose structure allows for tracing an explanation of how the final decision was taken. Neural networks known as 'black box' models, do not readily and explicitly offer an explanation of how the decision was reached. However since Neural Networks are capable of learning knowledge representation it will be very useful to interpret the model's decisions.
\\
\tab In this project the Visual Relationship Detection problem will be explored in the form of different Neural Network implementations and training methods. These implementations include two Convolutional Neural Network architectures (VGG16 and SmallVGG) and two Feed Forward Neural Networks trained using Geometric features and Geometric with Language Features. These models will be treated as two kinds of problems, one is the Multi-Label Classification problem and the other is the Single-Label Classification problem.
\\
\tab Activation Maximisation will be used to interpret the different Convolutional Neural Networks under different training methods by maximizing a specific class output to visualize what it is learning. This study is grounded in the recognition of spatial relations between objects in images. Activation Maximization will shed light on what models are learning about objects in 2D images which should give insight into how the system can be improved. The spatial relation problem is one where given a subject and an object the correct spatial preposition is predicted. This problem extends beyond just predicting one correct spatial preposition as there are multiple possible relationships associated between two objects.
}

\begin{document}
\section*{Acknowledgements.}
I would like to thank and express my special gratitude to my supervisor Dr.Adrian Muscat for assisting me and guiding me throughout this final year project. He was very crucial to my learning experience as he guided me to specific learning resources in this sea of data.
\section*{Declaration}
I, the undersigned, declare that the dissertation entitled:
\\
\tab Interpreting Neural Networks via Activation Maximization
\\
submitted is my work, except where acknowledged and referenced.
\newpage

\pagenumbering{roman} 
\tableofcontents

\listoffigures
\newpage
\listoftables
\printglossaries

\pagenumbering{arabic} 
\setcounter{page}{1}
\newpage


\section{Introduction}
\label{s:intro}
\tab
Research in computer vision has excelled in recent years largely due to the technological advancements in hardware. This allowed for more computationally intensive ideas to be explored and implemented. Deep Learning a subset of Machine Learning is one of those ideas based on artificial neural networks. Deep learning in computer vision comes in the form of Convolutional Neural Networks(CNNs). This form of Machine Learning is effective as it takes the given images and through a sequence of convolutions and pooling layers transforms this data into that of smaller size while retaining important features. This reduces the training time and computational power required to classify images compared to a Neural Network.
\\
\tab
CNNs have become very precise and effective in solving the problems of object detection and localization in images. Up until recently it was thought to be impossible for a computer to distinguish between a cat and a dog in an image due to them having similar general features but nowadays anyone can implement a simple CNN to solve this classification problem. The focus is shifting from object recognition to the study of visual relationships between objects in an image. This is the Visual Relationship Detection (VRD) problem. In this problem given a subject and an object, the machine learning model must predict the best \Gls{predicate} that describes the visual relationship between those two objects.
\\
\tab
The VRD problem was first tackled by Sadeghi and Farhadiâ€™s(2011)~\cite{VisualPhrases} by taking triplet representation $< subject, predicate, object >$ as a unique class, this has led to an exponential growth in classes and a long tail distribution problem. A solution to that was to divide the problem up into parts. The first part would be to perform object detection on the two objects and then pass their Union into a new network which was specialized in \Gls{predicate} prediction as done by Lu et al(2016)~\cite{lu2016visual}.
There have been improvements to accuracy for this method such as having geometric and text features accompany the CNN model for increased accuracy. VRD is important as it would give greater context to images which would provide real world solutions to problems such as giving audio descriptions to blind people of the environment around them.
\\
\tab
The research aim is to use Activation Maximization on the VRD problem to interpret and understand what the CNN is looking for when classifying relations. Since VRD contains many relationships the main focus will be on spatial relations between two objects. The focus will be on interpreting different models and configurations to have an understanding of what the model is learning and whether or not Activation Maximization is a useful method of doing so.
\\
\tab In this dissertation different CNN architectures will be trained and tested as Multi-Label and Single-Label Classification problems. The same will be done for the Feed Forward Neural Networks trained on the Geometric and Language Features. All the results will then be compared and contrasted. Activation Maximization will be used on the CNN models to interpret what the model is learning this will help give a better understanding to the achieved results.

\section{Background and Literature Review}
\subsection{Preamable}
In this chapter we will be going through the components that make up a CNN and the different architectures of CNNs. This chapter also includes literature reviews on existing works tackling the VRD problem and Activation Maximization.

\subsection{Convolutional Neural Networks}
\subsubsection{Image Components}
RGB images consists of 3 channels Red, Green and Blue. These 3 channels are each represented by a 2D array with each entry being a pixel value that ranges from 0 to 255. The pixel value in the array represents the colour intensity of the channel. Combining these 3 arrays together will yield the original image. The image details are represented as (Height ,Width,Channels) and this is the shape that the CNN expects as input. This input shape is kept the same for all images during training.


\subsubsection{Convolutional Layer}
The convolutional layer is composed of a kernel/filter which is a 2D matrix of a given size e.g (3x3) that performs matrix multiplication between itself and a portion of a region of the image. It does this by striding from left to right with a certain stride range e.g (Stride = 1 ) until the entire image is traversed. This extracts features from the image, for the first convolutional layer it would extract low level features such as colours and edges, as the CNN gets deeper with more layers the features will increase in complexity to become high level features such as the wheels of car, ears of a cat, tail of a dog etc. The kernel performs two types of operations on the image, one where the feature dimensionality is reduced compared to input and another where it is increased or stays the same due to padding. Padding is when the image width and height is increased and the pixel values that have been added are filled with 0, e.g Padding = 1 would increase the image width and height by 2. This is done as there could be valuable information in the pixel values on the outskirts of the image which would otherwise be lost as the kernel would combine them with the inner pixels.

\subsubsection{ReLU Layer}
After a convolutional layer there is an activation function, normally this function is the ReLU (rectified linear unit). This applies the activation function f(x)=max(0,x) which removes negative values by setting them to 0. The effect of this is that the CNN learns much faster as it increases the nonlinear properties of the decision function by allowing negative value through(as they are set to 0).

\subsubsection{Pooling Layer}
The pooling layer is used to decrease the computational power required to process data by reducing the spatial size of the convoluted features. The dominant features are extracted without losing major information and keeping the training model effective. The two types of pooling are average pooling which returns the average of all the values from the region and max pooling which returns the maximum value. Max pooling performs better than average pooling as average pooling mostly reduces the dimensionality but max pooling acts as a noise suppressant by discarding low values (Noise).

\subsubsection{Fully Connected Layer}
After a multitude of Convolutional, ReLU and Pooling layers the final output is passed through a fully connected layer which is where the high-level reasoning and decision making is done. Here the output from the last pooling layer is Flattened meaning the image output goes from a 2D array to a 1D array by concatenating the rows from underneath each other to the right side of the rows above. Then all the values are passed into neurons in a fully connected layer which all have connections to the activations as done in a regular Neural Network. 

\subsubsection{Final Layer}
The last layer is the output layer where all the neurons from the fully connected layer connect to. They connect to a specifically set amount of neurons which is defined by the amount of classes the CNN is being trained for. This last Densely Connected layer has an activation function. The final activation function of the CNN depends on the problem being solved. In this dissertation we will be focusing on two types of problems a Single Label Classification (SLC) problem and a Multi Label Classification (MLC) problem.

\subsubsection{Dropout Layer}
This layer is used to reduce overfitting of the training data on the model being trained. Overfitting is when the model doesnâ€™t generalize the parameters enough and represents the training data too much, meaning it would keep getting increasingly better accuracy during training but would have decreasing accuracy on the validation data. The dropout layer combats this by disabling neurons by setting them to 0 at each training stage which have a probability less than p = 0.5. This increases generalization as it forces the layer to learn the same concept with new neurons.

\subsubsection{Single vs Multi Label Classification}
The SLC problem consists of having only one correct label associated to the input. This problem would require for the last activation function to be a Softmax function. Softmax outputs a range of probabilities per class that all add up to 100\%, the highest probability is considered as the best label. \\
\tab The MLC problem consists of having multiple correct labels associated to the input. This problem would require for the last activation function to be a Sigmoid function. Sigmoid outputs a range of probabilities per class where each class ranges from 0\% to 100\% individually and independent of the other classes. Unlike the Softmax output the probabilities do not necessarily add up to 100\% therefore a threshold is usually set to what is an accepted output or not e.g anything above 50\% probability is accepted.

\subsubsection{Training and Terminology}
Once the structure of the CNN is setup it needs parameters for it to be compiled. The parameters consist of a \Gls{loss} function, optimizer and a metric. The \Gls{loss} function dictates how the model is penalized when the predicted values deviate from the true values. The optimizerâ€™s job is to make sure that the \Gls{loss} function is minimized as much as possible. Finally the metric used is what shows the final accuracy of the current training cycle between the predicted and the true values. The dataset would be seperated into 3 parts training/testing/validation , the training data is used to train the CNN and the test data is used to evaluate the trained model on unseen data. The validation data is used during training as unseen data to show if the model is being overfitted during training, it can be seen that it is overfitting when the training accuracy is going up and \Gls{loss} is going down but the opposite is occurring for the validation accuracy and \Gls{loss}. 

\subsubsection{Very Deep Convolutional Networks For Large-Scale Image Recognition}
In attempt to improve CNNs at the time, mainly the original architecture of Krizhevsky et al. (2012)~\cite{NIPS2012_4824}. The Visual Geometry Group ~\cite{Simonyan14c} focused on the depth of the convolutional network an important aspect which affects how high the level of the features could be learned. The architecture had itsâ€™ parameters fixed and the convolutional layers increased thus increasing the depth of the network, this was feasible as small kernels (3x3) had been utilized in all the layers. This method produced better results than previous architectures at the time. The two best performing modelsâ€™ weights have been released to the public. The CNN was trained using fixed-size 224x224 RGB images which had been preprocessed by subtracting the mean RGB value from each pixel. The images had been then passed through a stack of convolutional and max pooling layers with a window of 2x2 pixels of stride 2. The CNN has 5 max pooling layers and not all the convolutional layers are followed by a max pooling layer. The convolutional strides are performed by a kernel of size (3x3) with Stride = 1 and a padding of 'Same' to preserve the spatial resolution after the convolution. Each hidden layer is equipped with a ReLU function, the architecture of VGG16 ~\cite{Simonyan14c} is shown in Figure 1. After all the convolutional and max pooling layers there are 3 fully connected layers with the first two having 4096 neurons each and the last densely connected layer having 1000 output neurons for every class. The models had been trained as a SLC problem therefore the last activation function is Softmax. The top performing models had 16 and 19 layers therefore their acquired names are Visual Geometry Group 16 (VGG16)~\cite{Simonyan14c} and Visual Geometry Group 19 (VGG19).
\\
\tab
The model was trained using mini-batch processing with a batch size of 256. The \Gls{loss} function used was stochastic gradient descent(SGD) with a momentum of 0.9, drop out ratio of 0.5 and was trained for 74 \Gls{epoch}s. The initial  learning rate was 0.1 and was decreased by a factor of 10 when accuracy stagnated. It was trained using four NVIDIA Titan Black GPUs for two to three weeks.
\\
\begin{figure}[!htbp]
\includegraphics[scale=0.50,center]{VGG16Blocks.pdf}
\caption{Architecture of VGG16}
\end{figure}

\subsection{Visual Relationship Detection}
Visual relationships describe interactions between objects in images. Object detection models would equal images with objects of the same type but wouldn't understand the context behind them, for example you could have two images with a dog sitting near a cat or a dog chasing a cat but for the model they would mean the same thing. The problem of classifying those relationships is the large amount of possible relationships that the same pair of objects could have between them and which best fits the description. 


\subsubsection{Recognition Using Visual Phrases}
Sadeghi and Farhadi(2011)~\cite{VisualPhrases} approached the VRD problem by taking the visual phrase $<subject, relationship , object>$ as a unique class. It was believed that detecting visual phrases as a whole was much easier than detecting participating objects due to the fact the objects change when participating in relations such as in $<person,riding,horse>$ the personâ€™s leg might be obscured by the horse making it harder for the system to detect them. Since one class represents a visual phrase that makes it a SLC multi-class problem. To implement this theory the Pascal VOC2008 dataset was used to extract the 8 object classes and 17 visual phrases then Bing was used to gather images for the phrases and filtered manually to keep the relevant ones.  A concern was that the number of phrases grew exponentially and there wouldnâ€™t be enough training data for each visual phrase but it was thought that the number of useful visual phrases is significantly smaller than all the possible combinations. The results showed that the model achieved higher accuracies than the baseline. The problem with this is only 17 visual phrases had been used meaning that it was tested on a small dataset and it wouldnâ€™t be scalable.

\subsubsection{Visual Relationship Detection with Language Priors}
Lu et al(2016)~\cite{lu2016visual} showed that there is no need to have that many unique detectors. Having N objects and K \Gls{predicate}s it would take $O(N^2 K)$ unique detectors as used by Sadeghi and Farhadi(2011)~\cite{VisualPhrases}.By separating object and relationship detection it therefore reduces the amount of unique detectors to $O(N+K)$. This solved the exponential growth of classes problem, the reason this wasn't previously implemented by Sadeghi and Farhadi(2011)~\cite{VisualPhrases} is that object detection models were lacking. Another noted problem was that relationships occur in a Long Tail Distribution meaning that $<Car,On,Street>$ would be a very common occurrence while $<Elephant, Drinking , Milk>$ is a rare one which makes supervised learning a problem. The proposed solution came in 2 modules the visual appearance module to solve the problem of quadratic explosion of classes and the language module to solve the long tail distribution problem. The work was done on the Visual Genome dataset which contained 33k object categories and 42k relationships  categories making this a bigger dataset over the previously used one by Sadeghi and Farhadi(2011)~\cite{VisualPhrases}.
\\
\tab
The visual appearance module was done by first training an object detection CNN by Fine-Tuning the VGG with (Image Net weights) to classify N = 100 object categories. Then similarly another CNN was created by Fine-Tuning a new VGG with (Image Net weights) to classify K = 70 \Gls{predicate}s by using the Union box of two objects. The language module was done by having relationships of similar semantic relations be optimally mapped close together into an embedding space. The projection function (mapping) was done by firstly using word2vec (pre-trained word vectors) to have the two objects in a relationship projected into a word embedding space. Then the two vectors had been concatenated together and transformed into a relationship vector space. The relationship vector space is used to represent how all the objects interact with each other. If the language module had only seen $<person , riding , horse>$ and was shown a person riding an elephant it would predict $<person , riding , elephant>$ correctly as the word vectors of elephant and horse would be close to each other due to being rideable animals.
\\
\tab
The process went as follows, firstly the image was passed through the object detection CNN which located objects in the image. Then a pair of those objectsâ€™s bounding boxes was taken and their Union box was passed through the relationship detection CNN which predicted the probability of how likely a particular relationship is given the two bounding boxes. The probability and triplet output was then passed through the language module which then filtered out improbable relationships. The model was compared to the Visual phrases model as done  by Sadeghi and Farhadi(2011)~\cite{VisualPhrases} and the visual appearances model where only the visual apperances were taken into account. Due to the amount of possible combinations of objects and relationships there had been a shortage of training examples for the Visual phrase model which caused poor performance. The Visual module alone had problems discriminating against similar relationships. The full model had an 11\% improvement over the visual module alone which proves that the language module from similar relationships significantly helped relationship detection. 

\subsubsection{Detecting Visual Relationships with Deep Relational Networks}
Dai et al(2017)~\cite{Dai2017DetectingVR} proposed a Deep Relational Network to statistically exploit dependencies and spatial configurations between objects and their relationships to solve the problems of having a high diversity of visual appearances for relationships and the large amount of unique visual phrases. The solution proposed by Lu et al(2016)~\cite{lu2016visual} was noted to have a problem in the visual appearance module of high diversity with different object categories sharing the same relationship \Gls{predicate} and even some having nothing in common. This work has contributed two main things to solve the VRD problem a DR-Net which combines statistical models with deep learning and a state-of-the-art framework for visual relationship detection. 

Framework Process:
Object Detection : the proposed framework works by first detecting individual objects and localizing them with a bounding box and an appearance feature each. The object detector used is the Faster RCNN~\cite{NIPS2015_5638}.

Pair filtering:
For all the objects that had been detected by the Faster RCNN~\cite{NIPS2015_5638} the next step was to produce a set of object pairs, with a total of n objects in an image there will be $n(n-1)$ possible pairs. Most of these pair combinations are meaningless therefore they are filtered out using a low-cost neural network which focuses on spatial configuration and object categories. Once the pairs are finalized they are fed into a Joint Recognition module.

Joint Recognition:
A combination of the appearance module and a spatial module are used and their output is joined together in two fully connected layers. The appearance module is used on the bounding box of the image where it captures not only the object features but also itâ€™s surrounding area giving more context to it. The spatial module is used by taking spatial masks from the bounding boxes and downsampling them to a size of 32x32 which are then passed into three convolutional layers to output a spatial vector. 

Integrated Prediction:
The compressed pair feature outputted from the fully connected layers are combined with the subject and object feature vectors and fed into the DR-Net through multiple inference units. The subject and object features are used to remove the ambiguities caused by visual or spatial cues by exploiting the statistical relations of the \Gls{predicate}s most found between the subject and object. The DR-Net using a combination of all the data finally outputs a prediction by choosing the most probable classes for each of these components. 
The network was tested on the VRD and sVG datasets which produced results of 80.78\% Recall@50 for VRD \Gls{predicate} prediction and 88.26\% Recall@50 for sVG \Gls{predicate} prediction.


\subsubsection{A Study on the Detection of Visual Relationships}
Mizzi (2018)~\cite{detectionRelationships} has worked on expanding Dai et al(2017)'s ~\cite{Dai2017DetectingVR} spatial masks method of preparing images for training. Before the focus had been on the Image size and the amount of Convolutional layers a CNN has but this focuses on the way the images are prepared and fed into the CNN. Several methods had been explored and tested on two different CNN architectures which were VGG16 and VGG19. The dataset used was the Stanford VRD dataset.  Training was done as a SLC problem with evaluation metric of \Gls{Recall1}.

Union method takes the Union of the subject and and object bounding boxes cropping the image to those dimensions. The cropped image containing the subject and object image pixels was then resized to 224 by 224 and fed into the CNNâ€™s.

Union-WB method expands on the Union method by keep the pixels within the subject and object bounding boxes but removing all the pixels that do not fall within those bounding boxes by setting them to black [0,0,0]. 

\Gls{UnionWBB} method expands the Union-WB method by having the background set to black, subject bounding box set to green [0,255,0] and the object bounding box set to [0,0,255]. When there is an overlap of bounding boxes the pixel values are set to [0,255,255] a combination of green and blue.

Blur method the objective of this method was to exploit the lower layers of the CNN as they act as edge detectors. In this method the pixels within the subject and object bounding boxes are kept the same but the background was blurred. The Union of the two objects had been extracted and a Gaussian low-pass filter was used to blur the background. The background was blurred three ways by using standard deviations of 3(low), 5(medium) and 7(high) in the X and Y directions. 

Segment method took the subject and object segmentations, created a blank image, set the pixel values of where the original objects were to that of the previously taken segmentations.
This created more specific images with no noisy background. 

Segment-B method expands on the segment method by setting the original subject and object pixel values to green and blue masks. This same method was used in \Gls{UnionWBB} to generalize the data to only focus on visual relationships and not the objects themselves. 

The VGG16 and VGG19 models had been loaded and Fine-tuned to these datasets and evaluated using the \Gls{Recall1} metric. Together with the evaluation metrics the trained CNNâ€™s had been interpreted using Activation maximization and CAMs.

The significant results showed that on both CNN types the \Gls{UnionWBB} method outperformed all other methods. The reason behind this is that using Green and Blue spatial masks for the subject and objects would generalize relationships throughout different object categories. 
Having \Gls{UnionWBB} outperform Segment-B meant that the Segment-B method was too specific and that the CNNâ€™s prefer a more general input.

\subsection{Datasets}
\subsubsection{SpatialVOC2K: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects}
Muscat et al(2018)~\cite{belz-etal-2018-spatialvoc2k} came out with a dataset SpatialVOC2k which is a multilingual dataset focused on a portion of the VRD problem mainly the spatial relations. It was adapted from the PASCAL VOC2008 dataset by extracting 2026 images. These images had been chosen as they had 2 or more objects with given bounding boxes making this datasets main focus be a multilabel dataset. This dataset also proposed 18 Geometric features (Table 1) which proved to be useful for classification together with multiple models for training and evaluating this data. This dataset doesnâ€™t only focus on the VRD problem but also on the Depth prediction problem of objects. This dataset contains 17 English prepositions and 17 French ones, the process was done by translating the english prepositions into French and then eliminating those prepositions that had fewer than 3 examples. Figure 2 shows the occurrence distribution of classes in the dataset.

\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{SV2k.pdf}
	\caption{Class distribution SpatialVoc2k}
\end{figure}

\subsubsection{Stanford VRD}
The Stanford VRD dataset was introduced by Lu et al(2016)~\cite{lu2016visual}. This dataset contains 5000 images with 100 object categories and 70 \Gls{predicate}s. In total this dataset has 78872 single labels, 4504 examples with 2 labels, 685 examples with 3 labels and 71 examples with 4 or more labels. This distribution makes it mostly a SLC dataset. Figure 3 shows the occurrence distribution of classes in the dataset.

\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{VRD_Labels.pdf}
	\caption{VRD Distribution of Classes}
\end{figure}

\subsection{A Review on Multi-Label Learning Algorithms}
Zhang et al(2014)~\cite{6471714} showed that Multi-label classification(MLC) problems come in the form of training examples containing multiple labels associated with to them. Take the VRD problem a pair of objects will have multiple correct relationships attached to them and sometimes won't have a best descriptor for them making them all equally valid. Therefore it would be best to train it as a MLC problem where multiple high probability predictions are correct. Taking the VRD problem as a single label classification (SLC) problem would lead to alienating other correct possible answers. 

The main concepts taken from this paper for this dissertation are the evaluation metrics used for evaluating multi-label classifiers. The evaluation metrics can either be Example-based or Label-based. The example-based metrics work by evaluating the example instances separately and then returning the mean value across all of the test data. The label-based metrics are opposite to the one above as they evaluate the systems performance on each class label separately and then return the micro/macro-averaged values across all labels. Example-based Metrics include Subset Accuracy, Hamming \Gls{loss}, One-error, Average Precision, Coverage, Ranking \Gls{loss}, Accuracy, Precision, Recall, $F^B$ . The Label-based metrics focus on using the True Positive(TP), False Positive(FP), True Negative(TN) and False Negative(FN) for each label through out the test data. The number of examples is denoted as n, the ground truth label is $Y_{i}$ and $h(x_{i})$ is the predicted label output of the $i^{th}$ example.

Example Based:
\begin{equation}
Precision = \frac{1}{n}\sum_{i=1}^{n}\frac{|Y_{i}\cap h(x_{i})|}{|h(x_{i})|}
\end{equation}

\begin{equation}
Recall = \frac{1}{n}\sum_{i=1}^{n}\frac{|Y_{i}\cap h(x_{i})|}{|Y_{i}|}
\end{equation}

Label Based:
\begin{equation}
B \in { Accuracy, Precision, Recall, F^B} 
\end{equation}
Macro-averaging
\begin{equation}
â€¢B(h) = \frac{1}{q}\sum_{j=1}^{q} B(TP_j,FP_j,TN_j,FN_j)
\end{equation}
Micro-averaging
\begin{equation}
â€¢B(h) = B(\sum_{j=1}^{q} TP_j,\sum_{j=1}^{q} FP_j,\sum_{j=1}^{q} TN_j,\sum_{j=1}^{q} FN_j)
\end{equation}

\begin{equation}
Recall(TP_j,FP_j,TN_j,FN_j) = \frac{TP_j}{(TP_j + FN_j)}
\end{equation}

\begin{equation}
Precision(TP_j,FP_j,TN_j,FN_j) = \frac{TP_j}{(TP_j + FP_j)}
\end{equation}

\begin{equation}
F1(TP_j,FP_j,TN_j,FN_j) = \frac{(1+B^2)*TP_j}{((1+B^2)*TP_j+B^2*FN_j+FP_j)}
\end{equation}


Recall is described as the intersection of the relevant labels and retrieved labels over the total number of relevant labels. Precision is described as the intersection of the relevant labels and retrieved labels over the total number of retrieved labels. F1-Score is the harmonic average between precision and recall. These metrics are important for the evaluation of MLC models in this dissertation as standard SLC metrics aren't viable.

\subsection{Activation Maximization}
 Qin et al(2018) ~\cite{AM} explored different methods to try and understand what Convoulational Neural Networks had been seeing. These methods include Network Inversion, Deconvolutional Neural Networks, Network Dissection based visualization and finally the method that is used in this dissertation Activation Maximization. This method starts with a random input image and through back propagation maximizes an output class, this creates a visualized pattern representing that class. It was tested the hidden layers of CNNs to visualize their patterns and see what each layer is learning. It was also applied to the final layer of the CaffeNet which contained 1000 neurons where 1 neuron corresponds to 1 output class. When visualizing those neurons distinct output classes could be made out from the activation maps and easy guesses could be made to what class they belong to without even knowing what kind of classes the CNN was trained for.
\\ 
For this dissertation once the spatial relations are trained and tested, Activation Maximisation will be used to maximize the outputs of all classes for all the Convolutional Neural Network models. This would be an easy and useful way of visualizign the final layer of the CNN for human understanding.


\section{Methodology}
In this section we will be able to see the procedures needed to be undertaken to reach the goals and objectives of this project.

\subsection{Data Preparation}
The Stanford VRD and SpatialVoc2k had been used to train the different network architectures and evaluate any differences between them. The dataset had been prepared for three uses, training a Convolutional Neural Network using images, training a Feed Forward Network using Geometric features and a Feed Forward Network using Geometric with Language Features. 

\subsubsection{Stanford VRD Dataset}
The dataset information is stored in the form of dictionaries in two JSON files containing the training and testing data. The data was loaded and the image location, object names, relationships and bounding box locations had been extracted. Firstly the relationships had been quantified and filtered to only include those that are spatial prepositions. Those with synonyms have been combined under one \Gls{predicate} name and those with low amounts of examples hadnâ€™t been included. Then the images had been loaded using their image location so that their height and width could been extracted to add to the existing data. Data entries where a pair of objects with same bounding boxes had multiple relationships to them had been concatenated to form one data entry with multiple relationships. Then the final list was randomized and stratified sampled into training/testing/validation data with a split of 60\%/20\%/20\%. Stratified sampling was done by first distributing all the multi-label entries and then distributing the single-label entries as it is easier to fit them and would create a more specific distribution. This was repeated for 10 times to have 10 different training/testing/validation datasets so that multiple models could be trained and their averages taken.

\subsubsection{SpatialVoc2k Dataset}
This dataset was easier to work with as it already had the image width and height already stored in itâ€™s JSON file together with the fact that it was specialized for Spatial relations therefore no filtering of spatial relations was necessary apart from removing 3 classes with less than 3 instances each. The data had already multiple relationships grouped for a pair of objects, therefore all that was needed was to randomize it and to apply stratified sampling for training/testing/validation datasets with a split of 60\%/20\%/20\%, the same process was used as before by first distributing the multi-labels and then the single-labels. This was repeated for 10 times to have 10 different training/testing/validation datasets so that multiple models could be trained and their averages taken.

\subsubsection{Geometric Datasets}
From the previously created datasets more datasets have been created. These datasets contained the geometric features derived from the pair of bounding boxes given to the objects. To preserve the distribution of classes the training/testing/validation data had not been joined together but kept separately, this would enable for the results to be compared fairly. The object labels together with the directions are encoded using \Gls{ohe}. The object labels are considered as the Language Features, these wouldn't be considered when training a model with only Geometric Features.
Note : Let distance from image edge of left and right edges be a1,b1 for first box and a2,b2 for second box the same thing was done for top and bottom edges for c1,d1 and c2,d2.

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|lll}
\cline{1-2}
F0 : & Object Label Ls: Subject \\ \cline{1-2}
F1 : & Object Label Lo: Object  \\ \cline{1-2}
F2 : & Area of Objs normalized by Union Box Size \\ \cline{1-2}
F3 : & Area of Objo normalized by Union Box size \\ \cline{1-2}
F4 : &  Ratio of Bounding Box Objs to that of Objo  \\ \cline{1-2}
F5 : &  Distance between bounding box centers normalized by Union Box Diagonal \\ \cline{1-2}
F6 : & Area of Overlap of Bounding Boxes normalized by area of smaller bounding box \\ \cline{1-2}
F7 : & Minimum Distance between the two bounding boxes \\ \cline{1-2}
F8 : & Position of Objs relative to Objo in terms of North,South,East,West \\ \cline{1-2}
F9-F10 : & F9 = (a2-a1)/(b1-a1), F10 = (b2-a1)/(b1-a1) \\ \cline{1-2}
F11-F12 : & F11 = (c2-c1)/(d1-c1), F12 = (d2-c1)/(d1-c1) \\ \cline{1-2}
F13 : & Aspect ratio of box Objs \\ \cline{1-2}
F14 : & Aspect ratio of box Objo \\ \cline{1-2}
F15 : & Relationship \\ \cline{1-2}
\end{tabular}
\caption{Geometric Features}
\end{table}

\subsubsection{Single Label Datasets}
The datasets that have been created thus far all had multiple relationships between the pair of objects. These datasets would be used to train MLCs therefore for comparison they have been taken as they are and expanded to also train new networks as a SLC problem. Meaning that they were loaded in preserving their distribution and if a data entry had two or more relationships attached to it then it would separate into multiple single-label entries. This had to be done in this order as if you had used stratified sampling to randomize and distribute the single labels first then there would be entries with same characteristics with different relationships but in different datasets and it would reduce the amount of possible Multi-Labels.

\subsection{Image Preparation}
As it has been shown in A Study on the Detection of Visual Relationships (2018) ~\cite{detectionRelationships} that the best performing method for the VGG16 architecture is the \Gls{UnionWBB} method. This is where the Union box of the bounding boxes of the pair of objects has been taken, with the background set to Black $[0,0,0]$ ,Subject Bounding Box set to Green $[0,255,0]$ and the Object Bounding Box set to Blue $[0,0,255]$.

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.25]{originalImage.pdf}
\caption{Image from VRD dataset}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.35]{UnionBox.pdf}
\caption{Union Box of $<Laptop, Left-of , Bottle>$}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.35]{UnionWBB.pdf}
\caption{Union-WB-B  of $<Laptop, Left-of , Bottle>$}
\end{figure}

Since this method doesn't use any of the actual objects but only their bounding boxes, there is no need for an actual image only itâ€™s meta-data. Hence the datasets had been prepared in the form of [Labels, width, height, subject bounding box, object bounding box, subject label, object label]. From this data OpenCv was used create a black image of a certain width, height together with green, blue rectangles added to it in positions of the subject/object bounding boxes. This method was much faster as it didnâ€™t require any space, processing and loading time for images. The created images are then resized to 224x224 as that is what the VGG16 network had been initially trained using. It is important that during creation the bounding boxes do not overwrite each other in the image but instead if there is an overlap of objects, the overlapping pixel values will be set to $[0,255,255]$. This is so that the spatial masks are fed into separate colour channels maintaining their true form.


\subsection{Training}
Two CNNs are trained using these datasets, one is a Fine-Tuned VGG16 with ImageNet weights and another is a SmallVGG with no preloaded weights. Two Feed Forward Neural Networks had been trained using these datasets one containing only Geometric Features and another Geometric Features with Language Features. Activation Maximization is then applied on the final output layers of the CNN models and their activation maps are recorded.

\subsubsection{VGG16}
The process of Fine-Tuning the VGG16 model started by first loading the model with all its layers and weights. The last Dense layer was replaced with a new Dense layer with a specified amount of classes and activation function that is set before training according to the problem. The MLC problem would use the \Gls{loss} function of \Gls{binaryCross} and an activation function of Sigmoid while the SLC problem would use the \Gls{categoricalCross} \Gls{loss} function and an activation function of Softmax. All the layers had been set to non-trainable except for the last dense and fully connected layers. Using the optimizer stochastic gradient descent(SGD) with a learning of 0.001 and a Nesterov momentum of 0.9 the model was run for 5 \Gls{epoch}s for the VRD dataset and 15 \Gls{epoch}s for SpatialVoc2k dataset. Once the model finished training it was saved and a new model was created with all the layers set to non-trainable except that of the fully connected layers and the last convolutional block (Conv Block 5). The previous models weights had been loaded into the new model and again run for 5 \Gls{epoch}s for the VRD dataset and 15 \Gls{epoch}s for SpatialVoc2k dataset. Finally the last step was repeated but with a learning rate of 0.00001 for 5 \Gls{epoch}s for the VRD dataset and 15 \Gls{epoch}s for SpatialVoc2k dataset. This was done 10 times for both datasets and both problem types.

\subsubsection{SmallVGG}
The SmallVGG was created out of 7 Convolutional Layers instead of 16 like the VGG16. This was trained from scratch for 10 \Gls{epoch}s for the VRD dataset and 20 \Gls{epoch}s for the SpatialVoc2k dataset. The same \Gls{loss}, optimizer and activation functions have been used for this as the VGG16 network. This was trained as both MLC and SLC problems and evaluated using the appropriate MLC and SLC metrics. This was done 10 times for both datasets and both problem types.

\subsubsection{Feed Forward Neural Network}
A Feed Forward Neural Network was trained using the Geometric Features and Geometric with Language Features created from the datasets. The network had been made up out of two fully connected layers (256 neurons followed by 128 neurons) and a densly connected output layer containing the number of output classes with an activation function according to the problem being solved. The same parameters were applied to it with regards to MLC and SLC problems. The optimizer ADAM was used with default values and was trained for 10 \Gls{epoch}s. This was done 10 times for both datasets and both problem types and their results recorded.

\subsubsection{Data Generators}
Due to having a large data set custom Image generators had been used to load the image meta data and create the images. The batch size for the image generators had been set to 32. Once a batch of images had been created their pixel intensities had been rescaled to be between 0 and 1, this enables for faster and more precise training of CNNâ€™s.
\\
The Feed Forward network used custom data generators to load all the geometric features and rescale all the input features to be between 0 and 1. The language features and compass directions had been \Gls{ohe} so that they could be processed by the Neural Network.

\subsection{Evaluation and Metrics}
Once the model had been fully trained an Image Generator had been loaded in with the test data. The model used the predict functionality to predict probabilities on given inputs. The predict function was used over the evaluation function as specific MLC evaluation metrics had to be implemented to evaluate the MLC models. On a given input prediction a set of probabilities had been returned corresponding to the probability of each class detected by the model. Since this is a MLC problem the Sigmoid activation function was used so each class had their own independent probability ranging from 0\% to 100\%. A threshold of 50\% had been chosen as a probability cut off point. If a value was above 50\% then it would be turned on (set to 1) and if it was below then it would have been turned off (set to 0). The predicted values had then been compared to the ground truth labels and documented per class. The True positive, False positive , True negative and False negative values for each class are decided with the use of the help of the contingency table in Figure.7.

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.35]{TPFP.pdf}
\caption{Contingency table of Ground Truth vs Predicted Values }
\end{figure}

Once predictions have been made on the testing data and values recorded, the evaluation metrics taken from "A Review on Multi-Label Learning Algorithms"~\cite{6471714} mentioned above were run and recorded. These metrics are used to evaluate MLC models. To evaluate SLC models \Gls{Recall1},Precision@1 and F1-Score@1 had been utilized. Since @1 is used it means that the highest probability value is used for the metric. The predictions were first ranked in descending order by probability and the highest predicted class is compared to the ground truth value. The results are recorded per class together with the Micro/Macro averages. All the models corresponding to the same datasets of the same problem type had been evaluated and their averages taken.

\subsection{Interpreting the models}
To interpret the models via Activation Maximization the trained models are first loaded in, then the last activation function of the fully connected prediction layer was chosen and replaced by a linear activation. Activation maximization was then initialized on a random input image and run for 1024 back propagation iterations maximizing the output for each class. The resulting images were then saved and compared for each model. It was made sure that the input range was set to between 0 and 1 as that is scale the CNN's had been trained on.

\section{Findings}
\subsection{Preamble}
In this chapter the results achieved from various models trained in Chapter 3 are presented. Every dataset shows the results achieved on the test data for both the VGG16 and Feed Forward Neural Network together MLC vs SLC training methods.

\subsection{VRD Dataset Evaluation Results}
\subsubsection{VGG16 Evaluation Results}
This section shows the relevant results obtained by models for both MLC and SLC problems.The presented results are rounded to 2 decimal places.
\newpage
\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class   &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
English & Precision &  Recall   & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
above	&	0.66	&	0.44	&	0.53	&	0.52	&	0.63	&	0.57 \\ \cline{1-7}
at	    &	0.00	&	0.00	&	0.00	&	0.11	&	0	&	0 \\ \cline{1-7} 
behind	&	0.56	&	0.03	&	0.06	&	0.31	&	0.26	&	0.28 \\ \cline{1-7}
below	&	0.38	&	0.03	&	0.06	&	0.35	&	0.22	&	0.27 \\ \cline{1-7} 
beside	&	0.00	&	0.00	&	0.00	&	0.03	&	0	    &	0 \\ \cline{1-7} 
front	&	0.00	&	0.00	&	0.00	&	0.25	&	0.21	&	0.23 \\ \cline{1-7} 
in	    &	0.79	&	0.48	&	0.59	&	0.65	&	0.55	&	0.6 \\ \cline{1-7} 
left	&	0.13	&	0.00	&	0.01	&	0.3	    &	0.22	&	0.25 \\ \cline{1-7} 
near	&	0.00	&	0.00	&	0.00	&	0.19	&	0.03	&	0.05 \\ \cline{1-7} 
next	&	0.44	&	0.02	&	0.04	&	0.3	    &	0.44	&	0.36 \\ \cline{1-7} 
on	    &	0.66	&	0.69	&	0.67	&	0.56	&	0.86	&	0.68 \\ \cline{1-7} 
right	&	0.10	&	0.00	&	0.00	&	0.24	&	0.2	    &	0.22 \\ \cline{1-7} 
under	&	0.48	&	0.05	&	0.09	&	0.33	&	0.45	&	0.38 \\ \cline{1-7}
\end{tabular}
\caption{VGG16(VRD) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type  & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average &	0.26	&	0.11	&	0.13	&	0.26	&	0.26	&	0.24 \\ \cline{1-7}
Micro-Average &	0.68	&	0.29	&	0.41	&	0.47	&	0.47	&	0.47 \\ \cline{1-7}
Example       &	0.31	&	0.31	&	0.31  \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for VGG16(VRD)}
The results in Table 2 do not include the following labels:=\{ by, over, top\} as they have achieved a result of 0 for all metrics. These results show the averages of 10 VGG16 models Fine-Tuned using the VRD dataset as MLC and SLC problems. The Example based metric as seen in the MLC problem is not included for the SLC problem as it produces equivalent results to that of the Micro-Average Metric.  The SLC results shown are for @1, meaning only the highest probability labels are predicted to the ground truth label.
\vspace{-4mm}
\end{table}

\subsubsection{Feed Forward Evaluation Results}
This section shows the relevant results obtained by the Feed Forward models for both MLC and SLC problems.The presented results are rounded to 2 decimal places.
\newpage
\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
English & Precision &  Recall    & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
above	&	0.66	&	0.62	&	0.64	&	0.57	&	0.64	&	0.60 \\ \cline{1-7}
at	    &	0.26	&	0.11	&	0.16	&	0.19	&	0.13	&	0.15 \\ \cline{1-7} 
behind	&	0.56	&	0.41	&	0.47	&	0.46	&	0.49	&	0.48 \\ \cline{1-7} 
below	&	0.39	&	0.24	&	0.29	&	0.33	&	0.25	&	0.28 \\ \cline{1-7} 
beside	&	0.20	&	0.04	&	0.07	&	0.13	&	0.07	&	0.09 \\ \cline{1-7} 
by 	    &	0.23	&	0.05	&	0.07	&	0.12	&	0.07	&	0.09 \\ \cline{1-7} 
front	&	0.43	&	0.26	&	0.32	&	0.34	&	0.34	&	0.34 \\ \cline{1-7} 
in   	&	0.80	&	0.68	&	0.73	&	0.72	&	0.70	&	0.71 \\ \cline{1-7} 
left	&	0.31	&	0.12	&	0.17	&	0.24	&	0.19	&	0.21 \\ \cline{1-7} 
near	&	0.22	&	0.07	&	0.10	&	0.17	&	0.13	&	0.15 \\ \cline{1-7}
next	&	0.35	&	0.21	&	0.26	&	0.29	&	0.32	&	0.30 \\ \cline{1-7} 
on	    &	0.77	&	0.76	&	0.76	&	0.70	&	0.77	&	0.73 \\ \cline{1-7} 
over	&	0.30	&	0.09	&	0.14	&	0.24	&	0.11	&	0.15 \\ \cline{1-7} 
right	&	0.27	&	0.12	&	0.16	&	0.23	&	0.19	&	0.20 \\ \cline{1-7} 
top	    &	0.18	&	0.03	&	0.06	&	0.12	&	0.05	&	0.06 \\ \cline{1-7} 
under	&	0.54	&	0.50	&	0.51	&	0.43	&	0.54	&	0.48 \\ \cline{1-7}
\end{tabular}
\caption{Feed Forward NN(VRD) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type  & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average &	0.40	&	0.27	&	0.31	&	0.33	&	0.31	&	0.31 \\ \cline{1-7}
Micro-Average &	0.63	&	0.47	&	0.54	&	0.51	&	0.51	&	0.51 \\ \cline{1-7}
Example       &	0.49	&	0.49	&	0.49 \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for Feed Forward NN(VRD)}
These results show the averages of 10 Feed Forward Neural Network models trained using VRD Geometric Features as MLC and SLC problems. The Example based metric as seen in the MLC problem is not included for the SLC problem as it produces equivalent results to that of the Micro-Average Metric.  The SLC results shown are for @1, meaning only the highest probability labels are predicted to the ground truth label.
\vspace{-4mm}
\end{table}

\subsubsection{Activation Maximization Evaluation Results}
In this section the activation maps pretaining to the output classes of the VGG16 model trained on the VRD dataset are shown.
\newpage
\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{VRD_act.pdf}
	\caption{Activation Maximization on VGG16(VRD) Classes for MLC and SLC}
	\vspace{-4mm}
\end{figure}

\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{NewVRD2.pdf}
	\caption{Activation Maximization on VGG16(VRD) Classes for MLC and SLC}
	\vspace{-4mm}
\end{figure}
\newpage
\subsection{SpatialVoc2k Dataset Evaluation Results}
In this section the results evaluated from all the models trained on the SpatialVoc2k dataset are presented.

\subsubsection{VGG16 Evaluation Results}
\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l}
\cline{1-8}
Class & Class & MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-8}
French & English & Precision &  Recall    & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-8}
a cote de	      &	next to	&	0.67	&	0.52	&	0.58	&	0.27	&	0.20	&	0.23 \\ \cline{1-8}
a l'exterieur de  &	outside	&	0.00	&	0.00	&	0.00	&	0.03	&	0.01	&	0.01 \\ \cline{1-8}
au dessus de	  &	beyond	&	0.00	&	0.00	&	0.00	&	0.15	&	0.09	&	0.11 \\ \cline{1-8}
au niveau de	  &	near/at the level of	&	0.68	&	0.47	&	0.56	&	0.30	&	0.17	&	0.22 \\ \cline{1-8}
aucun	          &	 none	&	0.00	&	0.00	&	0.00	&	0.10	&	0.02	&	0.03 \\ \cline{1-8} 
autour de	&	around	&	0.00	&	0.00	&	0.00	&	0.47	&	0.31	&	0.32 \\ \cline{1-8}
contre	&	against	&	0.51	&	0.14	&	0.21	&	0.22	&	0.16	&	0.18 \\ \cline{1-8}
dans	&	in/into	&	0.00	&	0.00	&	0.00	&	0.37	&	0.28	&	0.30 \\ \cline{1-8}
derriere	&	behind	&	0.63	&	0.42	&	0.50	&	0.28	&	0.34	&	0.30 \\ \cline{1-8}
devant	&	front/before	&	0.64	&	0.46	&	0.53	&	0.30	&	0.34	&	0.32 \\ \cline{1-8} 
en face de	&	across from	&	0.00	&	0.00	&	0.00	&	0.07	&	0.02	&	0.03 \\ \cline{1-8} 
loin de	&	far from	&	0.70	&	0.25	&	0.36	&	0.37	&	0.30	&	0.33 \\ \cline{1-8}
pres de	&	near/by	&	0.75	&	0.72	&	0.73	&	0.32	&	0.44	&	0.37 \\ \cline{1-8}
sous	&	under/below	&	0.64	&	0.41	&	0.50	&	0.34	&	0.48	&	0.40 \\ \cline{1-8}
sur	&	on	&	0.67	&	0.45	&	0.53	&	0.36	&	0.46	&	0.40 \\ \cline{1-8}
\end{tabular}
\caption{VGG16(SpatialVoc2k) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type  & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average &	0.35	&	0.23	&	0.27	&	0.23	&	0.21	&	0.21 \\ \cline{1-7}
Micro-Average &	0.69	&	0.47	&	0.55	&	0.30	&	0.30	&	0.30 \\ \cline{1-7}
Example      &	0.58	&	0.46	&	0.51  \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for VGG16(SpatialVoc2k)}
The results in Table 6 do not include the following labels:=\{ par dela(beyond), le long de(along)\} as they have achieved a result of 0 for all metrics. These results show the averages of 10 VGG16 models Fine-Tuned using the SpatialVoc2k dataset as MLC and SLC problems. The Example based metric as seen in the MLC problem is not included for the SLC problem as it produces equivalent results to that of the Micro-Average Metric. The SLC results shown are for @1, meaning only the highest probability labels are predicted to the ground truth label.
\vspace{-4mm}
\end{table}

\subsubsection{Feed Forward Evaluation Results}
In this section the results for the Feed Forward Neural Networks are presented trained on the SpatialVoc2k dataset as MLC and SLC problems.

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l}
\cline{1-8}
Class & Class & MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-8}
French & English & Precision &  Recall    & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-8}
a cote de	&	next to	&	0.64	&	0.57	&	0.60	&	0.25	&	0.11	&	0.14 \\ \cline{1-8} 
a l'exterieur de	&	outside	&	0.00	&	0.00	&	0.00	&	0.10	&	0.07	&	0.07 \\ \cline{1-8}
au dessus d e	&	beyond	&	0.00	&	0.00	&	0.00	&	0.18	&	0.03	&	0.04 \\ \cline{1-8}
au niveau de	&	near/at the level of	&	0.62	&	0.39	&	0.47	&	0.24	&	0.02	&	0.04 \\ \cline{1-8}
autour de	&	around	&	0.00	&	0.00	&	0.00	&	0.58	&	0.44	&	0.44 \\ \cline{1-8}
contre	&	against	&	0.51	&	0.25	&	0.33	&	0.24	&	0.10	&	0.14 \\ \cline{1-8}
dans	&	in/into	&	0.17	&	0.03	&	0.04	&	0.49	&	0.33	&	0.36 \\ \cline{1-8}
derriere	&	behind	&	0.71	&	0.41	&	0.52	&	0.35	&	0.37	&	0.35 \\ \cline{1-8}
devant	&	front/before	&	0.67	&	0.43	&	0.52	&	0.36	&	0.34	&	0.35 \\ \cline{1-8}
en face de	&	across from	&	0.17	&	0.01	&	0.03	&	0.23	&	0.08	&	0.11 \\ \cline{1-8}
le long de	&	along/by	&	0.00	&	0.00	&	0.00	&	0.03	&	0.01	&	0.01\\ \cline{1-8} 
loin de	&	far from	&	0.69	&	0.36	&	0.46	&	0.36	&	0.22	&	0.26\\ \cline{1-8} 
pres de	&	near/by	&	0.76	&	0.79	&	0.78	&	0.32	&	0.64	&	0.43\\ \cline{1-8}
sous	&	under/below	&	0.78	&	0.65	&	0.70	&	0.50	&	0.66	&	0.57\\ \cline{1-8}
sur	&	on	&	0.79	&	0.75	&	0.76	&	0.48	&	0.73	&	0.58  \\ \cline{1-8}
\end{tabular}
\caption{Feed Forward NN(SpatialVoc2k) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type  & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average &	0.38	&	0.27	&	0.31	&	0.29	&	0.25	&	0.23\\ \cline{1-7}
Micro-Average &	0.70	&	0.51	&	0.59	&	0.34	&	0.34	&	0.34 \\ \cline{1-7}
Example       &	0.64	&	0.52	&	0.57 \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for Feed Forward NN(SpatialVoc2k)}
The results in Table 9 does not include the following label:=\{ aucun(none)\} as it has achieved a result of 0 for all metrics. These results show the averages of 10 Feed Forward NN models trained using the SpatialVoc2k dataset as MLC and SLC problems. The Example based metric as seen in the MLC problem is not included for the SLC problem as it produces equivalent results to that of the Micro-Average Metric. The SLC results shown are for @1, meaning only the highest probability labels are predicted to the ground truth label.
\vspace{-4mm}
\end{table}

\subsubsection{Activation Maximization Evaluation Results}
\newpage
\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{Spatial_act.pdf}
	\caption{Activation Maximization on VGG16(SpatialVoc2k) Classes for MLC and SLC}
\end{figure}
\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{VGG16_Spatial2.pdf}
	\caption{Activation Maximization on VGG16(VRD) Classes for MLC and SLC}
\end{figure}
\newpage
\section{Analysis of Results}
\subsection{Preamble}
In this chapter the results obtained in Chapter 4 are discussed. This chapter is split into three parts, -Section 5.2 analyses the VGG16 results, -Section 5.3 analyses the SpatialVoc2k results and -Section 5.4 discusses future work. 

\subsection{VRD Results}
In this section the results obtained by the VRD dataset will be discussed and compared across models and training methods.

\subsubsection{Multi-label vs Single-label Classification}
The VGG16 Single-Label Classification training method outperformed the Multi-Label Classification training method for all per-\Gls{predicate} accounts of Recall and F1. The MLC Precision metric recorded better results for labels:=\{ above, behind, below, in, next, on, under\} compared to that of SLC. The Macro-Average, meaning the average of all the \Gls{predicate}s combined achieved equal results for precision and higher results in Recall/F1 for the SLC method. This means that even though the MLC training method returned less \Gls{predicate} results the ones it did return had been more relevant compared to that of the SLC. SLC had higher or equal Recall for all \Gls{predicate}s compared to MLC meaning that most of the relevant labels had been retrieved for SLC compared to that of MLC. SLC has more \Gls{predicate} results due to the difference in metrics, MLC metrics have a threshold of 50\% which filter out low probability labels, while SLC uses @1 so low probability labels can be retrieved as long as they are the highest among the predicted. 
\\
\tab The Feed Forward Neural Network had a greater MLC precision score than that of the SLC for all accounts of per-\Gls{predicate} labels while SLC had better results for recall. Even though SLC produced higher recall for all \Gls{predicate}s the results aren't far off from MLC, the greater difference is in the precision scores. SLCs Micro-Average score shows 51\% for all metrics while MLC's Micro-Average score shows Precision: 63\%, Recall: 47\%, F1: 54\%. The difference in Micro-Average Precision made the MLC method outperform in F1 score over the SLC method.
\\
The MLC method proved to be more precise than the SLC while SLC was more sensitive and had recalled the most relevant labels.

\subsubsection{VGG16 vs Feed Forward Neural Network}
The Feed Forward Neural Network trained on the Geometric Features achieved better results than the Fine-Tuned VGG16 CNN trained on spatial masks. The geometric features didn't only contain spatial configurations of the bounding boxes but also the subject/object categories which the spatial masks couldn't possibly learn. These language features that have been combined with the geometric features are the main reason that the Feed Forward Network outperforms the VGG16. Language features allow for more ambiguous \Gls{predicate}s to be predicted as these \Gls{predicate}s would be indistinguishable as spatial masks or geometric features alone but would have a very distinct occurance between a pair of object categories. To show the importance of the language features in the appendix there are results of the Feed Forward Neural Network trained using only Geometric Features meaning that the subject/object categories have been removed. These results show a lower performance in all metrics than that of a Fine-Tuned VGG16 for both MLC and SLC. They also show that SLC has a higher F1 score than MLC which is the opposite of what was shown in the networks trained with both Language and Geometric features.
\\
\tab The results achieved by the Feed Forward Neural Network using Geometric and Language Features outperform VGG16 for all metrics. For real life implementation I would suggest using CNN's for object detection and localization from there extract the Geometric and Language features from the pair of objects and train a Feed Forward Neural Network. The Feed Forward Neural Network takes less time to train and produces better results. To improve these results word2vec from Lu et al(2016)~\cite{lu2016visual} should be used on the language features instead of \Gls{ohe}, this would enable for even faster training as you won't have a large input shape for all the possible object categories and increased results on unseen category combinations as was done by Muscat-Belz(2018)~\cite{belz-etal-2018-spatialvoc2k} .

\subsubsection{Activation Maximization}
The Activation Maps show what the CNN is looking for that specific output class. Knowing that the subject is Green and object is Blue we can interpret these maps. The \Gls{predicate}s Above,Over have similar activation maps while Top has a less clear version of those maps. These maps show that Green is Above/Over/Top the Blue colour which is a good indicator of how the network perceives these spatial relations. The below and under activation maps show the Green colour below/under the Blue colour which is a straight forward indicator. Left and Right also have quite clear activation maps that are easily understandable. The \Gls{predicate} "In" is shown to be a mix of $[0,255,255]$ pixels surrounded by an outline of Blue pixels indicating that the Green subject is inside the Blue object. "On" has multiple small concentrations  of Green points above small concentrations of $[0,255,255]$ followed by Blue points, this indicates that the \Gls{predicate} "On" has a Green subject constantly in contact with the Blue object while being in a higher position over it at the same time. While "above/over/top" \Gls{predicate}s have the concentration of Green confined to the upper limits of the activation map "On" has them more spread out. The \Gls{predicate}s Beside, By, Near and Next have similiar looking SLC activaion maps which is good as they are similiar \Gls{predicate}s, the MLC activation maps for those \Gls{predicate}s didn't show anything clear and it makes sense as the MLC results had been poorer that the SLC ones. The MLC activations produced similar looking maps to that of the SLC activations but they are less clear.


\subsubsection{Conclusions}
These results dictate that even though the Visual Relationship Detection problem should be taken as a Multi-Label Classification problem the label distributions in the datasets play a large role in how well the model will be able to recognise multiple relationships. Training a model which is heavily composed of single labels(VRD) as a Multi-Label model hinders the models ability to recall labels correctly. The Feed Forward Neural Network outperforms the CNN mainly due to the Language Features otherwise having only Geometric Features produce lower scores than the CNN. Language features also greatly close the gap in results between MLC and SLC for the VRD dataset. The activation maps proved to be useful in confirming the metric results between the training methods.

\subsection{SpatialVoc2k Results}

\subsubsection{Multi-label vs Single-label Classification}
SpatialVoc2k is multi-label dataset therefore the models trained using it achieved better results when they are trained as MLC rather than SLC problems. The VGG16 MLC models scores higher for all the evaluation metrics in the Macro and Micro-averages. All the non-zero per-\Gls{predicate} results returned by the MLC model had higher score than that of SLC except for the \Gls{predicate} "far from" which had a higher recall for SLC method. The SLC method has noteably higher results on the \Gls{predicate}s "around/into" compared to the zero results retrieved by MLC, this can be explained further by looking at the activation maps for insight to what the CNN is looking for. The Feed Forward Neural Network exhibits the same MLC vs SLC result distribution as the VGG16. This time the language features didn't close the gap between the MLC and SLC results as previously seen for the VRD results.

\subsubsection{VGG16 vs Feed Forward Neural Network}
The Feed Forward Neural Network achieved better results that the VGG16 for all the Macro/Micro-Average metrics.
The \Gls{predicate}s:=\{next to, near, front, behind\} had higher results for the Spatial Features over the Geometric and Language Features. Geometric Features alone (without language features) performed worse than the Spatial Features for all metrics and per \Gls{predicate} results, this can be seen in the appendix Table 17.

\subsubsection{Activation Maximization}
MLC has clearer maps than SLC except on "Around" and "In" where SLC has had higher metric scores. The SLC activation maps show rigid square shapes while the MLC maps show smoother and more curvy like activation maps. The SLC activation maps of \Gls{predicate}s:=\{next to, near, behind, front, far from, near by, on\} do not show anything concretely interpretable and the maps look quite similar. "Around" is shown to have the Green subject around a Blue object, "In" has a Blue box outline surrounding the pixel values of $[0,255,255]$ which means that Green is inside the Blue object and the Blue object is bigger in size. The MLC activation map "Front" shows a light green covering around the map which is a good indicator that Green subject is in front of the Blue object. The MLC \Gls{predicate}s:=\{Next to, Near, and Neary By\} have a similar patter inside them where the middle part of the activation map contains vertical lines, this would be interpreted as that when objects appear with those \Gls{predicate}s they would be close to each other and at the same level.
 
\subsubsection{Conclusions}
The MLC method is prefered over the SLC method for Multi-Label datasets and problems. Geometric Features outperform the Spatial Features but the Geometric Features alone without he Language Feature perform worse than the Spatial Features. Activation Maximization proved useful to understanding which \Gls{predicate}s are similar to each other and why a certain training type outperforms the other.

\subsection{Conclusion}
The label distribution in the dataset plays a big factor in deciding the way the model should be trained as MLC vs SLC. The more information a model is given the better it performs as was seen by adding and removing language features from the Feed Forward Neural Networks. Activation maximization is a useful way of understading and interpreting what CNN's are learning and why some methods perform better than others. It groups same \Gls{predicate}s together which is useful if you would want to train a multi-label dataset as a single label by grouping \Gls{predicate}s under the same activation maps under a single label. The VGG16 outperfomed the SmallVGG for all \Gls{predicate}s and results on both datasets as can be see in the appendix results. The MLC activation maps produced by the SmallVGG trained used SpatialVoc2k had been incomprehensible and mostly random meaning that the CNN hasn't learned anything using the MLC training method.

\subsubsection{Future Work}
Given that the Spatial Masks outperform the Geometric Features alone it would be wise to explore a combination of Spatial Masks and Language Features. It would be interesting to apply Activation Maximization to Feed Forward Neural Networks to be able to achieve inputs that maximize certain classes. These activation results would give more quantifiable outputs to relationships. The Language Features included with the Geometric Features can be upgraded from \Gls{ohe}s to word2vec to save space and increase accuracy over unseen examples as was done by Muscat-Belz(2018)~\cite{belz-etal-2018-spatialvoc2k}. More Geometric Features should be explored not including the language features, geometric models took less time to train and it would be better to get them to the same levels of accuracy as that of Spatial Masks.

\newpage
\section{Appendices}
\subsection{VRD}
Extra results pretaining to the VRD dataset for the SmallVGG and Feed Forward Neural Network without language features.

\subsubsection{SmallVGG}
\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class   &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
English & Precision &  Recall   & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
above	&	0.65	&	0.43	&	0.51	&	0.39	&	0.59	&	0.47  \\ \cline{1-7}
at	    &	0.00	&	0.00	&	0.00	&	0.06	&	0.01	&	0.01  \\ \cline{1-7}
behind	&	0.59	&	0.02	&	0.03	&	0.31	&	0.08	&	0.12  \\ \cline{1-7}
below	&	0.49	&	0.04	&	0.08	&	0.23	&	0.28	&	0.25  \\ \cline{1-7}
beside	&	0.01	&	0.00	&	0.00	&	0.09	&	0.02	&	0.03  \\ \cline{1-7}
by	    &	0.00	&	0.00	&	0.00	&	0.04	&	0.01	&	0.01  \\ \cline{1-7}
front	&	0.27	&	0.00	&	0.00	&	0.20	&	0.06	&	0.09  \\ \cline{1-7}
in	    &	0.75	&	0.48	&	0.57	&	0.43	&	0.56	&	0.48  \\ \cline{1-7}
left	&	0.36	&	0.02	&	0.03	&	0.18	&	0.22	&	0.18  \\ \cline{1-7}
near	&	0.00	&	0.00	&	0.00	&	0.11	&	0.04	&	0.05  \\ \cline{1-7}
next	&	0.41	&	0.04	&	0.07	&	0.30	&	0.29	&	0.29  \\ \cline{1-7}
on	    &	0.66	&	0.69	&	6.70	&	0.47	&	0.54	&	0.49  \\ \cline{1-7}
over	&	0.00	&	0.00	&	0.00	&	0.06	&	0.02	&	0.03  \\ \cline{1-7}
right	&	0.13	&	0.00	&	0.01	&	0.14	&	0.14	&	0.13  \\ \cline{1-7} 
top	    &	0.00	&	0.00	&	0.00	&	0.01	&	0.02	&	0.01  \\ \cline{1-7} 
under	&	0.46	&	0.07	&	0.11	&	0.24	&	0.30	&	0.25  \\ \cline{1-7} 
\end{tabular}
\caption{SmallVGG(VRD) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type  & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average&	0.30	&	0.11	&	0.13	&	0.20	&	0.20	&	0.18 \\ \cline{1-7}
Micro-Average&	0.66	&	0.30	&	0.41	&	0.35	&	0.35	&	0.35  \\ \cline{1-7}
Example      &	0.31	&	0.31	&	0.31	  \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for SmallVGG(VRD)}
These are the results obtained from training a SmallVGG from scratch consisting of 7 convolutional layers compared to that of the VGG16.
\vspace{-4mm}
\end{table}

\newpage
\subsubsection{Feed Forward Neural Network}
\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class   &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
English & Precision &  Recall   & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7} 
above	&	0.66	&	0.35	&	0.45	&	0.47	&	0.63	&	0.53 \\ \cline{1-7} 
at	    &	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-7}
behind	&	0.20	&	0.00	&	0.00	&	0.24	&	0.12	&	0.16 \\ \cline{1-7}
below	&	0.47	&	0.01	&	0.01	&	0.30	&	0.11	&	0.15 \\ \cline{1-7} 
beside	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-7} 
by	    &	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-7}
front	&	0.00	&	0.00	&	0.00	&	0.24	&	0.08	&	0.11 \\ \cline{1-7}
in	    &	0.85	&	0.48	&	0.61	&	0.66	&	0.56	&	0.61 \\ \cline{1-7}
left	&	0.00	&	0.00	&	0.00	&	0.28	&	0.25	&	0.26 \\ \cline{1-7}
near	&	0.00	&	0.00	&	0.00	&	0.09	&	0.01	&	0.01 \\ \cline{1-7}
next	&	0.46	&	0.01	&	0.02	&	0.29	&	0.45	&	0.35 \\ \cline{1-7}
on	    &	0.65	&	0.63	&	0.64	&	0.51	&	0.86	&	0.64 \\ \cline{1-7}
over	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-7}
right	&	0.00	&	0.00	&	0.00	&	0.24	&	0.24	&	0.23 \\ \cline{1-7}
top	    &	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-7}
under	&	0.50	&	0.02	&	0.03	&	0.29	&	0.47	&	0.36 \\ \cline{1-7}
\end{tabular}
\caption{Feed Forward(VRD) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type  & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average &	0.24	&	0.09	&	0.11	&	0.23	&	0.24	&	0.21 \\ \cline{1-7}
Micro-Average &	0.70	&	0.27	&	0.39	&	0.45	&	0.45	&	0.45 \\ \cline{1-7}
Example       &	0.29	&	0.28	&	0.28    \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for Feed Forward(VRD)}
These are the results achieved by the Feed Forward Neural Network trained only on the geometric features of the bounding boxes, meaning that the object categories had not been included. The results achieved are lower than that of the Geometric Features with Language Features and those of the VGG16 CNN.
\vspace{-4mm}
\end{table}
\newpage

\subsubsection{SmallVGG Activation Maximization}

\begin{figure}[!htbp]
\includegraphics[scale=0.60,center]{SmallVGG_VRD.pdf}
\caption{SmallVGG Activation Maps}
\end{figure}

\begin{figure}[!htbp]
\includegraphics[scale=0.60,center]{Small_VRD_2.pdf}
\caption{SmallVGG Activation Maps}
\end{figure}

\newpage

\subsection{SpatialVoc2k}
Extra results pretaining to the SpatialVoc2k dataset for the SmallVGG and Feed Forward Neural Network without language features.

\subsubsection{SmallVGG}
\begin{table}[!htbp]
\centering

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l}
\cline{1-8}
Class & Class & MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-8}
French & English & Precision &  Recall    & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-8}
a cote de	&	next to	&	0.31	&	0.40	&	0.29	&	0.24	&	0.11	&	0.14 \\ \cline{1-8}
a l'exterieur de	&	outside	&	0.01	&	0.09	&	0.01	&	0.00	&	0.00	&	0.00 \\ \cline{1-8}
au dessus de	&	beyond	&	0.03	&	0.37	&	0.05	&	0.05	&	0.03	&	0.03 \\ \cline{1-8}
au niveau de	&	near/at the level of	&	0.23	&	0.42	&	0.20	&	0.26	&	0.10	&	0.12 \\ \cline{1-8}
aucun	&	none	&	0.00	&	0.39	&	0.01	&	0.00	&	0.02	&	0.01 \\ \cline{1-8}
autour de	&	around	&	0.01	&	0.40	&	0.01	&	0.03	&	0.06	&	0.03 \\ \cline{1-8}
contre	&	against	&	0.16	&	0.43	&	0.18	&	0.17	&	0.14	&	0.13 \\ \cline{1-8}
dans	&	in/into	&	0.01	&	0.22	&	0.01	&	0.02	&	0.13	&	0.03 \\ \cline{1-8}
derriere	&	behind	&	0.28	&	0.32	&	0.21	&	0.21	&	0.19	&	0.17 \\ \cline{1-8}
devant	&	front/before	&	0.32	&	0.28	&	0.23	&	0.21	&	0.25	&	0.21 \\ \cline{1-8}
en face de	&	across from	&	0.06	&	0.36	&	0.08	&	0.05	&	0.04	&	0.03 \\ \cline{1-8}
le long de	&	along/by	&	0.01	&	0.12	&	0.01	&	0.01	&	0.01	&	0.01 \\ \cline{1-8}
loin de	&	far from	&	0.09	&	0.44	&	0.14	&	0.13	&	0.25	&	0.16 \\ \cline{1-8}
par dela	&	beyond	&	0.01	&	0.25	&	0.01	&	0.00	&	0.01	&	0.00 \\ \cline{1-8}
pres de	&	near/by	&	0.58	&	0.54	&	0.50	&	0.30	&	0.32	&	0.29 \\ \cline{1-8}
sous	&	under/below	&	0.18	&	0.25	&	0.15	&	0.33	&	0.32	&	0.30 \\ \cline{1-8}
sur	&	on	&	0.12	&	0.22	&	0.13	&	0.29	&	0.31	&	0.27 \\ \cline{1-8}
\end{tabular}
\caption{SmallVGG(SpatialVoc2k) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type   & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average &	0.14	&	0.32	&	0.13	&	0.14	&	0.13	&	0.11 \\ \cline{1-7}
Micro-Average &	0.18	&	0.40	&	0.24	&	0.21	&	0.21	&	0.21 \\ \cline{1-7}
Example       &	0.19	&	0.39	&	0.24  \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for SmallVGG(SpatialVoc2k)}
These are the results obtained from training a SmallVGG from scratch consisting of 7 convolutional layers compared to that of the VGG16.
\vspace{-4mm}
\end{table}

\newpage
\subsubsection{Feed Forward Neural Network}

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l}
\cline{1-8}
Class & Class & MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-8}
French & English & Precision &  Recall    & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-8}
a cote de	&	next to	&	0.64	&	0.54	&	0.58	&	0.29	&	0.65	&	0.09 \\ \cline{1-8}
a l'exterieur de	&	outside	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-8}
au dessus de	&	beyond	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-8}
au niveau de	&	near/at the level of	&	0.63	&	0.36	&	0.45	&	0.08	&	0.00	&	0.05 \\ \cline{1-8}
aucun	&	none	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-8}
autour de	&	around	&	0.00	&	0.00	&	0.00	&	0.38	&	0.09	&	0.13 \\ \cline{1-8}
contre	&	against	&	0.46	&	0.08	&	0.14	&	0.21	&	0.17	&	0.18 \\ \cline{1-8}
dans	&	in/into	&	0.00	&	0.00	&	0.00	&	0.24	&	0.07	&	0.09 \\ \cline{1-8}
derriere	&	behind	&	0.63	&	0.27	&	0.37	&	0.30	&	0.29	&	0.29 \\ \cline{1-8}
devant	&	front/before	&	0.60	&	0.32	&	0.42	&	0.32	&	0.31	&	0.31 \\ \cline{1-8}
en face de	&	across from	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-8}
le long de	&	along/by	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-8}
loin de	&	far from	&	0.65	&	0.24	&	0.33	&	0.33	&	0.18	&	0.22 \\ \cline{1-8}
par dela	&	beyond	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00	&	0.00 \\ \cline{1-8}
pres de	&	near/by	&	0.73	&	0.71	&	0.71	&	0.31	&	0.66	&	0.42 \\ \cline{1-8}
sous	&	under/below	&	0.08	&	0.00	&	0.01	&	0.27	&	0.30	&	0.26 \\ \cline{1-8}
sur	&	on	&	0.47	&	0.09	&	0.14	&	0.24	&	0.40	&	0.29 \\ \cline{1-8}
\end{tabular}
\caption{Feed Forward NN(SpatialVoc2k) results for MLC and SLC}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l}
\cline{1-7}
Class &  MLC  &  MLC  &  MLC & SLC &  SLC & SLC \\ \cline{1-7}
Metric Type  & Precision &  Recall & F1 & Precision@1 & Recall@1 & F1@1 \\ \cline{1-7}
Macro-Average &	0.29	&	0.15	&	0.19	&	0.17	&	0.15	&	0.13\\ \cline{1-7}
Micro-Average &	0.66	&	0.39	&	0.49	&	0.29	&	0.29	&	0.29 \\ \cline{1-7}
Example       &	0.48	&	0.36	&	0.41 \\ \cline{1-4}
\end{tabular}
\caption{Example/Micro/Macro Average Label Based Metrics for Feed Forward NN(SpatialVoc2k)}
These are the results achived by the Feed Forward Neural Network trained only on the geometric features of the bounding boxes, meaning that the object categories had not been included.
\vspace{-4mm}
\end{table}

\subsubsection{SmallVGG Activation Maximization}
\newpage
\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{Small_Spatial.pdf}
	\caption{SmallVGG Activation Maps}
\end{figure}

\begin{figure}[!htbp]
	\includegraphics[scale=0.60,center]{Small_Spatial_2.pdf}
	\caption{SmallVGG Activation Maps}
\end{figure}

\section{References}
\bibliography{mybib}{}
\bibliographystyle{plain}


\end{document}
