# Interpreting-Neural-Networks-via-Activation-Maximisation

Decision trees are models whose structure allows for tracing an explanation of how the final decision was taken. Neural networks, known as 'black box' model,
 do not readily and explicitly offer an explanation of how the decision was reached. However, since Neural Networks are capable of learning the knowledge representation,
 it will be very useful to develop methods that interpret the model's decisions.

In this project activation maximisation will be used to search for prototypical inputs that maximise the model's response for a quantity of interest, 
Simonyan2013, Sameketal2017. A pair-wise prototype comparison is then carried out under different learning conditions, such as number of classes the model deals with. 
The study is grounded in the area of object recognition in images and will shed light on what models are learning about objects in 2D images.

The methodology in this project is of an experimental nature. 
Following is a possible list of tasks for this project; (a) Setting up an appropriate dataset  from which subsets can be selected (b) 
Building Neural Networks models (c) Maximising the activations and running the ablation studies. (d) concluding from the results in (c).